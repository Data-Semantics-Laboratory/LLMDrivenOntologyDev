{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTgM_LWPJG9N",
        "outputId": "8b8c0437-f81a-4433-f1d3-01c1638f4ce4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fuzzywuzzy\n",
        "!pip install jellyfish\n",
        "!pip install transformers\n",
        "!pip install python-Levenshtein"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdtXFafqJfgJ",
        "outputId": "d4cc2955-ed16-4de9-87f8-26b995ca4dae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.10/dist-packages (0.18.0)\n",
            "Requirement already satisfied: jellyfish in /usr/local/lib/python3.10/dist-packages (1.1.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.10/dist-packages (0.26.0)\n",
            "Requirement already satisfied: Levenshtein==0.26.0 in /usr/local/lib/python3.10/dist-packages (from python-Levenshtein) (0.26.0)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /usr/local/lib/python3.10/dist-packages (from Levenshtein==0.26.0->python-Levenshtein) (3.10.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textdistance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMZdHXZaXi89",
        "outputId": "5185c234-7859-479c-c660-2f60903d86ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textdistance\n",
            "  Downloading textdistance-4.6.3-py3-none-any.whl.metadata (18 kB)\n",
            "Downloading textdistance-4.6.3-py3-none-any.whl (31 kB)\n",
            "Installing collected packages: textdistance\n",
            "Successfully installed textdistance-4.6.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define the directory paths and asjust when wanting to change the generated directory\n",
        "truth_dir = '/content/drive/My Drive/LLM-Data/truth'\n",
        "generated_dir = '/content/drive/My Drive/LLM-Data/llm-generated/GPT4_Enslaved_MainAgent'\n",
        "\n",
        "# code to fix how names are included so we can match them\n",
        "\n",
        "# List of files in the truth directory\n",
        "truth_files = os.listdir(truth_dir)\n",
        "\n",
        "# Function to transform name from \"First Last\" format to \"Last_First\"\n",
        "def transform_name(name):\n",
        "    parts = name.split(\" \")\n",
        "    if len(parts) == 2:\n",
        "        return f\"{parts[1]}_{parts[0]}\"  # Reverse the order and join with underscore\n",
        "    return name\n",
        "\n",
        "# Process each truth file and compare it against generated files\n",
        "def process_files(truth_file):\n",
        "    # Load the truth file without headers\n",
        "    truth_path = os.path.join(truth_dir, truth_file)\n",
        "    truth_df = pd.read_csv(truth_path, sep='\\t', header=None)\n",
        "\n",
        "    # Display the truth DataFrame for debugging\n",
        "    print(f\"\\nProcessing truth file: {truth_file}\")\n",
        "    print(\"Truth DataFrame:\")\n",
        "    print(truth_df)\n",
        "\n",
        "    # Prepare the generated file base name by transforming the truth file name\n",
        "    generated_base_name = transform_name(truth_file.strip('.tsv'))\n",
        "\n",
        "    # Iterate over generated files in the generated directory\n",
        "    for generated_file in os.listdir(generated_dir):\n",
        "        # Check if the generated_file is a file and matches the base name\n",
        "        if os.path.isfile(os.path.join(generated_dir, generated_file)) and generated_base_name in generated_file:\n",
        "            generated_path = os.path.join(generated_dir, generated_file)\n",
        "            print(f\"\\nFound matching generated file: {generated_file}\")\n",
        "\n",
        "            # Load the generated file and drop the first row\n",
        "            generated_df = pd.read_csv(generated_path, sep='\\t', header=0)\n",
        "            generated_df = generated_df.drop(index=0)  # Drop the first row\n",
        "\n",
        "            # Display the generated DataFrame for debugging\n",
        "            print(\"Generated DataFrame before transformation:\")\n",
        "            print(generated_df)\n",
        "\n",
        "            # Transform the Subject column dynamically\n",
        "            generated_df['Subject'] = generated_df['Subject'].apply(lambda x: x.replace('_triples', '').replace('_', ' ').strip())\n",
        "\n",
        "            # Normalize text for matching\n",
        "            truth_df[1] = truth_df[1].str.lower()  # Assuming Predicate is in the second column\n",
        "            truth_df[2] = truth_df[2].str.lower()  # Assuming Object is in the third column\n",
        "            generated_df['Predicate'] = generated_df['Predicate'].str.lower()\n",
        "            generated_df['Object'] = generated_df['Object'].str.lower()\n",
        "\n",
        "            # Display the transformed generated DataFrame for debugging\n",
        "            print(\"Generated DataFrame after transformation:\")\n",
        "            print(generated_df)\n",
        "\n",
        "            # Perform similarity metrics matching\n",
        "            for index, truth_row in truth_df.iterrows():\n",
        "                match_found = False  # Track if any match is found\n",
        "                for _, generated_row in generated_df.iterrows():\n",
        "                    # Print debug information to see what is being compared\n",
        "                    print(f\"Comparing: {truth_row[1]} -> {truth_row[2]} with {generated_row['Predicate']} -> {generated_row['Object']} (Subject: {generated_row['Subject']})\")\n",
        "\n",
        "                    # Compare Predicate, Object pairs\n",
        "                    if truth_row[1] == generated_row['Predicate'] and truth_row[2] == generated_row['Object']:\n",
        "                        print(f\"Match found for truth row {truth_row[0]} with generated row {generated_row['Subject']}\")\n",
        "                        match_found = True\n",
        "                if not match_found:\n",
        "                    print(f\"No match found for truth row: {truth_row[0]}, Predicate: {truth_row[1]}, Object: {truth_row[2]}\")\n",
        "\n",
        "# Process each file in the truth directory\n",
        "for truth_file in truth_files:\n",
        "    process_files(truth_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GHoIse4ZJSgc",
        "outputId": "7440c792-fc77-4ddf-d25b-55ee3a005068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Comparing: hassurname -> haralson with fullnameasstring -> jeremiah haralson (Subject: Jeremiah Haralson)\n",
            "Comparing: hassurname -> haralson with hassurnameasstring -> haralson (Subject: Jeremiah Haralson)\n",
            "Comparing: hassurname -> haralson with hasfirstnameasstring -> jeremiah (Subject: Jeremiah Haralson)\n",
            "Comparing: hassurname -> haralson with hasdescription -> born into slavery, self-educated, served in congress, convicted of pension fraud, vanished from historical record (Subject: Jeremiah Haralson)\n",
            "Comparing: hassurname -> haralson with referstoplaceoforigin -> columbus, georgia (Subject: Jeremiah Haralson)\n",
            "Comparing: hassurname -> haralson with has_race -> african-american (Subject: Jeremiah Haralson)\n",
            "Comparing: hassurname -> haralson with hassex -> male (Subject: Jeremiah Haralson)\n",
            "Comparing: hassurname -> haralson with hasstatusgeneratedevent -> enslavement, self-education, congressional service, conviction for pension fraud (Subject: Jeremiah Haralson)\n",
            "Comparing: hassurname -> haralson with hasvalue -> slave, self-educated, congressman, convict (Subject: Jeremiah Haralson)\n",
            "Comparing: hassurname -> haralson with hasvalue -> congressman, coal miner (Subject: Jeremiah Haralson)\n",
            "Comparing: hassurname -> haralson with hasinteragentrelationshiptype -> spouse (Subject: Jeremiah Haralson)\n",
            "Comparing: hassurname -> haralson with isrelationshipto -> ellen norwood (Subject: Jeremiah Haralson)\n",
            "Comparing: hassurname -> haralson with isrelationshipfrom -> jeremiah haralson (Subject: Jeremiah Haralson)\n",
            "Comparing: hassurname -> haralson with hasparticipantroletype -> congressman (Subject: Jeremiah Haralson)\n",
            "Comparing: hassurname -> haralson with roleprovidedby -> united states congress (Subject: Jeremiah Haralson)\n",
            "No match found for truth row: Jeremiah Haralson, Predicate: hassurname, Object: haralson\n",
            "Comparing: hasname -> jeremiah haralson with haspreferrednamevariant -> jeremiah haralson (Subject: Jeremiah Haralson)\n",
            "Comparing: hasname -> jeremiah haralson with fullnameasstring -> jeremiah haralson (Subject: Jeremiah Haralson)\n",
            "Comparing: hasname -> jeremiah haralson with hassurnameasstring -> haralson (Subject: Jeremiah Haralson)\n",
            "Comparing: hasname -> jeremiah haralson with hasfirstnameasstring -> jeremiah (Subject: Jeremiah Haralson)\n",
            "Comparing: hasname -> jeremiah haralson with hasdescription -> born into slavery, self-educated, served in congress, convicted of pension fraud, vanished from historical record (Subject: Jeremiah Haralson)\n",
            "Comparing: hasname -> jeremiah haralson with referstoplaceoforigin -> columbus, georgia (Subject: Jeremiah Haralson)\n",
            "Comparing: hasname -> jeremiah haralson with has_race -> african-american (Subject: Jeremiah Haralson)\n",
            "Comparing: hasname -> jeremiah haralson with hassex -> male (Subject: Jeremiah Haralson)\n",
            "Comparing: hasname -> jeremiah haralson with hasstatusgeneratedevent -> enslavement, self-education, congressional service, conviction for pension fraud (Subject: Jeremiah Haralson)\n",
            "Comparing: hasname -> jeremiah haralson with hasvalue -> slave, self-educated, congressman, convict (Subject: Jeremiah Haralson)\n",
            "Comparing: hasname -> jeremiah haralson with hasvalue -> congressman, coal miner (Subject: Jeremiah Haralson)\n",
            "Comparing: hasname -> jeremiah haralson with hasinteragentrelationshiptype -> spouse (Subject: Jeremiah Haralson)\n",
            "Comparing: hasname -> jeremiah haralson with isrelationshipto -> ellen norwood (Subject: Jeremiah Haralson)\n",
            "Comparing: hasname -> jeremiah haralson with isrelationshipfrom -> jeremiah haralson (Subject: Jeremiah Haralson)\n",
            "Comparing: hasname -> jeremiah haralson with hasparticipantroletype -> congressman (Subject: Jeremiah Haralson)\n",
            "Comparing: hasname -> jeremiah haralson with roleprovidedby -> united states congress (Subject: Jeremiah Haralson)\n",
            "No match found for truth row: Jeremiah Haralson, Predicate: hasname, Object: jeremiah haralson\n",
            "Comparing: hasdescription -> born enslaved. initially affiliated with the democratic party, he won a contested election and joined the forty-fourth congress on march 4, 1875. with haspreferrednamevariant -> jeremiah haralson (Subject: Jeremiah Haralson)\n",
            "Comparing: hasdescription -> born enslaved. initially affiliated with the democratic party, he won a contested election and joined the forty-fourth congress on march 4, 1875. with fullnameasstring -> jeremiah haralson (Subject: Jeremiah Haralson)\n",
            "Comparing: hasdescription -> born enslaved. initially affiliated with the democratic party, he won a contested election and joined the forty-fourth congress on march 4, 1875. with hassurnameasstring -> haralson (Subject: Jeremiah Haralson)\n",
            "Comparing: hasdescription -> born enslaved. initially affiliated with the democratic party, he won a contested election and joined the forty-fourth congress on march 4, 1875. with hasfirstnameasstring -> jeremiah (Subject: Jeremiah Haralson)\n",
            "Comparing: hasdescription -> born enslaved. initially affiliated with the democratic party, he won a contested election and joined the forty-fourth congress on march 4, 1875. with hasdescription -> born into slavery, self-educated, served in congress, convicted of pension fraud, vanished from historical record (Subject: Jeremiah Haralson)\n",
            "Comparing: hasdescription -> born enslaved. initially affiliated with the democratic party, he won a contested election and joined the forty-fourth congress on march 4, 1875. with referstoplaceoforigin -> columbus, georgia (Subject: Jeremiah Haralson)\n",
            "Comparing: hasdescription -> born enslaved. initially affiliated with the democratic party, he won a contested election and joined the forty-fourth congress on march 4, 1875. with has_race -> african-american (Subject: Jeremiah Haralson)\n",
            "Comparing: hasdescription -> born enslaved. initially affiliated with the democratic party, he won a contested election and joined the forty-fourth congress on march 4, 1875. with hassex -> male (Subject: Jeremiah Haralson)\n",
            "Comparing: hasdescription -> born enslaved. initially affiliated with the democratic party, he won a contested election and joined the forty-fourth congress on march 4, 1875. with hasstatusgeneratedevent -> enslavement, self-education, congressional service, conviction for pension fraud (Subject: Jeremiah Haralson)\n",
            "Comparing: hasdescription -> born enslaved. initially affiliated with the democratic party, he won a contested election and joined the forty-fourth congress on march 4, 1875. with hasvalue -> slave, self-educated, congressman, convict (Subject: Jeremiah Haralson)\n",
            "Comparing: hasdescription -> born enslaved. initially affiliated with the democratic party, he won a contested election and joined the forty-fourth congress on march 4, 1875. with hasvalue -> congressman, coal miner (Subject: Jeremiah Haralson)\n",
            "Comparing: hasdescription -> born enslaved. initially affiliated with the democratic party, he won a contested election and joined the forty-fourth congress on march 4, 1875. with hasinteragentrelationshiptype -> spouse (Subject: Jeremiah Haralson)\n",
            "Comparing: hasdescription -> born enslaved. initially affiliated with the democratic party, he won a contested election and joined the forty-fourth congress on march 4, 1875. with isrelationshipto -> ellen norwood (Subject: Jeremiah Haralson)\n",
            "Comparing: hasdescription -> born enslaved. initially affiliated with the democratic party, he won a contested election and joined the forty-fourth congress on march 4, 1875. with isrelationshipfrom -> jeremiah haralson (Subject: Jeremiah Haralson)\n",
            "Comparing: hasdescription -> born enslaved. initially affiliated with the democratic party, he won a contested election and joined the forty-fourth congress on march 4, 1875. with hasparticipantroletype -> congressman (Subject: Jeremiah Haralson)\n",
            "Comparing: hasdescription -> born enslaved. initially affiliated with the democratic party, he won a contested election and joined the forty-fourth congress on march 4, 1875. with roleprovidedby -> united states congress (Subject: Jeremiah Haralson)\n",
            "No match found for truth row: Jeremiah Haralson, Predicate: hasdescription, Object: born enslaved. initially affiliated with the democratic party, he won a contested election and joined the forty-fourth congress on march 4, 1875.\n",
            "\n",
            "Processing truth file: Prince_Estabrook.tsv\n",
            "Truth DataFrame:\n",
            "                  0                         1  \\\n",
            "0  Prince Estabrook               instance of   \n",
            "1  Prince Estabrook                    hasSex   \n",
            "2  Prince Estabrook        hasParticipantRole   \n",
            "3  Prince Estabrook        hasParticipantRole   \n",
            "4  Prince Estabrook  hasDescriptiveOccupation   \n",
            "5  Prince Estabrook  hasDescriptiveOccupation   \n",
            "6  Prince Estabrook              hasFirstName   \n",
            "7  Prince Estabrook                hasSurname   \n",
            "8  Prince Estabrook                   hasName   \n",
            "9  Prince Estabrook            hasDescription   \n",
            "\n",
            "                                                   2  \n",
            "0                                             Person  \n",
            "1                                               Male  \n",
            "2                                              Child  \n",
            "3                                    Deceased Person  \n",
            "4                                              Slave  \n",
            "5                                Colonial Militiaman  \n",
            "6                                             Prince  \n",
            "7                                          Estabrook  \n",
            "8                                   Prince Estabrook  \n",
            "9  Enslaved. Free before 13th Amendment. Estabroo...  \n",
            "\n",
            "Found matching generated file: Prince_Estabrook_triples.tsv\n",
            "Generated DataFrame before transformation:\n",
            "                     Subject                      Predicate  \\\n",
            "1   Prince_Estabrook_triples        hasPreferredNameVariant   \n",
            "2   Prince_Estabrook_triples               fullNameAsString   \n",
            "3   Prince_Estabrook_triples             hasSurnameAsString   \n",
            "4   Prince_Estabrook_triples           hasFirstnameAsString   \n",
            "5   Prince_Estabrook_triples                 hasDescription   \n",
            "6   Prince_Estabrook_triples          refersToPlaceOfOrigin   \n",
            "7   Prince_Estabrook_triples                       has_Race   \n",
            "8   Prince_Estabrook_triples                   hasBirthDate   \n",
            "9   Prince_Estabrook_triples                   hasDeathDate   \n",
            "10  Prince_Estabrook_triples                         hasSex   \n",
            "11  Prince_Estabrook_triples        hasStatusGeneratedEvent   \n",
            "12  Prince_Estabrook_triples                       hasValue   \n",
            "13  Prince_Estabrook_triples                       hasValue   \n",
            "14  Prince_Estabrook_triples  hasInterAgentRelationshipType   \n",
            "15  Prince_Estabrook_triples               isRelationshipTo   \n",
            "16  Prince_Estabrook_triples             isRelationshipFrom   \n",
            "17  Prince_Estabrook_triples         hasParticipantRoleType   \n",
            "18  Prince_Estabrook_triples                 roleProvidedBy   \n",
            "\n",
            "                                               Object  \n",
            "1                                    Prince Estabrook  \n",
            "2                                    Prince Estabrook  \n",
            "3                                           Estabrook  \n",
            "4                                              Prince  \n",
            "5   Enslaved black man and Minutemen Private who f...  \n",
            "6                            Lexington, Massachusetts  \n",
            "7                                    African-American  \n",
            "8                                                1741  \n",
            "9                                                1830  \n",
            "10                                               Male  \n",
            "11                         American Revolutionary War  \n",
            "12                                    Enslaved, Freed  \n",
            "13                                  Minutemen Private  \n",
            "14                                        Enslaved by  \n",
            "15                                 Benjamin Estabrook  \n",
            "16                                   Prince Estabrook  \n",
            "17                                            Soldier  \n",
            "18                   Battles of Lexington and Concord  \n",
            "Generated DataFrame after transformation:\n",
            "             Subject                      Predicate  \\\n",
            "1   Prince Estabrook        haspreferrednamevariant   \n",
            "2   Prince Estabrook               fullnameasstring   \n",
            "3   Prince Estabrook             hassurnameasstring   \n",
            "4   Prince Estabrook           hasfirstnameasstring   \n",
            "5   Prince Estabrook                 hasdescription   \n",
            "6   Prince Estabrook          referstoplaceoforigin   \n",
            "7   Prince Estabrook                       has_race   \n",
            "8   Prince Estabrook                   hasbirthdate   \n",
            "9   Prince Estabrook                   hasdeathdate   \n",
            "10  Prince Estabrook                         hassex   \n",
            "11  Prince Estabrook        hasstatusgeneratedevent   \n",
            "12  Prince Estabrook                       hasvalue   \n",
            "13  Prince Estabrook                       hasvalue   \n",
            "14  Prince Estabrook  hasinteragentrelationshiptype   \n",
            "15  Prince Estabrook               isrelationshipto   \n",
            "16  Prince Estabrook             isrelationshipfrom   \n",
            "17  Prince Estabrook         hasparticipantroletype   \n",
            "18  Prince Estabrook                 roleprovidedby   \n",
            "\n",
            "                                               Object  \n",
            "1                                    prince estabrook  \n",
            "2                                    prince estabrook  \n",
            "3                                           estabrook  \n",
            "4                                              prince  \n",
            "5   enslaved black man and minutemen private who f...  \n",
            "6                            lexington, massachusetts  \n",
            "7                                    african-american  \n",
            "8                                                1741  \n",
            "9                                                1830  \n",
            "10                                               male  \n",
            "11                         american revolutionary war  \n",
            "12                                    enslaved, freed  \n",
            "13                                  minutemen private  \n",
            "14                                        enslaved by  \n",
            "15                                 benjamin estabrook  \n",
            "16                                   prince estabrook  \n",
            "17                                            soldier  \n",
            "18                   battles of lexington and concord  \n",
            "Comparing: instance of -> person with haspreferrednamevariant -> prince estabrook (Subject: Prince Estabrook)\n",
            "Comparing: instance of -> person with fullnameasstring -> prince estabrook (Subject: Prince Estabrook)\n",
            "Comparing: instance of -> person with hassurnameasstring -> estabrook (Subject: Prince Estabrook)\n",
            "Comparing: instance of -> person with hasfirstnameasstring -> prince (Subject: Prince Estabrook)\n",
            "Comparing: instance of -> person with hasdescription -> enslaved black man and minutemen private who fought and was wounded at the battles of lexington and concord, the initial engagements of the american revolutionary war. (Subject: Prince Estabrook)\n",
            "Comparing: instance of -> person with referstoplaceoforigin -> lexington, massachusetts (Subject: Prince Estabrook)\n",
            "Comparing: instance of -> person with has_race -> african-american (Subject: Prince Estabrook)\n",
            "Comparing: instance of -> person with hasbirthdate -> 1741 (Subject: Prince Estabrook)\n",
            "Comparing: instance of -> person with hasdeathdate -> 1830 (Subject: Prince Estabrook)\n",
            "Comparing: instance of -> person with hassex -> male (Subject: Prince Estabrook)\n",
            "Comparing: instance of -> person with hasstatusgeneratedevent -> american revolutionary war (Subject: Prince Estabrook)\n",
            "Comparing: instance of -> person with hasvalue -> enslaved, freed (Subject: Prince Estabrook)\n",
            "Comparing: instance of -> person with hasvalue -> minutemen private (Subject: Prince Estabrook)\n",
            "Comparing: instance of -> person with hasinteragentrelationshiptype -> enslaved by (Subject: Prince Estabrook)\n",
            "Comparing: instance of -> person with isrelationshipto -> benjamin estabrook (Subject: Prince Estabrook)\n",
            "Comparing: instance of -> person with isrelationshipfrom -> prince estabrook (Subject: Prince Estabrook)\n",
            "Comparing: instance of -> person with hasparticipantroletype -> soldier (Subject: Prince Estabrook)\n",
            "Comparing: instance of -> person with roleprovidedby -> battles of lexington and concord (Subject: Prince Estabrook)\n",
            "No match found for truth row: Prince Estabrook, Predicate: instance of, Object: person\n",
            "Comparing: hassex -> male with haspreferrednamevariant -> prince estabrook (Subject: Prince Estabrook)\n",
            "Comparing: hassex -> male with fullnameasstring -> prince estabrook (Subject: Prince Estabrook)\n",
            "Comparing: hassex -> male with hassurnameasstring -> estabrook (Subject: Prince Estabrook)\n",
            "Comparing: hassex -> male with hasfirstnameasstring -> prince (Subject: Prince Estabrook)\n",
            "Comparing: hassex -> male with hasdescription -> enslaved black man and minutemen private who fought and was wounded at the battles of lexington and concord, the initial engagements of the american revolutionary war. (Subject: Prince Estabrook)\n",
            "Comparing: hassex -> male with referstoplaceoforigin -> lexington, massachusetts (Subject: Prince Estabrook)\n",
            "Comparing: hassex -> male with has_race -> african-american (Subject: Prince Estabrook)\n",
            "Comparing: hassex -> male with hasbirthdate -> 1741 (Subject: Prince Estabrook)\n",
            "Comparing: hassex -> male with hasdeathdate -> 1830 (Subject: Prince Estabrook)\n",
            "Comparing: hassex -> male with hassex -> male (Subject: Prince Estabrook)\n",
            "Match found for truth row Prince Estabrook with generated row Prince Estabrook\n",
            "Comparing: hassex -> male with hasstatusgeneratedevent -> american revolutionary war (Subject: Prince Estabrook)\n",
            "Comparing: hassex -> male with hasvalue -> enslaved, freed (Subject: Prince Estabrook)\n",
            "Comparing: hassex -> male with hasvalue -> minutemen private (Subject: Prince Estabrook)\n",
            "Comparing: hassex -> male with hasinteragentrelationshiptype -> enslaved by (Subject: Prince Estabrook)\n",
            "Comparing: hassex -> male with isrelationshipto -> benjamin estabrook (Subject: Prince Estabrook)\n",
            "Comparing: hassex -> male with isrelationshipfrom -> prince estabrook (Subject: Prince Estabrook)\n",
            "Comparing: hassex -> male with hasparticipantroletype -> soldier (Subject: Prince Estabrook)\n",
            "Comparing: hassex -> male with roleprovidedby -> battles of lexington and concord (Subject: Prince Estabrook)\n",
            "Comparing: hasparticipantrole -> child with haspreferrednamevariant -> prince estabrook (Subject: Prince Estabrook)\n",
            "Comparing: hasparticipantrole -> child with fullnameasstring -> prince estabrook (Subject: Prince Estabrook)\n",
            "Comparing: hasparticipantrole -> child with hassurnameasstring -> estabrook (Subject: Prince Estabrook)\n",
            "Comparing: hasparticipantrole -> child with hasfirstnameasstring -> prince (Subject: Prince Estabrook)\n",
            "Comparing: hasparticipantrole -> child with hasdescription -> enslaved black man and minutemen private who fought and was wounded at the battles of lexington and concord, the initial engagements of the american revolutionary war. (Subject: Prince Estabrook)\n",
            "Comparing: hasparticipantrole -> child with referstoplaceoforigin -> lexington, massachusetts (Subject: Prince Estabrook)\n",
            "Comparing: hasparticipantrole -> child with has_race -> african-american (Subject: Prince Estabrook)\n",
            "Comparing: hasparticipantrole -> child with hasbirthdate -> 1741 (Subject: Prince Estabrook)\n",
            "Comparing: hasparticipantrole -> child with hasdeathdate -> 1830 (Subject: Prince Estabrook)\n",
            "Comparing: hasparticipantrole -> child with hassex -> male (Subject: Prince Estabrook)\n",
            "Comparing: hasparticipantrole -> child with hasstatusgeneratedevent -> american revolutionary war (Subject: Prince Estabrook)\n",
            "Comparing: hasparticipantrole -> child with hasvalue -> enslaved, freed (Subject: Prince Estabrook)\n",
            "Comparing: hasparticipantrole -> child with hasvalue -> minutemen private (Subject: Prince Estabrook)\n",
            "Comparing: hasparticipantrole -> child with hasinteragentrelationshiptype -> enslaved by (Subject: Prince Estabrook)\n",
            "Comparing: hasparticipantrole -> child with isrelationshipto -> benjamin estabrook (Subject: Prince Estabrook)\n",
            "Comparing: hasparticipantrole -> child with isrelationshipfrom -> prince estabrook (Subject: Prince Estabrook)\n",
            "Comparing: hasparticipantrole -> child with hasparticipantroletype -> soldier (Subject: Prince Estabrook)\n",
            "Comparing: hasparticipantrole -> child with roleprovidedby -> battles of lexington and concord (Subject: Prince Estabrook)\n",
            "No match found for truth row: Prince Estabrook, Predicate: hasparticipantrole, Object: child\n",
            "Comparing: hasparticipantrole -> deceased person with haspreferrednamevariant -> prince estabrook (Subject: Prince Estabrook)\n",
            "Comparing: hasparticipantrole -> deceased person with fullnameasstring -> prince estabrook (Subject: Prince Estabrook)\n",
            "Comparing: hasparticipantrole -> deceased person with hassurnameasstring -> estabrook (Subject: Prince Estabrook)\n",
            "Comparing: hasparticipantrole -> deceased person with hasfirstnameasstring -> prince (Subject: Prince Estabrook)\n",
            "Comparing: hasparticipantrole -> deceased person with hasdescription -> enslaved black man and minutemen private who fought and was wounded at the battles of lexington and concord, the initial engagements of the american revolutionary war. (Subject: Prince Estabrook)\n",
            "Comparing: hasparticipantrole -> deceased person with referstoplaceoforigin -> lexington, massachusetts (Subject: Prince Estabrook)\n",
            "Comparing: hasparticipantrole -> deceased person with has_race -> african-american (Subject: Prince Estabrook)\n",
            "Comparing: hasparticipantrole -> deceased person with hasbirthdate -> 1741 (Subject: Prince Estabrook)\n",
            "Comparing: hasparticipantrole -> deceased person with hasdeathdate -> 1830 (Subject: Prince Estabrook)\n",
            "Comparing: hasparticipantrole -> deceased person with hassex -> male (Subject: Prince Estabrook)\n",
            "Comparing: hasparticipantrole -> deceased person with hasstatusgeneratedevent -> american revolutionary war (Subject: Prince Estabrook)\n",
            "Comparing: hasparticipantrole -> deceased person with hasvalue -> enslaved, freed (Subject: Prince Estabrook)\n",
            "Comparing: hasparticipantrole -> deceased person with hasvalue -> minutemen private (Subject: Prince Estabrook)\n",
            "Comparing: hasparticipantrole -> deceased person with hasinteragentrelationshiptype -> enslaved by (Subject: Prince Estabrook)\n",
            "Comparing: hasparticipantrole -> deceased person with isrelationshipto -> benjamin estabrook (Subject: Prince Estabrook)\n",
            "Comparing: hasparticipantrole -> deceased person with isrelationshipfrom -> prince estabrook (Subject: Prince Estabrook)\n",
            "Comparing: hasparticipantrole -> deceased person with hasparticipantroletype -> soldier (Subject: Prince Estabrook)\n",
            "Comparing: hasparticipantrole -> deceased person with roleprovidedby -> battles of lexington and concord (Subject: Prince Estabrook)\n",
            "No match found for truth row: Prince Estabrook, Predicate: hasparticipantrole, Object: deceased person\n",
            "Comparing: hasdescriptiveoccupation -> slave with haspreferrednamevariant -> prince estabrook (Subject: Prince Estabrook)\n",
            "Comparing: hasdescriptiveoccupation -> slave with fullnameasstring -> prince estabrook (Subject: Prince Estabrook)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hassurnameasstring -> estabrook (Subject: Prince Estabrook)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasfirstnameasstring -> prince (Subject: Prince Estabrook)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasdescription -> enslaved black man and minutemen private who fought and was wounded at the battles of lexington and concord, the initial engagements of the american revolutionary war. (Subject: Prince Estabrook)\n",
            "Comparing: hasdescriptiveoccupation -> slave with referstoplaceoforigin -> lexington, massachusetts (Subject: Prince Estabrook)\n",
            "Comparing: hasdescriptiveoccupation -> slave with has_race -> african-american (Subject: Prince Estabrook)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasbirthdate -> 1741 (Subject: Prince Estabrook)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasdeathdate -> 1830 (Subject: Prince Estabrook)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hassex -> male (Subject: Prince Estabrook)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasstatusgeneratedevent -> american revolutionary war (Subject: Prince Estabrook)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasvalue -> enslaved, freed (Subject: Prince Estabrook)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasvalue -> minutemen private (Subject: Prince Estabrook)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasinteragentrelationshiptype -> enslaved by (Subject: Prince Estabrook)\n",
            "Comparing: hasdescriptiveoccupation -> slave with isrelationshipto -> benjamin estabrook (Subject: Prince Estabrook)\n",
            "Comparing: hasdescriptiveoccupation -> slave with isrelationshipfrom -> prince estabrook (Subject: Prince Estabrook)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasparticipantroletype -> soldier (Subject: Prince Estabrook)\n",
            "Comparing: hasdescriptiveoccupation -> slave with roleprovidedby -> battles of lexington and concord (Subject: Prince Estabrook)\n",
            "No match found for truth row: Prince Estabrook, Predicate: hasdescriptiveoccupation, Object: slave\n",
            "Comparing: hasdescriptiveoccupation -> colonial militiaman with haspreferrednamevariant -> prince estabrook (Subject: Prince Estabrook)\n",
            "Comparing: hasdescriptiveoccupation -> colonial militiaman with fullnameasstring -> prince estabrook (Subject: Prince Estabrook)\n",
            "Comparing: hasdescriptiveoccupation -> colonial militiaman with hassurnameasstring -> estabrook (Subject: Prince Estabrook)\n",
            "Comparing: hasdescriptiveoccupation -> colonial militiaman with hasfirstnameasstring -> prince (Subject: Prince Estabrook)\n",
            "Comparing: hasdescriptiveoccupation -> colonial militiaman with hasdescription -> enslaved black man and minutemen private who fought and was wounded at the battles of lexington and concord, the initial engagements of the american revolutionary war. (Subject: Prince Estabrook)\n",
            "Comparing: hasdescriptiveoccupation -> colonial militiaman with referstoplaceoforigin -> lexington, massachusetts (Subject: Prince Estabrook)\n",
            "Comparing: hasdescriptiveoccupation -> colonial militiaman with has_race -> african-american (Subject: Prince Estabrook)\n",
            "Comparing: hasdescriptiveoccupation -> colonial militiaman with hasbirthdate -> 1741 (Subject: Prince Estabrook)\n",
            "Comparing: hasdescriptiveoccupation -> colonial militiaman with hasdeathdate -> 1830 (Subject: Prince Estabrook)\n",
            "Comparing: hasdescriptiveoccupation -> colonial militiaman with hassex -> male (Subject: Prince Estabrook)\n",
            "Comparing: hasdescriptiveoccupation -> colonial militiaman with hasstatusgeneratedevent -> american revolutionary war (Subject: Prince Estabrook)\n",
            "Comparing: hasdescriptiveoccupation -> colonial militiaman with hasvalue -> enslaved, freed (Subject: Prince Estabrook)\n",
            "Comparing: hasdescriptiveoccupation -> colonial militiaman with hasvalue -> minutemen private (Subject: Prince Estabrook)\n",
            "Comparing: hasdescriptiveoccupation -> colonial militiaman with hasinteragentrelationshiptype -> enslaved by (Subject: Prince Estabrook)\n",
            "Comparing: hasdescriptiveoccupation -> colonial militiaman with isrelationshipto -> benjamin estabrook (Subject: Prince Estabrook)\n",
            "Comparing: hasdescriptiveoccupation -> colonial militiaman with isrelationshipfrom -> prince estabrook (Subject: Prince Estabrook)\n",
            "Comparing: hasdescriptiveoccupation -> colonial militiaman with hasparticipantroletype -> soldier (Subject: Prince Estabrook)\n",
            "Comparing: hasdescriptiveoccupation -> colonial militiaman with roleprovidedby -> battles of lexington and concord (Subject: Prince Estabrook)\n",
            "No match found for truth row: Prince Estabrook, Predicate: hasdescriptiveoccupation, Object: colonial militiaman\n",
            "Comparing: hasfirstname -> prince with haspreferrednamevariant -> prince estabrook (Subject: Prince Estabrook)\n",
            "Comparing: hasfirstname -> prince with fullnameasstring -> prince estabrook (Subject: Prince Estabrook)\n",
            "Comparing: hasfirstname -> prince with hassurnameasstring -> estabrook (Subject: Prince Estabrook)\n",
            "Comparing: hasfirstname -> prince with hasfirstnameasstring -> prince (Subject: Prince Estabrook)\n",
            "Comparing: hasfirstname -> prince with hasdescription -> enslaved black man and minutemen private who fought and was wounded at the battles of lexington and concord, the initial engagements of the american revolutionary war. (Subject: Prince Estabrook)\n",
            "Comparing: hasfirstname -> prince with referstoplaceoforigin -> lexington, massachusetts (Subject: Prince Estabrook)\n",
            "Comparing: hasfirstname -> prince with has_race -> african-american (Subject: Prince Estabrook)\n",
            "Comparing: hasfirstname -> prince with hasbirthdate -> 1741 (Subject: Prince Estabrook)\n",
            "Comparing: hasfirstname -> prince with hasdeathdate -> 1830 (Subject: Prince Estabrook)\n",
            "Comparing: hasfirstname -> prince with hassex -> male (Subject: Prince Estabrook)\n",
            "Comparing: hasfirstname -> prince with hasstatusgeneratedevent -> american revolutionary war (Subject: Prince Estabrook)\n",
            "Comparing: hasfirstname -> prince with hasvalue -> enslaved, freed (Subject: Prince Estabrook)\n",
            "Comparing: hasfirstname -> prince with hasvalue -> minutemen private (Subject: Prince Estabrook)\n",
            "Comparing: hasfirstname -> prince with hasinteragentrelationshiptype -> enslaved by (Subject: Prince Estabrook)\n",
            "Comparing: hasfirstname -> prince with isrelationshipto -> benjamin estabrook (Subject: Prince Estabrook)\n",
            "Comparing: hasfirstname -> prince with isrelationshipfrom -> prince estabrook (Subject: Prince Estabrook)\n",
            "Comparing: hasfirstname -> prince with hasparticipantroletype -> soldier (Subject: Prince Estabrook)\n",
            "Comparing: hasfirstname -> prince with roleprovidedby -> battles of lexington and concord (Subject: Prince Estabrook)\n",
            "No match found for truth row: Prince Estabrook, Predicate: hasfirstname, Object: prince\n",
            "Comparing: hassurname -> estabrook with haspreferrednamevariant -> prince estabrook (Subject: Prince Estabrook)\n",
            "Comparing: hassurname -> estabrook with fullnameasstring -> prince estabrook (Subject: Prince Estabrook)\n",
            "Comparing: hassurname -> estabrook with hassurnameasstring -> estabrook (Subject: Prince Estabrook)\n",
            "Comparing: hassurname -> estabrook with hasfirstnameasstring -> prince (Subject: Prince Estabrook)\n",
            "Comparing: hassurname -> estabrook with hasdescription -> enslaved black man and minutemen private who fought and was wounded at the battles of lexington and concord, the initial engagements of the american revolutionary war. (Subject: Prince Estabrook)\n",
            "Comparing: hassurname -> estabrook with referstoplaceoforigin -> lexington, massachusetts (Subject: Prince Estabrook)\n",
            "Comparing: hassurname -> estabrook with has_race -> african-american (Subject: Prince Estabrook)\n",
            "Comparing: hassurname -> estabrook with hasbirthdate -> 1741 (Subject: Prince Estabrook)\n",
            "Comparing: hassurname -> estabrook with hasdeathdate -> 1830 (Subject: Prince Estabrook)\n",
            "Comparing: hassurname -> estabrook with hassex -> male (Subject: Prince Estabrook)\n",
            "Comparing: hassurname -> estabrook with hasstatusgeneratedevent -> american revolutionary war (Subject: Prince Estabrook)\n",
            "Comparing: hassurname -> estabrook with hasvalue -> enslaved, freed (Subject: Prince Estabrook)\n",
            "Comparing: hassurname -> estabrook with hasvalue -> minutemen private (Subject: Prince Estabrook)\n",
            "Comparing: hassurname -> estabrook with hasinteragentrelationshiptype -> enslaved by (Subject: Prince Estabrook)\n",
            "Comparing: hassurname -> estabrook with isrelationshipto -> benjamin estabrook (Subject: Prince Estabrook)\n",
            "Comparing: hassurname -> estabrook with isrelationshipfrom -> prince estabrook (Subject: Prince Estabrook)\n",
            "Comparing: hassurname -> estabrook with hasparticipantroletype -> soldier (Subject: Prince Estabrook)\n",
            "Comparing: hassurname -> estabrook with roleprovidedby -> battles of lexington and concord (Subject: Prince Estabrook)\n",
            "No match found for truth row: Prince Estabrook, Predicate: hassurname, Object: estabrook\n",
            "Comparing: hasname -> prince estabrook with haspreferrednamevariant -> prince estabrook (Subject: Prince Estabrook)\n",
            "Comparing: hasname -> prince estabrook with fullnameasstring -> prince estabrook (Subject: Prince Estabrook)\n",
            "Comparing: hasname -> prince estabrook with hassurnameasstring -> estabrook (Subject: Prince Estabrook)\n",
            "Comparing: hasname -> prince estabrook with hasfirstnameasstring -> prince (Subject: Prince Estabrook)\n",
            "Comparing: hasname -> prince estabrook with hasdescription -> enslaved black man and minutemen private who fought and was wounded at the battles of lexington and concord, the initial engagements of the american revolutionary war. (Subject: Prince Estabrook)\n",
            "Comparing: hasname -> prince estabrook with referstoplaceoforigin -> lexington, massachusetts (Subject: Prince Estabrook)\n",
            "Comparing: hasname -> prince estabrook with has_race -> african-american (Subject: Prince Estabrook)\n",
            "Comparing: hasname -> prince estabrook with hasbirthdate -> 1741 (Subject: Prince Estabrook)\n",
            "Comparing: hasname -> prince estabrook with hasdeathdate -> 1830 (Subject: Prince Estabrook)\n",
            "Comparing: hasname -> prince estabrook with hassex -> male (Subject: Prince Estabrook)\n",
            "Comparing: hasname -> prince estabrook with hasstatusgeneratedevent -> american revolutionary war (Subject: Prince Estabrook)\n",
            "Comparing: hasname -> prince estabrook with hasvalue -> enslaved, freed (Subject: Prince Estabrook)\n",
            "Comparing: hasname -> prince estabrook with hasvalue -> minutemen private (Subject: Prince Estabrook)\n",
            "Comparing: hasname -> prince estabrook with hasinteragentrelationshiptype -> enslaved by (Subject: Prince Estabrook)\n",
            "Comparing: hasname -> prince estabrook with isrelationshipto -> benjamin estabrook (Subject: Prince Estabrook)\n",
            "Comparing: hasname -> prince estabrook with isrelationshipfrom -> prince estabrook (Subject: Prince Estabrook)\n",
            "Comparing: hasname -> prince estabrook with hasparticipantroletype -> soldier (Subject: Prince Estabrook)\n",
            "Comparing: hasname -> prince estabrook with roleprovidedby -> battles of lexington and concord (Subject: Prince Estabrook)\n",
            "No match found for truth row: Prince Estabrook, Predicate: hasname, Object: prince estabrook\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. estabrook participated in the battle of lexington and is the first known african american who fought in the war of independence. with haspreferrednamevariant -> prince estabrook (Subject: Prince Estabrook)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. estabrook participated in the battle of lexington and is the first known african american who fought in the war of independence. with fullnameasstring -> prince estabrook (Subject: Prince Estabrook)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. estabrook participated in the battle of lexington and is the first known african american who fought in the war of independence. with hassurnameasstring -> estabrook (Subject: Prince Estabrook)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. estabrook participated in the battle of lexington and is the first known african american who fought in the war of independence. with hasfirstnameasstring -> prince (Subject: Prince Estabrook)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. estabrook participated in the battle of lexington and is the first known african american who fought in the war of independence. with hasdescription -> enslaved black man and minutemen private who fought and was wounded at the battles of lexington and concord, the initial engagements of the american revolutionary war. (Subject: Prince Estabrook)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. estabrook participated in the battle of lexington and is the first known african american who fought in the war of independence. with referstoplaceoforigin -> lexington, massachusetts (Subject: Prince Estabrook)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. estabrook participated in the battle of lexington and is the first known african american who fought in the war of independence. with has_race -> african-american (Subject: Prince Estabrook)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. estabrook participated in the battle of lexington and is the first known african american who fought in the war of independence. with hasbirthdate -> 1741 (Subject: Prince Estabrook)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. estabrook participated in the battle of lexington and is the first known african american who fought in the war of independence. with hasdeathdate -> 1830 (Subject: Prince Estabrook)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. estabrook participated in the battle of lexington and is the first known african american who fought in the war of independence. with hassex -> male (Subject: Prince Estabrook)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. estabrook participated in the battle of lexington and is the first known african american who fought in the war of independence. with hasstatusgeneratedevent -> american revolutionary war (Subject: Prince Estabrook)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. estabrook participated in the battle of lexington and is the first known african american who fought in the war of independence. with hasvalue -> enslaved, freed (Subject: Prince Estabrook)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. estabrook participated in the battle of lexington and is the first known african american who fought in the war of independence. with hasvalue -> minutemen private (Subject: Prince Estabrook)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. estabrook participated in the battle of lexington and is the first known african american who fought in the war of independence. with hasinteragentrelationshiptype -> enslaved by (Subject: Prince Estabrook)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. estabrook participated in the battle of lexington and is the first known african american who fought in the war of independence. with isrelationshipto -> benjamin estabrook (Subject: Prince Estabrook)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. estabrook participated in the battle of lexington and is the first known african american who fought in the war of independence. with isrelationshipfrom -> prince estabrook (Subject: Prince Estabrook)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. estabrook participated in the battle of lexington and is the first known african american who fought in the war of independence. with hasparticipantroletype -> soldier (Subject: Prince Estabrook)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. estabrook participated in the battle of lexington and is the first known african american who fought in the war of independence. with roleprovidedby -> battles of lexington and concord (Subject: Prince Estabrook)\n",
            "No match found for truth row: Prince Estabrook, Predicate: hasdescription, Object: enslaved. free before 13th amendment. estabrook participated in the battle of lexington and is the first known african american who fought in the war of independence.\n",
            "\n",
            "Processing truth file: Boston_King.tsv\n",
            "Truth DataFrame:\n",
            "              0                         1  \\\n",
            "0   Boston King               instance of   \n",
            "1   Boston King                    hasSex   \n",
            "2   Boston King        hasParticipantRole   \n",
            "3   Boston King        hasParticipantRole   \n",
            "4   Boston King  hasDescriptiveOccupation   \n",
            "5   Boston King  hasDescriptiveOccupation   \n",
            "6   Boston King  hasDescriptiveOccupation   \n",
            "7   Boston King  hasDescriptiveOccupation   \n",
            "8   Boston King              hasFirstName   \n",
            "9   Boston King                hasSurname   \n",
            "10  Boston King                   hasName   \n",
            "11  Boston King            hasDescription   \n",
            "\n",
            "                                                    2  \n",
            "0                                              Person  \n",
            "1                                                Male  \n",
            "2                                               Child  \n",
            "3                                     Deceased Person  \n",
            "4                                               Slave  \n",
            "5                      Loyalist (American Revolution)  \n",
            "6                      Methodist Episcopal Lay Leader  \n",
            "7                          Autobiographer / Memoirist  \n",
            "8                                              Boston  \n",
            "9                                                King  \n",
            "10                                        Boston King  \n",
            "11  Born Enslaved. Free before 13th Amendment. Res...  \n",
            "\n",
            "Found matching generated file: Boston_King_triples.tsv\n",
            "Generated DataFrame before transformation:\n",
            "                Subject                      Predicate  \\\n",
            "1   Boston_King_triples        hasPreferredNameVariant   \n",
            "2   Boston_King_triples               fullNameAsString   \n",
            "3   Boston_King_triples             hasSurnameAsString   \n",
            "4   Boston_King_triples           hasFirstnameAsString   \n",
            "5   Boston_King_triples                 hasDescription   \n",
            "6   Boston_King_triples          refersToPlaceOfOrigin   \n",
            "7   Boston_King_triples                       has_Race   \n",
            "8   Boston_King_triples                   hasBirthDate   \n",
            "9   Boston_King_triples                   hasDeathDate   \n",
            "10  Boston_King_triples                         hasSex   \n",
            "11  Boston_King_triples        hasStatusGeneratedEvent   \n",
            "12  Boston_King_triples                       hasValue   \n",
            "13  Boston_King_triples                       hasValue   \n",
            "14  Boston_King_triples  hasInterAgentRelationshipType   \n",
            "15  Boston_King_triples               isRelationshipTo   \n",
            "16  Boston_King_triples             isRelationshipFrom   \n",
            "17  Boston_King_triples         hasParticipantRoleType   \n",
            "18  Boston_King_triples                 roleProvidedBy   \n",
            "\n",
            "                                               Object  \n",
            "1                                         Boston King  \n",
            "2                                         Boston King  \n",
            "3                                                King  \n",
            "4                                              Boston  \n",
            "5   A former American slave and Black Loyalist, wh...  \n",
            "6                                      South Carolina  \n",
            "7                                    African-American  \n",
            "8                                             c. 1760  \n",
            "9                                                1802  \n",
            "10                                               Male  \n",
            "11  American Revolutionary War, Migration to Nova ...  \n",
            "12  Former Slave, Black Loyalist, Methodist Minist...  \n",
            "13               Methodist Minister, Master Carpenter  \n",
            "14                                             Spouse  \n",
            "15                                             Violet  \n",
            "16                                        Boston King  \n",
            "17                       Founder, Teacher, Missionary  \n",
            "18  Founding of Freetown, Sierra Leone, Teaching s...  \n",
            "Generated DataFrame after transformation:\n",
            "        Subject                      Predicate  \\\n",
            "1   Boston King        haspreferrednamevariant   \n",
            "2   Boston King               fullnameasstring   \n",
            "3   Boston King             hassurnameasstring   \n",
            "4   Boston King           hasfirstnameasstring   \n",
            "5   Boston King                 hasdescription   \n",
            "6   Boston King          referstoplaceoforigin   \n",
            "7   Boston King                       has_race   \n",
            "8   Boston King                   hasbirthdate   \n",
            "9   Boston King                   hasdeathdate   \n",
            "10  Boston King                         hassex   \n",
            "11  Boston King        hasstatusgeneratedevent   \n",
            "12  Boston King                       hasvalue   \n",
            "13  Boston King                       hasvalue   \n",
            "14  Boston King  hasinteragentrelationshiptype   \n",
            "15  Boston King               isrelationshipto   \n",
            "16  Boston King             isrelationshipfrom   \n",
            "17  Boston King         hasparticipantroletype   \n",
            "18  Boston King                 roleprovidedby   \n",
            "\n",
            "                                               Object  \n",
            "1                                         boston king  \n",
            "2                                         boston king  \n",
            "3                                                king  \n",
            "4                                              boston  \n",
            "5   a former american slave and black loyalist, wh...  \n",
            "6                                      south carolina  \n",
            "7                                    african-american  \n",
            "8                                             c. 1760  \n",
            "9                                                1802  \n",
            "10                                               male  \n",
            "11  american revolutionary war, migration to nova ...  \n",
            "12  former slave, black loyalist, methodist minist...  \n",
            "13               methodist minister, master carpenter  \n",
            "14                                             spouse  \n",
            "15                                             violet  \n",
            "16                                        boston king  \n",
            "17                       founder, teacher, missionary  \n",
            "18  founding of freetown, sierra leone, teaching s...  \n",
            "Comparing: instance of -> person with haspreferrednamevariant -> boston king (Subject: Boston King)\n",
            "Comparing: instance of -> person with fullnameasstring -> boston king (Subject: Boston King)\n",
            "Comparing: instance of -> person with hassurnameasstring -> king (Subject: Boston King)\n",
            "Comparing: instance of -> person with hasfirstnameasstring -> boston (Subject: Boston King)\n",
            "Comparing: instance of -> person with hasdescription -> a former american slave and black loyalist, who gained freedom from the british and settled in nova scotia after the american revolutionary war. he later immigrated to sierra leone, where he helped found freetown and became the first methodist missionary to african indigenous people. (Subject: Boston King)\n",
            "Comparing: instance of -> person with referstoplaceoforigin -> south carolina (Subject: Boston King)\n",
            "Comparing: instance of -> person with has_race -> african-american (Subject: Boston King)\n",
            "Comparing: instance of -> person with hasbirthdate -> c. 1760 (Subject: Boston King)\n",
            "Comparing: instance of -> person with hasdeathdate -> 1802 (Subject: Boston King)\n",
            "Comparing: instance of -> person with hassex -> male (Subject: Boston King)\n",
            "Comparing: instance of -> person with hasstatusgeneratedevent -> american revolutionary war, migration to nova scotia, immigration to sierra leone (Subject: Boston King)\n",
            "Comparing: instance of -> person with hasvalue -> former slave, black loyalist, methodist minister, master carpenter (Subject: Boston King)\n",
            "Comparing: instance of -> person with hasvalue -> methodist minister, master carpenter (Subject: Boston King)\n",
            "Comparing: instance of -> person with hasinteragentrelationshiptype -> spouse (Subject: Boston King)\n",
            "Comparing: instance of -> person with isrelationshipto -> violet (Subject: Boston King)\n",
            "Comparing: instance of -> person with isrelationshipfrom -> boston king (Subject: Boston King)\n",
            "Comparing: instance of -> person with hasparticipantroletype -> founder, teacher, missionary (Subject: Boston King)\n",
            "Comparing: instance of -> person with roleprovidedby -> founding of freetown, sierra leone, teaching settlers, missionary work among sherbro people (Subject: Boston King)\n",
            "No match found for truth row: Boston King, Predicate: instance of, Object: person\n",
            "Comparing: hassex -> male with haspreferrednamevariant -> boston king (Subject: Boston King)\n",
            "Comparing: hassex -> male with fullnameasstring -> boston king (Subject: Boston King)\n",
            "Comparing: hassex -> male with hassurnameasstring -> king (Subject: Boston King)\n",
            "Comparing: hassex -> male with hasfirstnameasstring -> boston (Subject: Boston King)\n",
            "Comparing: hassex -> male with hasdescription -> a former american slave and black loyalist, who gained freedom from the british and settled in nova scotia after the american revolutionary war. he later immigrated to sierra leone, where he helped found freetown and became the first methodist missionary to african indigenous people. (Subject: Boston King)\n",
            "Comparing: hassex -> male with referstoplaceoforigin -> south carolina (Subject: Boston King)\n",
            "Comparing: hassex -> male with has_race -> african-american (Subject: Boston King)\n",
            "Comparing: hassex -> male with hasbirthdate -> c. 1760 (Subject: Boston King)\n",
            "Comparing: hassex -> male with hasdeathdate -> 1802 (Subject: Boston King)\n",
            "Comparing: hassex -> male with hassex -> male (Subject: Boston King)\n",
            "Match found for truth row Boston King with generated row Boston King\n",
            "Comparing: hassex -> male with hasstatusgeneratedevent -> american revolutionary war, migration to nova scotia, immigration to sierra leone (Subject: Boston King)\n",
            "Comparing: hassex -> male with hasvalue -> former slave, black loyalist, methodist minister, master carpenter (Subject: Boston King)\n",
            "Comparing: hassex -> male with hasvalue -> methodist minister, master carpenter (Subject: Boston King)\n",
            "Comparing: hassex -> male with hasinteragentrelationshiptype -> spouse (Subject: Boston King)\n",
            "Comparing: hassex -> male with isrelationshipto -> violet (Subject: Boston King)\n",
            "Comparing: hassex -> male with isrelationshipfrom -> boston king (Subject: Boston King)\n",
            "Comparing: hassex -> male with hasparticipantroletype -> founder, teacher, missionary (Subject: Boston King)\n",
            "Comparing: hassex -> male with roleprovidedby -> founding of freetown, sierra leone, teaching settlers, missionary work among sherbro people (Subject: Boston King)\n",
            "Comparing: hasparticipantrole -> child with haspreferrednamevariant -> boston king (Subject: Boston King)\n",
            "Comparing: hasparticipantrole -> child with fullnameasstring -> boston king (Subject: Boston King)\n",
            "Comparing: hasparticipantrole -> child with hassurnameasstring -> king (Subject: Boston King)\n",
            "Comparing: hasparticipantrole -> child with hasfirstnameasstring -> boston (Subject: Boston King)\n",
            "Comparing: hasparticipantrole -> child with hasdescription -> a former american slave and black loyalist, who gained freedom from the british and settled in nova scotia after the american revolutionary war. he later immigrated to sierra leone, where he helped found freetown and became the first methodist missionary to african indigenous people. (Subject: Boston King)\n",
            "Comparing: hasparticipantrole -> child with referstoplaceoforigin -> south carolina (Subject: Boston King)\n",
            "Comparing: hasparticipantrole -> child with has_race -> african-american (Subject: Boston King)\n",
            "Comparing: hasparticipantrole -> child with hasbirthdate -> c. 1760 (Subject: Boston King)\n",
            "Comparing: hasparticipantrole -> child with hasdeathdate -> 1802 (Subject: Boston King)\n",
            "Comparing: hasparticipantrole -> child with hassex -> male (Subject: Boston King)\n",
            "Comparing: hasparticipantrole -> child with hasstatusgeneratedevent -> american revolutionary war, migration to nova scotia, immigration to sierra leone (Subject: Boston King)\n",
            "Comparing: hasparticipantrole -> child with hasvalue -> former slave, black loyalist, methodist minister, master carpenter (Subject: Boston King)\n",
            "Comparing: hasparticipantrole -> child with hasvalue -> methodist minister, master carpenter (Subject: Boston King)\n",
            "Comparing: hasparticipantrole -> child with hasinteragentrelationshiptype -> spouse (Subject: Boston King)\n",
            "Comparing: hasparticipantrole -> child with isrelationshipto -> violet (Subject: Boston King)\n",
            "Comparing: hasparticipantrole -> child with isrelationshipfrom -> boston king (Subject: Boston King)\n",
            "Comparing: hasparticipantrole -> child with hasparticipantroletype -> founder, teacher, missionary (Subject: Boston King)\n",
            "Comparing: hasparticipantrole -> child with roleprovidedby -> founding of freetown, sierra leone, teaching settlers, missionary work among sherbro people (Subject: Boston King)\n",
            "No match found for truth row: Boston King, Predicate: hasparticipantrole, Object: child\n",
            "Comparing: hasparticipantrole -> deceased person with haspreferrednamevariant -> boston king (Subject: Boston King)\n",
            "Comparing: hasparticipantrole -> deceased person with fullnameasstring -> boston king (Subject: Boston King)\n",
            "Comparing: hasparticipantrole -> deceased person with hassurnameasstring -> king (Subject: Boston King)\n",
            "Comparing: hasparticipantrole -> deceased person with hasfirstnameasstring -> boston (Subject: Boston King)\n",
            "Comparing: hasparticipantrole -> deceased person with hasdescription -> a former american slave and black loyalist, who gained freedom from the british and settled in nova scotia after the american revolutionary war. he later immigrated to sierra leone, where he helped found freetown and became the first methodist missionary to african indigenous people. (Subject: Boston King)\n",
            "Comparing: hasparticipantrole -> deceased person with referstoplaceoforigin -> south carolina (Subject: Boston King)\n",
            "Comparing: hasparticipantrole -> deceased person with has_race -> african-american (Subject: Boston King)\n",
            "Comparing: hasparticipantrole -> deceased person with hasbirthdate -> c. 1760 (Subject: Boston King)\n",
            "Comparing: hasparticipantrole -> deceased person with hasdeathdate -> 1802 (Subject: Boston King)\n",
            "Comparing: hasparticipantrole -> deceased person with hassex -> male (Subject: Boston King)\n",
            "Comparing: hasparticipantrole -> deceased person with hasstatusgeneratedevent -> american revolutionary war, migration to nova scotia, immigration to sierra leone (Subject: Boston King)\n",
            "Comparing: hasparticipantrole -> deceased person with hasvalue -> former slave, black loyalist, methodist minister, master carpenter (Subject: Boston King)\n",
            "Comparing: hasparticipantrole -> deceased person with hasvalue -> methodist minister, master carpenter (Subject: Boston King)\n",
            "Comparing: hasparticipantrole -> deceased person with hasinteragentrelationshiptype -> spouse (Subject: Boston King)\n",
            "Comparing: hasparticipantrole -> deceased person with isrelationshipto -> violet (Subject: Boston King)\n",
            "Comparing: hasparticipantrole -> deceased person with isrelationshipfrom -> boston king (Subject: Boston King)\n",
            "Comparing: hasparticipantrole -> deceased person with hasparticipantroletype -> founder, teacher, missionary (Subject: Boston King)\n",
            "Comparing: hasparticipantrole -> deceased person with roleprovidedby -> founding of freetown, sierra leone, teaching settlers, missionary work among sherbro people (Subject: Boston King)\n",
            "No match found for truth row: Boston King, Predicate: hasparticipantrole, Object: deceased person\n",
            "Comparing: hasdescriptiveoccupation -> slave with haspreferrednamevariant -> boston king (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> slave with fullnameasstring -> boston king (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hassurnameasstring -> king (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasfirstnameasstring -> boston (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasdescription -> a former american slave and black loyalist, who gained freedom from the british and settled in nova scotia after the american revolutionary war. he later immigrated to sierra leone, where he helped found freetown and became the first methodist missionary to african indigenous people. (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> slave with referstoplaceoforigin -> south carolina (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> slave with has_race -> african-american (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasbirthdate -> c. 1760 (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasdeathdate -> 1802 (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hassex -> male (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasstatusgeneratedevent -> american revolutionary war, migration to nova scotia, immigration to sierra leone (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasvalue -> former slave, black loyalist, methodist minister, master carpenter (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasvalue -> methodist minister, master carpenter (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasinteragentrelationshiptype -> spouse (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> slave with isrelationshipto -> violet (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> slave with isrelationshipfrom -> boston king (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasparticipantroletype -> founder, teacher, missionary (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> slave with roleprovidedby -> founding of freetown, sierra leone, teaching settlers, missionary work among sherbro people (Subject: Boston King)\n",
            "No match found for truth row: Boston King, Predicate: hasdescriptiveoccupation, Object: slave\n",
            "Comparing: hasdescriptiveoccupation -> loyalist (american revolution) with haspreferrednamevariant -> boston king (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> loyalist (american revolution) with fullnameasstring -> boston king (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> loyalist (american revolution) with hassurnameasstring -> king (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> loyalist (american revolution) with hasfirstnameasstring -> boston (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> loyalist (american revolution) with hasdescription -> a former american slave and black loyalist, who gained freedom from the british and settled in nova scotia after the american revolutionary war. he later immigrated to sierra leone, where he helped found freetown and became the first methodist missionary to african indigenous people. (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> loyalist (american revolution) with referstoplaceoforigin -> south carolina (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> loyalist (american revolution) with has_race -> african-american (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> loyalist (american revolution) with hasbirthdate -> c. 1760 (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> loyalist (american revolution) with hasdeathdate -> 1802 (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> loyalist (american revolution) with hassex -> male (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> loyalist (american revolution) with hasstatusgeneratedevent -> american revolutionary war, migration to nova scotia, immigration to sierra leone (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> loyalist (american revolution) with hasvalue -> former slave, black loyalist, methodist minister, master carpenter (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> loyalist (american revolution) with hasvalue -> methodist minister, master carpenter (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> loyalist (american revolution) with hasinteragentrelationshiptype -> spouse (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> loyalist (american revolution) with isrelationshipto -> violet (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> loyalist (american revolution) with isrelationshipfrom -> boston king (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> loyalist (american revolution) with hasparticipantroletype -> founder, teacher, missionary (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> loyalist (american revolution) with roleprovidedby -> founding of freetown, sierra leone, teaching settlers, missionary work among sherbro people (Subject: Boston King)\n",
            "No match found for truth row: Boston King, Predicate: hasdescriptiveoccupation, Object: loyalist (american revolution)\n",
            "Comparing: hasdescriptiveoccupation -> methodist episcopal lay leader with haspreferrednamevariant -> boston king (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> methodist episcopal lay leader with fullnameasstring -> boston king (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> methodist episcopal lay leader with hassurnameasstring -> king (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> methodist episcopal lay leader with hasfirstnameasstring -> boston (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> methodist episcopal lay leader with hasdescription -> a former american slave and black loyalist, who gained freedom from the british and settled in nova scotia after the american revolutionary war. he later immigrated to sierra leone, where he helped found freetown and became the first methodist missionary to african indigenous people. (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> methodist episcopal lay leader with referstoplaceoforigin -> south carolina (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> methodist episcopal lay leader with has_race -> african-american (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> methodist episcopal lay leader with hasbirthdate -> c. 1760 (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> methodist episcopal lay leader with hasdeathdate -> 1802 (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> methodist episcopal lay leader with hassex -> male (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> methodist episcopal lay leader with hasstatusgeneratedevent -> american revolutionary war, migration to nova scotia, immigration to sierra leone (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> methodist episcopal lay leader with hasvalue -> former slave, black loyalist, methodist minister, master carpenter (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> methodist episcopal lay leader with hasvalue -> methodist minister, master carpenter (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> methodist episcopal lay leader with hasinteragentrelationshiptype -> spouse (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> methodist episcopal lay leader with isrelationshipto -> violet (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> methodist episcopal lay leader with isrelationshipfrom -> boston king (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> methodist episcopal lay leader with hasparticipantroletype -> founder, teacher, missionary (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> methodist episcopal lay leader with roleprovidedby -> founding of freetown, sierra leone, teaching settlers, missionary work among sherbro people (Subject: Boston King)\n",
            "No match found for truth row: Boston King, Predicate: hasdescriptiveoccupation, Object: methodist episcopal lay leader\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with haspreferrednamevariant -> boston king (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with fullnameasstring -> boston king (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with hassurnameasstring -> king (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with hasfirstnameasstring -> boston (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with hasdescription -> a former american slave and black loyalist, who gained freedom from the british and settled in nova scotia after the american revolutionary war. he later immigrated to sierra leone, where he helped found freetown and became the first methodist missionary to african indigenous people. (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with referstoplaceoforigin -> south carolina (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with has_race -> african-american (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with hasbirthdate -> c. 1760 (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with hasdeathdate -> 1802 (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with hassex -> male (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with hasstatusgeneratedevent -> american revolutionary war, migration to nova scotia, immigration to sierra leone (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with hasvalue -> former slave, black loyalist, methodist minister, master carpenter (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with hasvalue -> methodist minister, master carpenter (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with hasinteragentrelationshiptype -> spouse (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with isrelationshipto -> violet (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with isrelationshipfrom -> boston king (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with hasparticipantroletype -> founder, teacher, missionary (Subject: Boston King)\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with roleprovidedby -> founding of freetown, sierra leone, teaching settlers, missionary work among sherbro people (Subject: Boston King)\n",
            "No match found for truth row: Boston King, Predicate: hasdescriptiveoccupation, Object: autobiographer / memoirist\n",
            "Comparing: hasfirstname -> boston with haspreferrednamevariant -> boston king (Subject: Boston King)\n",
            "Comparing: hasfirstname -> boston with fullnameasstring -> boston king (Subject: Boston King)\n",
            "Comparing: hasfirstname -> boston with hassurnameasstring -> king (Subject: Boston King)\n",
            "Comparing: hasfirstname -> boston with hasfirstnameasstring -> boston (Subject: Boston King)\n",
            "Comparing: hasfirstname -> boston with hasdescription -> a former american slave and black loyalist, who gained freedom from the british and settled in nova scotia after the american revolutionary war. he later immigrated to sierra leone, where he helped found freetown and became the first methodist missionary to african indigenous people. (Subject: Boston King)\n",
            "Comparing: hasfirstname -> boston with referstoplaceoforigin -> south carolina (Subject: Boston King)\n",
            "Comparing: hasfirstname -> boston with has_race -> african-american (Subject: Boston King)\n",
            "Comparing: hasfirstname -> boston with hasbirthdate -> c. 1760 (Subject: Boston King)\n",
            "Comparing: hasfirstname -> boston with hasdeathdate -> 1802 (Subject: Boston King)\n",
            "Comparing: hasfirstname -> boston with hassex -> male (Subject: Boston King)\n",
            "Comparing: hasfirstname -> boston with hasstatusgeneratedevent -> american revolutionary war, migration to nova scotia, immigration to sierra leone (Subject: Boston King)\n",
            "Comparing: hasfirstname -> boston with hasvalue -> former slave, black loyalist, methodist minister, master carpenter (Subject: Boston King)\n",
            "Comparing: hasfirstname -> boston with hasvalue -> methodist minister, master carpenter (Subject: Boston King)\n",
            "Comparing: hasfirstname -> boston with hasinteragentrelationshiptype -> spouse (Subject: Boston King)\n",
            "Comparing: hasfirstname -> boston with isrelationshipto -> violet (Subject: Boston King)\n",
            "Comparing: hasfirstname -> boston with isrelationshipfrom -> boston king (Subject: Boston King)\n",
            "Comparing: hasfirstname -> boston with hasparticipantroletype -> founder, teacher, missionary (Subject: Boston King)\n",
            "Comparing: hasfirstname -> boston with roleprovidedby -> founding of freetown, sierra leone, teaching settlers, missionary work among sherbro people (Subject: Boston King)\n",
            "No match found for truth row: Boston King, Predicate: hasfirstname, Object: boston\n",
            "Comparing: hassurname -> king with haspreferrednamevariant -> boston king (Subject: Boston King)\n",
            "Comparing: hassurname -> king with fullnameasstring -> boston king (Subject: Boston King)\n",
            "Comparing: hassurname -> king with hassurnameasstring -> king (Subject: Boston King)\n",
            "Comparing: hassurname -> king with hasfirstnameasstring -> boston (Subject: Boston King)\n",
            "Comparing: hassurname -> king with hasdescription -> a former american slave and black loyalist, who gained freedom from the british and settled in nova scotia after the american revolutionary war. he later immigrated to sierra leone, where he helped found freetown and became the first methodist missionary to african indigenous people. (Subject: Boston King)\n",
            "Comparing: hassurname -> king with referstoplaceoforigin -> south carolina (Subject: Boston King)\n",
            "Comparing: hassurname -> king with has_race -> african-american (Subject: Boston King)\n",
            "Comparing: hassurname -> king with hasbirthdate -> c. 1760 (Subject: Boston King)\n",
            "Comparing: hassurname -> king with hasdeathdate -> 1802 (Subject: Boston King)\n",
            "Comparing: hassurname -> king with hassex -> male (Subject: Boston King)\n",
            "Comparing: hassurname -> king with hasstatusgeneratedevent -> american revolutionary war, migration to nova scotia, immigration to sierra leone (Subject: Boston King)\n",
            "Comparing: hassurname -> king with hasvalue -> former slave, black loyalist, methodist minister, master carpenter (Subject: Boston King)\n",
            "Comparing: hassurname -> king with hasvalue -> methodist minister, master carpenter (Subject: Boston King)\n",
            "Comparing: hassurname -> king with hasinteragentrelationshiptype -> spouse (Subject: Boston King)\n",
            "Comparing: hassurname -> king with isrelationshipto -> violet (Subject: Boston King)\n",
            "Comparing: hassurname -> king with isrelationshipfrom -> boston king (Subject: Boston King)\n",
            "Comparing: hassurname -> king with hasparticipantroletype -> founder, teacher, missionary (Subject: Boston King)\n",
            "Comparing: hassurname -> king with roleprovidedby -> founding of freetown, sierra leone, teaching settlers, missionary work among sherbro people (Subject: Boston King)\n",
            "No match found for truth row: Boston King, Predicate: hassurname, Object: king\n",
            "Comparing: hasname -> boston king with haspreferrednamevariant -> boston king (Subject: Boston King)\n",
            "Comparing: hasname -> boston king with fullnameasstring -> boston king (Subject: Boston King)\n",
            "Comparing: hasname -> boston king with hassurnameasstring -> king (Subject: Boston King)\n",
            "Comparing: hasname -> boston king with hasfirstnameasstring -> boston (Subject: Boston King)\n",
            "Comparing: hasname -> boston king with hasdescription -> a former american slave and black loyalist, who gained freedom from the british and settled in nova scotia after the american revolutionary war. he later immigrated to sierra leone, where he helped found freetown and became the first methodist missionary to african indigenous people. (Subject: Boston King)\n",
            "Comparing: hasname -> boston king with referstoplaceoforigin -> south carolina (Subject: Boston King)\n",
            "Comparing: hasname -> boston king with has_race -> african-american (Subject: Boston King)\n",
            "Comparing: hasname -> boston king with hasbirthdate -> c. 1760 (Subject: Boston King)\n",
            "Comparing: hasname -> boston king with hasdeathdate -> 1802 (Subject: Boston King)\n",
            "Comparing: hasname -> boston king with hassex -> male (Subject: Boston King)\n",
            "Comparing: hasname -> boston king with hasstatusgeneratedevent -> american revolutionary war, migration to nova scotia, immigration to sierra leone (Subject: Boston King)\n",
            "Comparing: hasname -> boston king with hasvalue -> former slave, black loyalist, methodist minister, master carpenter (Subject: Boston King)\n",
            "Comparing: hasname -> boston king with hasvalue -> methodist minister, master carpenter (Subject: Boston King)\n",
            "Comparing: hasname -> boston king with hasinteragentrelationshiptype -> spouse (Subject: Boston King)\n",
            "Comparing: hasname -> boston king with isrelationshipto -> violet (Subject: Boston King)\n",
            "Comparing: hasname -> boston king with isrelationshipfrom -> boston king (Subject: Boston King)\n",
            "Comparing: hasname -> boston king with hasparticipantroletype -> founder, teacher, missionary (Subject: Boston King)\n",
            "Comparing: hasname -> boston king with roleprovidedby -> founding of freetown, sierra leone, teaching settlers, missionary work among sherbro people (Subject: Boston King)\n",
            "No match found for truth row: Boston King, Predicate: hasname, Object: boston king\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. resettled in nova scotia after the american revolutionary war, boston and his wife emigrated to sierra leone in 1792, in part to bring their christian faith. with haspreferrednamevariant -> boston king (Subject: Boston King)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. resettled in nova scotia after the american revolutionary war, boston and his wife emigrated to sierra leone in 1792, in part to bring their christian faith. with fullnameasstring -> boston king (Subject: Boston King)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. resettled in nova scotia after the american revolutionary war, boston and his wife emigrated to sierra leone in 1792, in part to bring their christian faith. with hassurnameasstring -> king (Subject: Boston King)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. resettled in nova scotia after the american revolutionary war, boston and his wife emigrated to sierra leone in 1792, in part to bring their christian faith. with hasfirstnameasstring -> boston (Subject: Boston King)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. resettled in nova scotia after the american revolutionary war, boston and his wife emigrated to sierra leone in 1792, in part to bring their christian faith. with hasdescription -> a former american slave and black loyalist, who gained freedom from the british and settled in nova scotia after the american revolutionary war. he later immigrated to sierra leone, where he helped found freetown and became the first methodist missionary to african indigenous people. (Subject: Boston King)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. resettled in nova scotia after the american revolutionary war, boston and his wife emigrated to sierra leone in 1792, in part to bring their christian faith. with referstoplaceoforigin -> south carolina (Subject: Boston King)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. resettled in nova scotia after the american revolutionary war, boston and his wife emigrated to sierra leone in 1792, in part to bring their christian faith. with has_race -> african-american (Subject: Boston King)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. resettled in nova scotia after the american revolutionary war, boston and his wife emigrated to sierra leone in 1792, in part to bring their christian faith. with hasbirthdate -> c. 1760 (Subject: Boston King)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. resettled in nova scotia after the american revolutionary war, boston and his wife emigrated to sierra leone in 1792, in part to bring their christian faith. with hasdeathdate -> 1802 (Subject: Boston King)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. resettled in nova scotia after the american revolutionary war, boston and his wife emigrated to sierra leone in 1792, in part to bring their christian faith. with hassex -> male (Subject: Boston King)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. resettled in nova scotia after the american revolutionary war, boston and his wife emigrated to sierra leone in 1792, in part to bring their christian faith. with hasstatusgeneratedevent -> american revolutionary war, migration to nova scotia, immigration to sierra leone (Subject: Boston King)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. resettled in nova scotia after the american revolutionary war, boston and his wife emigrated to sierra leone in 1792, in part to bring their christian faith. with hasvalue -> former slave, black loyalist, methodist minister, master carpenter (Subject: Boston King)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. resettled in nova scotia after the american revolutionary war, boston and his wife emigrated to sierra leone in 1792, in part to bring their christian faith. with hasvalue -> methodist minister, master carpenter (Subject: Boston King)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. resettled in nova scotia after the american revolutionary war, boston and his wife emigrated to sierra leone in 1792, in part to bring their christian faith. with hasinteragentrelationshiptype -> spouse (Subject: Boston King)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. resettled in nova scotia after the american revolutionary war, boston and his wife emigrated to sierra leone in 1792, in part to bring their christian faith. with isrelationshipto -> violet (Subject: Boston King)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. resettled in nova scotia after the american revolutionary war, boston and his wife emigrated to sierra leone in 1792, in part to bring their christian faith. with isrelationshipfrom -> boston king (Subject: Boston King)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. resettled in nova scotia after the american revolutionary war, boston and his wife emigrated to sierra leone in 1792, in part to bring their christian faith. with hasparticipantroletype -> founder, teacher, missionary (Subject: Boston King)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. resettled in nova scotia after the american revolutionary war, boston and his wife emigrated to sierra leone in 1792, in part to bring their christian faith. with roleprovidedby -> founding of freetown, sierra leone, teaching settlers, missionary work among sherbro people (Subject: Boston King)\n",
            "No match found for truth row: Boston King, Predicate: hasdescription, Object: born enslaved. free before 13th amendment. resettled in nova scotia after the american revolutionary war, boston and his wife emigrated to sierra leone in 1792, in part to bring their christian faith.\n",
            "\n",
            "Processing truth file: Mary_Fields.tsv\n",
            "Truth DataFrame:\n",
            "                         0                         1  \\\n",
            "0   Stagecoach Mary Fields               instance of   \n",
            "1   Stagecoach Mary Fields                    hasSex   \n",
            "2   Stagecoach Mary Fields        hasParticipantRole   \n",
            "3   Stagecoach Mary Fields        hasParticipantRole   \n",
            "4   Stagecoach Mary Fields  hasDescriptiveOccupation   \n",
            "5   Stagecoach Mary Fields  hasDescriptiveOccupation   \n",
            "6   Stagecoach Mary Fields  hasDescriptiveOccupation   \n",
            "7   Stagecoach Mary Fields              hasFirstName   \n",
            "8   Stagecoach Mary Fields                hasSurname   \n",
            "9   Stagecoach Mary Fields                   hasName   \n",
            "10  Stagecoach Mary Fields            hasDescription   \n",
            "\n",
            "                                                    2  \n",
            "0                                              Person  \n",
            "1                                              Female  \n",
            "2                                               Child  \n",
            "3                                     Deceased Person  \n",
            "4                                               Slave  \n",
            "5                              Frontiersman / Pioneer  \n",
            "6                                     Postal Official  \n",
            "7                                     Stagecoach Mary  \n",
            "8                                              Fields  \n",
            "9                              Stagecoach Mary Fields  \n",
            "10  Born Enslaved. First African American woman to...  \n",
            "\n",
            "Found matching generated file: Mary_Fields_triples.tsv\n",
            "Generated DataFrame before transformation:\n",
            "                Subject                Predicate  \\\n",
            "1   Mary_Fields_triples  hasPreferredNameVariant   \n",
            "2   Mary_Fields_triples         fullNameAsString   \n",
            "3   Mary_Fields_triples       hasSurnameAsString   \n",
            "4   Mary_Fields_triples     hasFirstnameAsString   \n",
            "5   Mary_Fields_triples           hasDescription   \n",
            "6   Mary_Fields_triples    refersToPlaceOfOrigin   \n",
            "7   Mary_Fields_triples                 has_Race   \n",
            "8   Mary_Fields_triples             hasDeathDate   \n",
            "9   Mary_Fields_triples                   hasSex   \n",
            "10  Mary_Fields_triples  hasStatusGeneratedEvent   \n",
            "11  Mary_Fields_triples                 hasValue   \n",
            "12  Mary_Fields_triples                 hasValue   \n",
            "13  Mary_Fields_triples         isRelationshipTo   \n",
            "14  Mary_Fields_triples       isRelationshipFrom   \n",
            "15  Mary_Fields_triples   hasParticipantRoleType   \n",
            "16  Mary_Fields_triples           roleProvidedBy   \n",
            "\n",
            "                                               Object  \n",
            "1                                     Stagecoach Mary  \n",
            "2                                     Stagecoach Mary  \n",
            "3                                              Fields  \n",
            "4                                                Mary  \n",
            "5   First African-American woman to work for the U...  \n",
            "6                                             Montana  \n",
            "7                                    African-American  \n",
            "8                                                1914  \n",
            "9                                              Female  \n",
            "10                     Worked as a Star Route Carrier  \n",
            "11                        Public Figure, Mail Carrier  \n",
            "12                   Star Route Carrier, Tavern Owner  \n",
            "13                                Mother Mary Amadeus  \n",
            "14                                    Stagecoach Mary  \n",
            "15                                             Worker  \n",
            "16                      Worked at St. Peter's Mission  \n",
            "Generated DataFrame after transformation:\n",
            "        Subject                Predicate  \\\n",
            "1   Mary Fields  haspreferrednamevariant   \n",
            "2   Mary Fields         fullnameasstring   \n",
            "3   Mary Fields       hassurnameasstring   \n",
            "4   Mary Fields     hasfirstnameasstring   \n",
            "5   Mary Fields           hasdescription   \n",
            "6   Mary Fields    referstoplaceoforigin   \n",
            "7   Mary Fields                 has_race   \n",
            "8   Mary Fields             hasdeathdate   \n",
            "9   Mary Fields                   hassex   \n",
            "10  Mary Fields  hasstatusgeneratedevent   \n",
            "11  Mary Fields                 hasvalue   \n",
            "12  Mary Fields                 hasvalue   \n",
            "13  Mary Fields         isrelationshipto   \n",
            "14  Mary Fields       isrelationshipfrom   \n",
            "15  Mary Fields   hasparticipantroletype   \n",
            "16  Mary Fields           roleprovidedby   \n",
            "\n",
            "                                               Object  \n",
            "1                                     stagecoach mary  \n",
            "2                                     stagecoach mary  \n",
            "3                                              fields  \n",
            "4                                                mary  \n",
            "5   first african-american woman to work for the u...  \n",
            "6                                             montana  \n",
            "7                                    african-american  \n",
            "8                                                1914  \n",
            "9                                              female  \n",
            "10                     worked as a star route carrier  \n",
            "11                        public figure, mail carrier  \n",
            "12                   star route carrier, tavern owner  \n",
            "13                                mother mary amadeus  \n",
            "14                                    stagecoach mary  \n",
            "15                                             worker  \n",
            "16                      worked at st. peter's mission  \n",
            "Comparing: instance of -> person with haspreferrednamevariant -> stagecoach mary (Subject: Mary Fields)\n",
            "Comparing: instance of -> person with fullnameasstring -> stagecoach mary (Subject: Mary Fields)\n",
            "Comparing: instance of -> person with hassurnameasstring -> fields (Subject: Mary Fields)\n",
            "Comparing: instance of -> person with hasfirstnameasstring -> mary (Subject: Mary Fields)\n",
            "Comparing: instance of -> person with hasdescription -> first african-american woman to work for the u.s. postal service, known for her fearless demeanor and reliability. (Subject: Mary Fields)\n",
            "Comparing: instance of -> person with referstoplaceoforigin -> montana (Subject: Mary Fields)\n",
            "Comparing: instance of -> person with has_race -> african-american (Subject: Mary Fields)\n",
            "Comparing: instance of -> person with hasdeathdate -> 1914 (Subject: Mary Fields)\n",
            "Comparing: instance of -> person with hassex -> female (Subject: Mary Fields)\n",
            "Comparing: instance of -> person with hasstatusgeneratedevent -> worked as a star route carrier (Subject: Mary Fields)\n",
            "Comparing: instance of -> person with hasvalue -> public figure, mail carrier (Subject: Mary Fields)\n",
            "Comparing: instance of -> person with hasvalue -> star route carrier, tavern owner (Subject: Mary Fields)\n",
            "Comparing: instance of -> person with isrelationshipto -> mother mary amadeus (Subject: Mary Fields)\n",
            "Comparing: instance of -> person with isrelationshipfrom -> stagecoach mary (Subject: Mary Fields)\n",
            "Comparing: instance of -> person with hasparticipantroletype -> worker (Subject: Mary Fields)\n",
            "Comparing: instance of -> person with roleprovidedby -> worked at st. peter's mission (Subject: Mary Fields)\n",
            "No match found for truth row: Stagecoach Mary Fields, Predicate: instance of, Object: person\n",
            "Comparing: hassex -> female with haspreferrednamevariant -> stagecoach mary (Subject: Mary Fields)\n",
            "Comparing: hassex -> female with fullnameasstring -> stagecoach mary (Subject: Mary Fields)\n",
            "Comparing: hassex -> female with hassurnameasstring -> fields (Subject: Mary Fields)\n",
            "Comparing: hassex -> female with hasfirstnameasstring -> mary (Subject: Mary Fields)\n",
            "Comparing: hassex -> female with hasdescription -> first african-american woman to work for the u.s. postal service, known for her fearless demeanor and reliability. (Subject: Mary Fields)\n",
            "Comparing: hassex -> female with referstoplaceoforigin -> montana (Subject: Mary Fields)\n",
            "Comparing: hassex -> female with has_race -> african-american (Subject: Mary Fields)\n",
            "Comparing: hassex -> female with hasdeathdate -> 1914 (Subject: Mary Fields)\n",
            "Comparing: hassex -> female with hassex -> female (Subject: Mary Fields)\n",
            "Match found for truth row Stagecoach Mary Fields with generated row Mary Fields\n",
            "Comparing: hassex -> female with hasstatusgeneratedevent -> worked as a star route carrier (Subject: Mary Fields)\n",
            "Comparing: hassex -> female with hasvalue -> public figure, mail carrier (Subject: Mary Fields)\n",
            "Comparing: hassex -> female with hasvalue -> star route carrier, tavern owner (Subject: Mary Fields)\n",
            "Comparing: hassex -> female with isrelationshipto -> mother mary amadeus (Subject: Mary Fields)\n",
            "Comparing: hassex -> female with isrelationshipfrom -> stagecoach mary (Subject: Mary Fields)\n",
            "Comparing: hassex -> female with hasparticipantroletype -> worker (Subject: Mary Fields)\n",
            "Comparing: hassex -> female with roleprovidedby -> worked at st. peter's mission (Subject: Mary Fields)\n",
            "Comparing: hasparticipantrole -> child with haspreferrednamevariant -> stagecoach mary (Subject: Mary Fields)\n",
            "Comparing: hasparticipantrole -> child with fullnameasstring -> stagecoach mary (Subject: Mary Fields)\n",
            "Comparing: hasparticipantrole -> child with hassurnameasstring -> fields (Subject: Mary Fields)\n",
            "Comparing: hasparticipantrole -> child with hasfirstnameasstring -> mary (Subject: Mary Fields)\n",
            "Comparing: hasparticipantrole -> child with hasdescription -> first african-american woman to work for the u.s. postal service, known for her fearless demeanor and reliability. (Subject: Mary Fields)\n",
            "Comparing: hasparticipantrole -> child with referstoplaceoforigin -> montana (Subject: Mary Fields)\n",
            "Comparing: hasparticipantrole -> child with has_race -> african-american (Subject: Mary Fields)\n",
            "Comparing: hasparticipantrole -> child with hasdeathdate -> 1914 (Subject: Mary Fields)\n",
            "Comparing: hasparticipantrole -> child with hassex -> female (Subject: Mary Fields)\n",
            "Comparing: hasparticipantrole -> child with hasstatusgeneratedevent -> worked as a star route carrier (Subject: Mary Fields)\n",
            "Comparing: hasparticipantrole -> child with hasvalue -> public figure, mail carrier (Subject: Mary Fields)\n",
            "Comparing: hasparticipantrole -> child with hasvalue -> star route carrier, tavern owner (Subject: Mary Fields)\n",
            "Comparing: hasparticipantrole -> child with isrelationshipto -> mother mary amadeus (Subject: Mary Fields)\n",
            "Comparing: hasparticipantrole -> child with isrelationshipfrom -> stagecoach mary (Subject: Mary Fields)\n",
            "Comparing: hasparticipantrole -> child with hasparticipantroletype -> worker (Subject: Mary Fields)\n",
            "Comparing: hasparticipantrole -> child with roleprovidedby -> worked at st. peter's mission (Subject: Mary Fields)\n",
            "No match found for truth row: Stagecoach Mary Fields, Predicate: hasparticipantrole, Object: child\n",
            "Comparing: hasparticipantrole -> deceased person with haspreferrednamevariant -> stagecoach mary (Subject: Mary Fields)\n",
            "Comparing: hasparticipantrole -> deceased person with fullnameasstring -> stagecoach mary (Subject: Mary Fields)\n",
            "Comparing: hasparticipantrole -> deceased person with hassurnameasstring -> fields (Subject: Mary Fields)\n",
            "Comparing: hasparticipantrole -> deceased person with hasfirstnameasstring -> mary (Subject: Mary Fields)\n",
            "Comparing: hasparticipantrole -> deceased person with hasdescription -> first african-american woman to work for the u.s. postal service, known for her fearless demeanor and reliability. (Subject: Mary Fields)\n",
            "Comparing: hasparticipantrole -> deceased person with referstoplaceoforigin -> montana (Subject: Mary Fields)\n",
            "Comparing: hasparticipantrole -> deceased person with has_race -> african-american (Subject: Mary Fields)\n",
            "Comparing: hasparticipantrole -> deceased person with hasdeathdate -> 1914 (Subject: Mary Fields)\n",
            "Comparing: hasparticipantrole -> deceased person with hassex -> female (Subject: Mary Fields)\n",
            "Comparing: hasparticipantrole -> deceased person with hasstatusgeneratedevent -> worked as a star route carrier (Subject: Mary Fields)\n",
            "Comparing: hasparticipantrole -> deceased person with hasvalue -> public figure, mail carrier (Subject: Mary Fields)\n",
            "Comparing: hasparticipantrole -> deceased person with hasvalue -> star route carrier, tavern owner (Subject: Mary Fields)\n",
            "Comparing: hasparticipantrole -> deceased person with isrelationshipto -> mother mary amadeus (Subject: Mary Fields)\n",
            "Comparing: hasparticipantrole -> deceased person with isrelationshipfrom -> stagecoach mary (Subject: Mary Fields)\n",
            "Comparing: hasparticipantrole -> deceased person with hasparticipantroletype -> worker (Subject: Mary Fields)\n",
            "Comparing: hasparticipantrole -> deceased person with roleprovidedby -> worked at st. peter's mission (Subject: Mary Fields)\n",
            "No match found for truth row: Stagecoach Mary Fields, Predicate: hasparticipantrole, Object: deceased person\n",
            "Comparing: hasdescriptiveoccupation -> slave with haspreferrednamevariant -> stagecoach mary (Subject: Mary Fields)\n",
            "Comparing: hasdescriptiveoccupation -> slave with fullnameasstring -> stagecoach mary (Subject: Mary Fields)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hassurnameasstring -> fields (Subject: Mary Fields)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasfirstnameasstring -> mary (Subject: Mary Fields)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasdescription -> first african-american woman to work for the u.s. postal service, known for her fearless demeanor and reliability. (Subject: Mary Fields)\n",
            "Comparing: hasdescriptiveoccupation -> slave with referstoplaceoforigin -> montana (Subject: Mary Fields)\n",
            "Comparing: hasdescriptiveoccupation -> slave with has_race -> african-american (Subject: Mary Fields)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasdeathdate -> 1914 (Subject: Mary Fields)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hassex -> female (Subject: Mary Fields)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasstatusgeneratedevent -> worked as a star route carrier (Subject: Mary Fields)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasvalue -> public figure, mail carrier (Subject: Mary Fields)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasvalue -> star route carrier, tavern owner (Subject: Mary Fields)\n",
            "Comparing: hasdescriptiveoccupation -> slave with isrelationshipto -> mother mary amadeus (Subject: Mary Fields)\n",
            "Comparing: hasdescriptiveoccupation -> slave with isrelationshipfrom -> stagecoach mary (Subject: Mary Fields)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasparticipantroletype -> worker (Subject: Mary Fields)\n",
            "Comparing: hasdescriptiveoccupation -> slave with roleprovidedby -> worked at st. peter's mission (Subject: Mary Fields)\n",
            "No match found for truth row: Stagecoach Mary Fields, Predicate: hasdescriptiveoccupation, Object: slave\n",
            "Comparing: hasdescriptiveoccupation -> frontiersman / pioneer with haspreferrednamevariant -> stagecoach mary (Subject: Mary Fields)\n",
            "Comparing: hasdescriptiveoccupation -> frontiersman / pioneer with fullnameasstring -> stagecoach mary (Subject: Mary Fields)\n",
            "Comparing: hasdescriptiveoccupation -> frontiersman / pioneer with hassurnameasstring -> fields (Subject: Mary Fields)\n",
            "Comparing: hasdescriptiveoccupation -> frontiersman / pioneer with hasfirstnameasstring -> mary (Subject: Mary Fields)\n",
            "Comparing: hasdescriptiveoccupation -> frontiersman / pioneer with hasdescription -> first african-american woman to work for the u.s. postal service, known for her fearless demeanor and reliability. (Subject: Mary Fields)\n",
            "Comparing: hasdescriptiveoccupation -> frontiersman / pioneer with referstoplaceoforigin -> montana (Subject: Mary Fields)\n",
            "Comparing: hasdescriptiveoccupation -> frontiersman / pioneer with has_race -> african-american (Subject: Mary Fields)\n",
            "Comparing: hasdescriptiveoccupation -> frontiersman / pioneer with hasdeathdate -> 1914 (Subject: Mary Fields)\n",
            "Comparing: hasdescriptiveoccupation -> frontiersman / pioneer with hassex -> female (Subject: Mary Fields)\n",
            "Comparing: hasdescriptiveoccupation -> frontiersman / pioneer with hasstatusgeneratedevent -> worked as a star route carrier (Subject: Mary Fields)\n",
            "Comparing: hasdescriptiveoccupation -> frontiersman / pioneer with hasvalue -> public figure, mail carrier (Subject: Mary Fields)\n",
            "Comparing: hasdescriptiveoccupation -> frontiersman / pioneer with hasvalue -> star route carrier, tavern owner (Subject: Mary Fields)\n",
            "Comparing: hasdescriptiveoccupation -> frontiersman / pioneer with isrelationshipto -> mother mary amadeus (Subject: Mary Fields)\n",
            "Comparing: hasdescriptiveoccupation -> frontiersman / pioneer with isrelationshipfrom -> stagecoach mary (Subject: Mary Fields)\n",
            "Comparing: hasdescriptiveoccupation -> frontiersman / pioneer with hasparticipantroletype -> worker (Subject: Mary Fields)\n",
            "Comparing: hasdescriptiveoccupation -> frontiersman / pioneer with roleprovidedby -> worked at st. peter's mission (Subject: Mary Fields)\n",
            "No match found for truth row: Stagecoach Mary Fields, Predicate: hasdescriptiveoccupation, Object: frontiersman / pioneer\n",
            "Comparing: hasdescriptiveoccupation -> postal official with haspreferrednamevariant -> stagecoach mary (Subject: Mary Fields)\n",
            "Comparing: hasdescriptiveoccupation -> postal official with fullnameasstring -> stagecoach mary (Subject: Mary Fields)\n",
            "Comparing: hasdescriptiveoccupation -> postal official with hassurnameasstring -> fields (Subject: Mary Fields)\n",
            "Comparing: hasdescriptiveoccupation -> postal official with hasfirstnameasstring -> mary (Subject: Mary Fields)\n",
            "Comparing: hasdescriptiveoccupation -> postal official with hasdescription -> first african-american woman to work for the u.s. postal service, known for her fearless demeanor and reliability. (Subject: Mary Fields)\n",
            "Comparing: hasdescriptiveoccupation -> postal official with referstoplaceoforigin -> montana (Subject: Mary Fields)\n",
            "Comparing: hasdescriptiveoccupation -> postal official with has_race -> african-american (Subject: Mary Fields)\n",
            "Comparing: hasdescriptiveoccupation -> postal official with hasdeathdate -> 1914 (Subject: Mary Fields)\n",
            "Comparing: hasdescriptiveoccupation -> postal official with hassex -> female (Subject: Mary Fields)\n",
            "Comparing: hasdescriptiveoccupation -> postal official with hasstatusgeneratedevent -> worked as a star route carrier (Subject: Mary Fields)\n",
            "Comparing: hasdescriptiveoccupation -> postal official with hasvalue -> public figure, mail carrier (Subject: Mary Fields)\n",
            "Comparing: hasdescriptiveoccupation -> postal official with hasvalue -> star route carrier, tavern owner (Subject: Mary Fields)\n",
            "Comparing: hasdescriptiveoccupation -> postal official with isrelationshipto -> mother mary amadeus (Subject: Mary Fields)\n",
            "Comparing: hasdescriptiveoccupation -> postal official with isrelationshipfrom -> stagecoach mary (Subject: Mary Fields)\n",
            "Comparing: hasdescriptiveoccupation -> postal official with hasparticipantroletype -> worker (Subject: Mary Fields)\n",
            "Comparing: hasdescriptiveoccupation -> postal official with roleprovidedby -> worked at st. peter's mission (Subject: Mary Fields)\n",
            "No match found for truth row: Stagecoach Mary Fields, Predicate: hasdescriptiveoccupation, Object: postal official\n",
            "Comparing: hasfirstname -> stagecoach mary with haspreferrednamevariant -> stagecoach mary (Subject: Mary Fields)\n",
            "Comparing: hasfirstname -> stagecoach mary with fullnameasstring -> stagecoach mary (Subject: Mary Fields)\n",
            "Comparing: hasfirstname -> stagecoach mary with hassurnameasstring -> fields (Subject: Mary Fields)\n",
            "Comparing: hasfirstname -> stagecoach mary with hasfirstnameasstring -> mary (Subject: Mary Fields)\n",
            "Comparing: hasfirstname -> stagecoach mary with hasdescription -> first african-american woman to work for the u.s. postal service, known for her fearless demeanor and reliability. (Subject: Mary Fields)\n",
            "Comparing: hasfirstname -> stagecoach mary with referstoplaceoforigin -> montana (Subject: Mary Fields)\n",
            "Comparing: hasfirstname -> stagecoach mary with has_race -> african-american (Subject: Mary Fields)\n",
            "Comparing: hasfirstname -> stagecoach mary with hasdeathdate -> 1914 (Subject: Mary Fields)\n",
            "Comparing: hasfirstname -> stagecoach mary with hassex -> female (Subject: Mary Fields)\n",
            "Comparing: hasfirstname -> stagecoach mary with hasstatusgeneratedevent -> worked as a star route carrier (Subject: Mary Fields)\n",
            "Comparing: hasfirstname -> stagecoach mary with hasvalue -> public figure, mail carrier (Subject: Mary Fields)\n",
            "Comparing: hasfirstname -> stagecoach mary with hasvalue -> star route carrier, tavern owner (Subject: Mary Fields)\n",
            "Comparing: hasfirstname -> stagecoach mary with isrelationshipto -> mother mary amadeus (Subject: Mary Fields)\n",
            "Comparing: hasfirstname -> stagecoach mary with isrelationshipfrom -> stagecoach mary (Subject: Mary Fields)\n",
            "Comparing: hasfirstname -> stagecoach mary with hasparticipantroletype -> worker (Subject: Mary Fields)\n",
            "Comparing: hasfirstname -> stagecoach mary with roleprovidedby -> worked at st. peter's mission (Subject: Mary Fields)\n",
            "No match found for truth row: Stagecoach Mary Fields, Predicate: hasfirstname, Object: stagecoach mary\n",
            "Comparing: hassurname -> fields with haspreferrednamevariant -> stagecoach mary (Subject: Mary Fields)\n",
            "Comparing: hassurname -> fields with fullnameasstring -> stagecoach mary (Subject: Mary Fields)\n",
            "Comparing: hassurname -> fields with hassurnameasstring -> fields (Subject: Mary Fields)\n",
            "Comparing: hassurname -> fields with hasfirstnameasstring -> mary (Subject: Mary Fields)\n",
            "Comparing: hassurname -> fields with hasdescription -> first african-american woman to work for the u.s. postal service, known for her fearless demeanor and reliability. (Subject: Mary Fields)\n",
            "Comparing: hassurname -> fields with referstoplaceoforigin -> montana (Subject: Mary Fields)\n",
            "Comparing: hassurname -> fields with has_race -> african-american (Subject: Mary Fields)\n",
            "Comparing: hassurname -> fields with hasdeathdate -> 1914 (Subject: Mary Fields)\n",
            "Comparing: hassurname -> fields with hassex -> female (Subject: Mary Fields)\n",
            "Comparing: hassurname -> fields with hasstatusgeneratedevent -> worked as a star route carrier (Subject: Mary Fields)\n",
            "Comparing: hassurname -> fields with hasvalue -> public figure, mail carrier (Subject: Mary Fields)\n",
            "Comparing: hassurname -> fields with hasvalue -> star route carrier, tavern owner (Subject: Mary Fields)\n",
            "Comparing: hassurname -> fields with isrelationshipto -> mother mary amadeus (Subject: Mary Fields)\n",
            "Comparing: hassurname -> fields with isrelationshipfrom -> stagecoach mary (Subject: Mary Fields)\n",
            "Comparing: hassurname -> fields with hasparticipantroletype -> worker (Subject: Mary Fields)\n",
            "Comparing: hassurname -> fields with roleprovidedby -> worked at st. peter's mission (Subject: Mary Fields)\n",
            "No match found for truth row: Stagecoach Mary Fields, Predicate: hassurname, Object: fields\n",
            "Comparing: hasname -> stagecoach mary fields with haspreferrednamevariant -> stagecoach mary (Subject: Mary Fields)\n",
            "Comparing: hasname -> stagecoach mary fields with fullnameasstring -> stagecoach mary (Subject: Mary Fields)\n",
            "Comparing: hasname -> stagecoach mary fields with hassurnameasstring -> fields (Subject: Mary Fields)\n",
            "Comparing: hasname -> stagecoach mary fields with hasfirstnameasstring -> mary (Subject: Mary Fields)\n",
            "Comparing: hasname -> stagecoach mary fields with hasdescription -> first african-american woman to work for the u.s. postal service, known for her fearless demeanor and reliability. (Subject: Mary Fields)\n",
            "Comparing: hasname -> stagecoach mary fields with referstoplaceoforigin -> montana (Subject: Mary Fields)\n",
            "Comparing: hasname -> stagecoach mary fields with has_race -> african-american (Subject: Mary Fields)\n",
            "Comparing: hasname -> stagecoach mary fields with hasdeathdate -> 1914 (Subject: Mary Fields)\n",
            "Comparing: hasname -> stagecoach mary fields with hassex -> female (Subject: Mary Fields)\n",
            "Comparing: hasname -> stagecoach mary fields with hasstatusgeneratedevent -> worked as a star route carrier (Subject: Mary Fields)\n",
            "Comparing: hasname -> stagecoach mary fields with hasvalue -> public figure, mail carrier (Subject: Mary Fields)\n",
            "Comparing: hasname -> stagecoach mary fields with hasvalue -> star route carrier, tavern owner (Subject: Mary Fields)\n",
            "Comparing: hasname -> stagecoach mary fields with isrelationshipto -> mother mary amadeus (Subject: Mary Fields)\n",
            "Comparing: hasname -> stagecoach mary fields with isrelationshipfrom -> stagecoach mary (Subject: Mary Fields)\n",
            "Comparing: hasname -> stagecoach mary fields with hasparticipantroletype -> worker (Subject: Mary Fields)\n",
            "Comparing: hasname -> stagecoach mary fields with roleprovidedby -> worked at st. peter's mission (Subject: Mary Fields)\n",
            "No match found for truth row: Stagecoach Mary Fields, Predicate: hasname, Object: stagecoach mary fields\n",
            "Comparing: hasdescription -> born enslaved. first african american woman to drive a coach for the postal service. became a business owner at age 70. a frontierswoman who drank, smoke, and got in a shootout. with haspreferrednamevariant -> stagecoach mary (Subject: Mary Fields)\n",
            "Comparing: hasdescription -> born enslaved. first african american woman to drive a coach for the postal service. became a business owner at age 70. a frontierswoman who drank, smoke, and got in a shootout. with fullnameasstring -> stagecoach mary (Subject: Mary Fields)\n",
            "Comparing: hasdescription -> born enslaved. first african american woman to drive a coach for the postal service. became a business owner at age 70. a frontierswoman who drank, smoke, and got in a shootout. with hassurnameasstring -> fields (Subject: Mary Fields)\n",
            "Comparing: hasdescription -> born enslaved. first african american woman to drive a coach for the postal service. became a business owner at age 70. a frontierswoman who drank, smoke, and got in a shootout. with hasfirstnameasstring -> mary (Subject: Mary Fields)\n",
            "Comparing: hasdescription -> born enslaved. first african american woman to drive a coach for the postal service. became a business owner at age 70. a frontierswoman who drank, smoke, and got in a shootout. with hasdescription -> first african-american woman to work for the u.s. postal service, known for her fearless demeanor and reliability. (Subject: Mary Fields)\n",
            "Comparing: hasdescription -> born enslaved. first african american woman to drive a coach for the postal service. became a business owner at age 70. a frontierswoman who drank, smoke, and got in a shootout. with referstoplaceoforigin -> montana (Subject: Mary Fields)\n",
            "Comparing: hasdescription -> born enslaved. first african american woman to drive a coach for the postal service. became a business owner at age 70. a frontierswoman who drank, smoke, and got in a shootout. with has_race -> african-american (Subject: Mary Fields)\n",
            "Comparing: hasdescription -> born enslaved. first african american woman to drive a coach for the postal service. became a business owner at age 70. a frontierswoman who drank, smoke, and got in a shootout. with hasdeathdate -> 1914 (Subject: Mary Fields)\n",
            "Comparing: hasdescription -> born enslaved. first african american woman to drive a coach for the postal service. became a business owner at age 70. a frontierswoman who drank, smoke, and got in a shootout. with hassex -> female (Subject: Mary Fields)\n",
            "Comparing: hasdescription -> born enslaved. first african american woman to drive a coach for the postal service. became a business owner at age 70. a frontierswoman who drank, smoke, and got in a shootout. with hasstatusgeneratedevent -> worked as a star route carrier (Subject: Mary Fields)\n",
            "Comparing: hasdescription -> born enslaved. first african american woman to drive a coach for the postal service. became a business owner at age 70. a frontierswoman who drank, smoke, and got in a shootout. with hasvalue -> public figure, mail carrier (Subject: Mary Fields)\n",
            "Comparing: hasdescription -> born enslaved. first african american woman to drive a coach for the postal service. became a business owner at age 70. a frontierswoman who drank, smoke, and got in a shootout. with hasvalue -> star route carrier, tavern owner (Subject: Mary Fields)\n",
            "Comparing: hasdescription -> born enslaved. first african american woman to drive a coach for the postal service. became a business owner at age 70. a frontierswoman who drank, smoke, and got in a shootout. with isrelationshipto -> mother mary amadeus (Subject: Mary Fields)\n",
            "Comparing: hasdescription -> born enslaved. first african american woman to drive a coach for the postal service. became a business owner at age 70. a frontierswoman who drank, smoke, and got in a shootout. with isrelationshipfrom -> stagecoach mary (Subject: Mary Fields)\n",
            "Comparing: hasdescription -> born enslaved. first african american woman to drive a coach for the postal service. became a business owner at age 70. a frontierswoman who drank, smoke, and got in a shootout. with hasparticipantroletype -> worker (Subject: Mary Fields)\n",
            "Comparing: hasdescription -> born enslaved. first african american woman to drive a coach for the postal service. became a business owner at age 70. a frontierswoman who drank, smoke, and got in a shootout. with roleprovidedby -> worked at st. peter's mission (Subject: Mary Fields)\n",
            "No match found for truth row: Stagecoach Mary Fields, Predicate: hasdescription, Object: born enslaved. first african american woman to drive a coach for the postal service. became a business owner at age 70. a frontierswoman who drank, smoke, and got in a shootout.\n",
            "\n",
            "Processing truth file: Prince_Whipple.tsv\n",
            "Truth DataFrame:\n",
            "                 0                         1  \\\n",
            "0   Prince Whipple               instance of   \n",
            "1   Prince Whipple                    hasSex   \n",
            "2   Prince Whipple        hasParticipantRole   \n",
            "3   Prince Whipple        hasParticipantRole   \n",
            "4   Prince Whipple  hasDescriptiveOccupation   \n",
            "5   Prince Whipple  hasDescriptiveOccupation   \n",
            "6   Prince Whipple  hasDescriptiveOccupation   \n",
            "7   Prince Whipple  hasDescriptiveOccupation   \n",
            "8   Prince Whipple              hasFirstName   \n",
            "9   Prince Whipple                hasSurname   \n",
            "10  Prince Whipple                   hasName   \n",
            "11  Prince Whipple            hasDescription   \n",
            "\n",
            "                                                    2  \n",
            "0                                              Person  \n",
            "1                                                Male  \n",
            "2                                               Child  \n",
            "3                                     Deceased Person  \n",
            "4                                               Slave  \n",
            "5                       Patriot (American Revolution)  \n",
            "6                                        Abolitionist  \n",
            "7                          Revolutionary Army Officer  \n",
            "8                                              Prince  \n",
            "9                                             Whipple  \n",
            "10                                     Prince Whipple  \n",
            "11  Enslaved. Free before 13th Amendment. After be...  \n",
            "\n",
            "Found matching generated file: Prince_Whipple_triples.tsv\n",
            "Generated DataFrame before transformation:\n",
            "                   Subject                      Predicate  \\\n",
            "1   Prince_Whipple_triples        hasPreferredNameVariant   \n",
            "2   Prince_Whipple_triples               fullNameAsString   \n",
            "3   Prince_Whipple_triples             hasSurnameAsString   \n",
            "4   Prince_Whipple_triples           hasFirstnameAsString   \n",
            "5   Prince_Whipple_triples                 hasDescription   \n",
            "6   Prince_Whipple_triples          refersToPlaceOfOrigin   \n",
            "7   Prince_Whipple_triples                       has_Race   \n",
            "8   Prince_Whipple_triples                   hasBirthDate   \n",
            "9   Prince_Whipple_triples                   hasDeathDate   \n",
            "10  Prince_Whipple_triples                         hasSex   \n",
            "11  Prince_Whipple_triples        hasStatusGeneratedEvent   \n",
            "12  Prince_Whipple_triples                       hasValue   \n",
            "13  Prince_Whipple_triples                       hasValue   \n",
            "14  Prince_Whipple_triples  hasInterAgentRelationshipType   \n",
            "15  Prince_Whipple_triples               isRelationshipTo   \n",
            "16  Prince_Whipple_triples             isRelationshipFrom   \n",
            "17  Prince_Whipple_triples         hasParticipantRoleType   \n",
            "18  Prince_Whipple_triples                 roleProvidedBy   \n",
            "\n",
            "                                               Object  \n",
            "1                                      Prince Whipple  \n",
            "2                                      Prince Whipple  \n",
            "3                                             Whipple  \n",
            "4                                              Prince  \n",
            "5   African American slave and later freedman. He ...  \n",
            "6                                       Ambou, Africa  \n",
            "7                                    African-American  \n",
            "8                                                1750  \n",
            "9                                                1796  \n",
            "10                                               Male  \n",
            "11                                        manumission  \n",
            "12                                           Freedman  \n",
            "13                                 Soldier, Bodyguard  \n",
            "14                                         Slaveowner  \n",
            "15                            General William Whipple  \n",
            "16                                     Prince Whipple  \n",
            "17                                 Soldier, Bodyguard  \n",
            "18                                American Revolution  \n",
            "Generated DataFrame after transformation:\n",
            "           Subject                      Predicate  \\\n",
            "1   Prince Whipple        haspreferrednamevariant   \n",
            "2   Prince Whipple               fullnameasstring   \n",
            "3   Prince Whipple             hassurnameasstring   \n",
            "4   Prince Whipple           hasfirstnameasstring   \n",
            "5   Prince Whipple                 hasdescription   \n",
            "6   Prince Whipple          referstoplaceoforigin   \n",
            "7   Prince Whipple                       has_race   \n",
            "8   Prince Whipple                   hasbirthdate   \n",
            "9   Prince Whipple                   hasdeathdate   \n",
            "10  Prince Whipple                         hassex   \n",
            "11  Prince Whipple        hasstatusgeneratedevent   \n",
            "12  Prince Whipple                       hasvalue   \n",
            "13  Prince Whipple                       hasvalue   \n",
            "14  Prince Whipple  hasinteragentrelationshiptype   \n",
            "15  Prince Whipple               isrelationshipto   \n",
            "16  Prince Whipple             isrelationshipfrom   \n",
            "17  Prince Whipple         hasparticipantroletype   \n",
            "18  Prince Whipple                 roleprovidedby   \n",
            "\n",
            "                                               Object  \n",
            "1                                      prince whipple  \n",
            "2                                      prince whipple  \n",
            "3                                             whipple  \n",
            "4                                              prince  \n",
            "5   african american slave and later freedman. he ...  \n",
            "6                                       ambou, africa  \n",
            "7                                    african-american  \n",
            "8                                                1750  \n",
            "9                                                1796  \n",
            "10                                               male  \n",
            "11                                        manumission  \n",
            "12                                           freedman  \n",
            "13                                 soldier, bodyguard  \n",
            "14                                         slaveowner  \n",
            "15                            general william whipple  \n",
            "16                                     prince whipple  \n",
            "17                                 soldier, bodyguard  \n",
            "18                                american revolution  \n",
            "Comparing: instance of -> person with haspreferrednamevariant -> prince whipple (Subject: Prince Whipple)\n",
            "Comparing: instance of -> person with fullnameasstring -> prince whipple (Subject: Prince Whipple)\n",
            "Comparing: instance of -> person with hassurnameasstring -> whipple (Subject: Prince Whipple)\n",
            "Comparing: instance of -> person with hasfirstnameasstring -> prince (Subject: Prince Whipple)\n",
            "Comparing: instance of -> person with hasdescription -> african american slave and later freedman. he was a soldier and a bodyguard during the american revolution under his slaveowner general william whipple of the new hampshire militia who formally manumitted him in 1784. (Subject: Prince Whipple)\n",
            "Comparing: instance of -> person with referstoplaceoforigin -> ambou, africa (Subject: Prince Whipple)\n",
            "Comparing: instance of -> person with has_race -> african-american (Subject: Prince Whipple)\n",
            "Comparing: instance of -> person with hasbirthdate -> 1750 (Subject: Prince Whipple)\n",
            "Comparing: instance of -> person with hasdeathdate -> 1796 (Subject: Prince Whipple)\n",
            "Comparing: instance of -> person with hassex -> male (Subject: Prince Whipple)\n",
            "Comparing: instance of -> person with hasstatusgeneratedevent -> manumission (Subject: Prince Whipple)\n",
            "Comparing: instance of -> person with hasvalue -> freedman (Subject: Prince Whipple)\n",
            "Comparing: instance of -> person with hasvalue -> soldier, bodyguard (Subject: Prince Whipple)\n",
            "Comparing: instance of -> person with hasinteragentrelationshiptype -> slaveowner (Subject: Prince Whipple)\n",
            "Comparing: instance of -> person with isrelationshipto -> general william whipple (Subject: Prince Whipple)\n",
            "Comparing: instance of -> person with isrelationshipfrom -> prince whipple (Subject: Prince Whipple)\n",
            "Comparing: instance of -> person with hasparticipantroletype -> soldier, bodyguard (Subject: Prince Whipple)\n",
            "Comparing: instance of -> person with roleprovidedby -> american revolution (Subject: Prince Whipple)\n",
            "No match found for truth row: Prince Whipple, Predicate: instance of, Object: person\n",
            "Comparing: hassex -> male with haspreferrednamevariant -> prince whipple (Subject: Prince Whipple)\n",
            "Comparing: hassex -> male with fullnameasstring -> prince whipple (Subject: Prince Whipple)\n",
            "Comparing: hassex -> male with hassurnameasstring -> whipple (Subject: Prince Whipple)\n",
            "Comparing: hassex -> male with hasfirstnameasstring -> prince (Subject: Prince Whipple)\n",
            "Comparing: hassex -> male with hasdescription -> african american slave and later freedman. he was a soldier and a bodyguard during the american revolution under his slaveowner general william whipple of the new hampshire militia who formally manumitted him in 1784. (Subject: Prince Whipple)\n",
            "Comparing: hassex -> male with referstoplaceoforigin -> ambou, africa (Subject: Prince Whipple)\n",
            "Comparing: hassex -> male with has_race -> african-american (Subject: Prince Whipple)\n",
            "Comparing: hassex -> male with hasbirthdate -> 1750 (Subject: Prince Whipple)\n",
            "Comparing: hassex -> male with hasdeathdate -> 1796 (Subject: Prince Whipple)\n",
            "Comparing: hassex -> male with hassex -> male (Subject: Prince Whipple)\n",
            "Match found for truth row Prince Whipple with generated row Prince Whipple\n",
            "Comparing: hassex -> male with hasstatusgeneratedevent -> manumission (Subject: Prince Whipple)\n",
            "Comparing: hassex -> male with hasvalue -> freedman (Subject: Prince Whipple)\n",
            "Comparing: hassex -> male with hasvalue -> soldier, bodyguard (Subject: Prince Whipple)\n",
            "Comparing: hassex -> male with hasinteragentrelationshiptype -> slaveowner (Subject: Prince Whipple)\n",
            "Comparing: hassex -> male with isrelationshipto -> general william whipple (Subject: Prince Whipple)\n",
            "Comparing: hassex -> male with isrelationshipfrom -> prince whipple (Subject: Prince Whipple)\n",
            "Comparing: hassex -> male with hasparticipantroletype -> soldier, bodyguard (Subject: Prince Whipple)\n",
            "Comparing: hassex -> male with roleprovidedby -> american revolution (Subject: Prince Whipple)\n",
            "Comparing: hasparticipantrole -> child with haspreferrednamevariant -> prince whipple (Subject: Prince Whipple)\n",
            "Comparing: hasparticipantrole -> child with fullnameasstring -> prince whipple (Subject: Prince Whipple)\n",
            "Comparing: hasparticipantrole -> child with hassurnameasstring -> whipple (Subject: Prince Whipple)\n",
            "Comparing: hasparticipantrole -> child with hasfirstnameasstring -> prince (Subject: Prince Whipple)\n",
            "Comparing: hasparticipantrole -> child with hasdescription -> african american slave and later freedman. he was a soldier and a bodyguard during the american revolution under his slaveowner general william whipple of the new hampshire militia who formally manumitted him in 1784. (Subject: Prince Whipple)\n",
            "Comparing: hasparticipantrole -> child with referstoplaceoforigin -> ambou, africa (Subject: Prince Whipple)\n",
            "Comparing: hasparticipantrole -> child with has_race -> african-american (Subject: Prince Whipple)\n",
            "Comparing: hasparticipantrole -> child with hasbirthdate -> 1750 (Subject: Prince Whipple)\n",
            "Comparing: hasparticipantrole -> child with hasdeathdate -> 1796 (Subject: Prince Whipple)\n",
            "Comparing: hasparticipantrole -> child with hassex -> male (Subject: Prince Whipple)\n",
            "Comparing: hasparticipantrole -> child with hasstatusgeneratedevent -> manumission (Subject: Prince Whipple)\n",
            "Comparing: hasparticipantrole -> child with hasvalue -> freedman (Subject: Prince Whipple)\n",
            "Comparing: hasparticipantrole -> child with hasvalue -> soldier, bodyguard (Subject: Prince Whipple)\n",
            "Comparing: hasparticipantrole -> child with hasinteragentrelationshiptype -> slaveowner (Subject: Prince Whipple)\n",
            "Comparing: hasparticipantrole -> child with isrelationshipto -> general william whipple (Subject: Prince Whipple)\n",
            "Comparing: hasparticipantrole -> child with isrelationshipfrom -> prince whipple (Subject: Prince Whipple)\n",
            "Comparing: hasparticipantrole -> child with hasparticipantroletype -> soldier, bodyguard (Subject: Prince Whipple)\n",
            "Comparing: hasparticipantrole -> child with roleprovidedby -> american revolution (Subject: Prince Whipple)\n",
            "No match found for truth row: Prince Whipple, Predicate: hasparticipantrole, Object: child\n",
            "Comparing: hasparticipantrole -> deceased person with haspreferrednamevariant -> prince whipple (Subject: Prince Whipple)\n",
            "Comparing: hasparticipantrole -> deceased person with fullnameasstring -> prince whipple (Subject: Prince Whipple)\n",
            "Comparing: hasparticipantrole -> deceased person with hassurnameasstring -> whipple (Subject: Prince Whipple)\n",
            "Comparing: hasparticipantrole -> deceased person with hasfirstnameasstring -> prince (Subject: Prince Whipple)\n",
            "Comparing: hasparticipantrole -> deceased person with hasdescription -> african american slave and later freedman. he was a soldier and a bodyguard during the american revolution under his slaveowner general william whipple of the new hampshire militia who formally manumitted him in 1784. (Subject: Prince Whipple)\n",
            "Comparing: hasparticipantrole -> deceased person with referstoplaceoforigin -> ambou, africa (Subject: Prince Whipple)\n",
            "Comparing: hasparticipantrole -> deceased person with has_race -> african-american (Subject: Prince Whipple)\n",
            "Comparing: hasparticipantrole -> deceased person with hasbirthdate -> 1750 (Subject: Prince Whipple)\n",
            "Comparing: hasparticipantrole -> deceased person with hasdeathdate -> 1796 (Subject: Prince Whipple)\n",
            "Comparing: hasparticipantrole -> deceased person with hassex -> male (Subject: Prince Whipple)\n",
            "Comparing: hasparticipantrole -> deceased person with hasstatusgeneratedevent -> manumission (Subject: Prince Whipple)\n",
            "Comparing: hasparticipantrole -> deceased person with hasvalue -> freedman (Subject: Prince Whipple)\n",
            "Comparing: hasparticipantrole -> deceased person with hasvalue -> soldier, bodyguard (Subject: Prince Whipple)\n",
            "Comparing: hasparticipantrole -> deceased person with hasinteragentrelationshiptype -> slaveowner (Subject: Prince Whipple)\n",
            "Comparing: hasparticipantrole -> deceased person with isrelationshipto -> general william whipple (Subject: Prince Whipple)\n",
            "Comparing: hasparticipantrole -> deceased person with isrelationshipfrom -> prince whipple (Subject: Prince Whipple)\n",
            "Comparing: hasparticipantrole -> deceased person with hasparticipantroletype -> soldier, bodyguard (Subject: Prince Whipple)\n",
            "Comparing: hasparticipantrole -> deceased person with roleprovidedby -> american revolution (Subject: Prince Whipple)\n",
            "No match found for truth row: Prince Whipple, Predicate: hasparticipantrole, Object: deceased person\n",
            "Comparing: hasdescriptiveoccupation -> slave with haspreferrednamevariant -> prince whipple (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> slave with fullnameasstring -> prince whipple (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hassurnameasstring -> whipple (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasfirstnameasstring -> prince (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasdescription -> african american slave and later freedman. he was a soldier and a bodyguard during the american revolution under his slaveowner general william whipple of the new hampshire militia who formally manumitted him in 1784. (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> slave with referstoplaceoforigin -> ambou, africa (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> slave with has_race -> african-american (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasbirthdate -> 1750 (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasdeathdate -> 1796 (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hassex -> male (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasstatusgeneratedevent -> manumission (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasvalue -> freedman (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasvalue -> soldier, bodyguard (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasinteragentrelationshiptype -> slaveowner (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> slave with isrelationshipto -> general william whipple (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> slave with isrelationshipfrom -> prince whipple (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasparticipantroletype -> soldier, bodyguard (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> slave with roleprovidedby -> american revolution (Subject: Prince Whipple)\n",
            "No match found for truth row: Prince Whipple, Predicate: hasdescriptiveoccupation, Object: slave\n",
            "Comparing: hasdescriptiveoccupation -> patriot (american revolution) with haspreferrednamevariant -> prince whipple (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> patriot (american revolution) with fullnameasstring -> prince whipple (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> patriot (american revolution) with hassurnameasstring -> whipple (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> patriot (american revolution) with hasfirstnameasstring -> prince (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> patriot (american revolution) with hasdescription -> african american slave and later freedman. he was a soldier and a bodyguard during the american revolution under his slaveowner general william whipple of the new hampshire militia who formally manumitted him in 1784. (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> patriot (american revolution) with referstoplaceoforigin -> ambou, africa (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> patriot (american revolution) with has_race -> african-american (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> patriot (american revolution) with hasbirthdate -> 1750 (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> patriot (american revolution) with hasdeathdate -> 1796 (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> patriot (american revolution) with hassex -> male (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> patriot (american revolution) with hasstatusgeneratedevent -> manumission (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> patriot (american revolution) with hasvalue -> freedman (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> patriot (american revolution) with hasvalue -> soldier, bodyguard (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> patriot (american revolution) with hasinteragentrelationshiptype -> slaveowner (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> patriot (american revolution) with isrelationshipto -> general william whipple (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> patriot (american revolution) with isrelationshipfrom -> prince whipple (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> patriot (american revolution) with hasparticipantroletype -> soldier, bodyguard (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> patriot (american revolution) with roleprovidedby -> american revolution (Subject: Prince Whipple)\n",
            "No match found for truth row: Prince Whipple, Predicate: hasdescriptiveoccupation, Object: patriot (american revolution)\n",
            "Comparing: hasdescriptiveoccupation -> abolitionist with haspreferrednamevariant -> prince whipple (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> abolitionist with fullnameasstring -> prince whipple (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> abolitionist with hassurnameasstring -> whipple (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> abolitionist with hasfirstnameasstring -> prince (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> abolitionist with hasdescription -> african american slave and later freedman. he was a soldier and a bodyguard during the american revolution under his slaveowner general william whipple of the new hampshire militia who formally manumitted him in 1784. (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> abolitionist with referstoplaceoforigin -> ambou, africa (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> abolitionist with has_race -> african-american (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> abolitionist with hasbirthdate -> 1750 (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> abolitionist with hasdeathdate -> 1796 (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> abolitionist with hassex -> male (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> abolitionist with hasstatusgeneratedevent -> manumission (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> abolitionist with hasvalue -> freedman (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> abolitionist with hasvalue -> soldier, bodyguard (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> abolitionist with hasinteragentrelationshiptype -> slaveowner (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> abolitionist with isrelationshipto -> general william whipple (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> abolitionist with isrelationshipfrom -> prince whipple (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> abolitionist with hasparticipantroletype -> soldier, bodyguard (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> abolitionist with roleprovidedby -> american revolution (Subject: Prince Whipple)\n",
            "No match found for truth row: Prince Whipple, Predicate: hasdescriptiveoccupation, Object: abolitionist\n",
            "Comparing: hasdescriptiveoccupation -> revolutionary army officer with haspreferrednamevariant -> prince whipple (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> revolutionary army officer with fullnameasstring -> prince whipple (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> revolutionary army officer with hassurnameasstring -> whipple (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> revolutionary army officer with hasfirstnameasstring -> prince (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> revolutionary army officer with hasdescription -> african american slave and later freedman. he was a soldier and a bodyguard during the american revolution under his slaveowner general william whipple of the new hampshire militia who formally manumitted him in 1784. (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> revolutionary army officer with referstoplaceoforigin -> ambou, africa (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> revolutionary army officer with has_race -> african-american (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> revolutionary army officer with hasbirthdate -> 1750 (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> revolutionary army officer with hasdeathdate -> 1796 (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> revolutionary army officer with hassex -> male (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> revolutionary army officer with hasstatusgeneratedevent -> manumission (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> revolutionary army officer with hasvalue -> freedman (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> revolutionary army officer with hasvalue -> soldier, bodyguard (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> revolutionary army officer with hasinteragentrelationshiptype -> slaveowner (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> revolutionary army officer with isrelationshipto -> general william whipple (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> revolutionary army officer with isrelationshipfrom -> prince whipple (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> revolutionary army officer with hasparticipantroletype -> soldier, bodyguard (Subject: Prince Whipple)\n",
            "Comparing: hasdescriptiveoccupation -> revolutionary army officer with roleprovidedby -> american revolution (Subject: Prince Whipple)\n",
            "No match found for truth row: Prince Whipple, Predicate: hasdescriptiveoccupation, Object: revolutionary army officer\n",
            "Comparing: hasfirstname -> prince with haspreferrednamevariant -> prince whipple (Subject: Prince Whipple)\n",
            "Comparing: hasfirstname -> prince with fullnameasstring -> prince whipple (Subject: Prince Whipple)\n",
            "Comparing: hasfirstname -> prince with hassurnameasstring -> whipple (Subject: Prince Whipple)\n",
            "Comparing: hasfirstname -> prince with hasfirstnameasstring -> prince (Subject: Prince Whipple)\n",
            "Comparing: hasfirstname -> prince with hasdescription -> african american slave and later freedman. he was a soldier and a bodyguard during the american revolution under his slaveowner general william whipple of the new hampshire militia who formally manumitted him in 1784. (Subject: Prince Whipple)\n",
            "Comparing: hasfirstname -> prince with referstoplaceoforigin -> ambou, africa (Subject: Prince Whipple)\n",
            "Comparing: hasfirstname -> prince with has_race -> african-american (Subject: Prince Whipple)\n",
            "Comparing: hasfirstname -> prince with hasbirthdate -> 1750 (Subject: Prince Whipple)\n",
            "Comparing: hasfirstname -> prince with hasdeathdate -> 1796 (Subject: Prince Whipple)\n",
            "Comparing: hasfirstname -> prince with hassex -> male (Subject: Prince Whipple)\n",
            "Comparing: hasfirstname -> prince with hasstatusgeneratedevent -> manumission (Subject: Prince Whipple)\n",
            "Comparing: hasfirstname -> prince with hasvalue -> freedman (Subject: Prince Whipple)\n",
            "Comparing: hasfirstname -> prince with hasvalue -> soldier, bodyguard (Subject: Prince Whipple)\n",
            "Comparing: hasfirstname -> prince with hasinteragentrelationshiptype -> slaveowner (Subject: Prince Whipple)\n",
            "Comparing: hasfirstname -> prince with isrelationshipto -> general william whipple (Subject: Prince Whipple)\n",
            "Comparing: hasfirstname -> prince with isrelationshipfrom -> prince whipple (Subject: Prince Whipple)\n",
            "Comparing: hasfirstname -> prince with hasparticipantroletype -> soldier, bodyguard (Subject: Prince Whipple)\n",
            "Comparing: hasfirstname -> prince with roleprovidedby -> american revolution (Subject: Prince Whipple)\n",
            "No match found for truth row: Prince Whipple, Predicate: hasfirstname, Object: prince\n",
            "Comparing: hassurname -> whipple with haspreferrednamevariant -> prince whipple (Subject: Prince Whipple)\n",
            "Comparing: hassurname -> whipple with fullnameasstring -> prince whipple (Subject: Prince Whipple)\n",
            "Comparing: hassurname -> whipple with hassurnameasstring -> whipple (Subject: Prince Whipple)\n",
            "Comparing: hassurname -> whipple with hasfirstnameasstring -> prince (Subject: Prince Whipple)\n",
            "Comparing: hassurname -> whipple with hasdescription -> african american slave and later freedman. he was a soldier and a bodyguard during the american revolution under his slaveowner general william whipple of the new hampshire militia who formally manumitted him in 1784. (Subject: Prince Whipple)\n",
            "Comparing: hassurname -> whipple with referstoplaceoforigin -> ambou, africa (Subject: Prince Whipple)\n",
            "Comparing: hassurname -> whipple with has_race -> african-american (Subject: Prince Whipple)\n",
            "Comparing: hassurname -> whipple with hasbirthdate -> 1750 (Subject: Prince Whipple)\n",
            "Comparing: hassurname -> whipple with hasdeathdate -> 1796 (Subject: Prince Whipple)\n",
            "Comparing: hassurname -> whipple with hassex -> male (Subject: Prince Whipple)\n",
            "Comparing: hassurname -> whipple with hasstatusgeneratedevent -> manumission (Subject: Prince Whipple)\n",
            "Comparing: hassurname -> whipple with hasvalue -> freedman (Subject: Prince Whipple)\n",
            "Comparing: hassurname -> whipple with hasvalue -> soldier, bodyguard (Subject: Prince Whipple)\n",
            "Comparing: hassurname -> whipple with hasinteragentrelationshiptype -> slaveowner (Subject: Prince Whipple)\n",
            "Comparing: hassurname -> whipple with isrelationshipto -> general william whipple (Subject: Prince Whipple)\n",
            "Comparing: hassurname -> whipple with isrelationshipfrom -> prince whipple (Subject: Prince Whipple)\n",
            "Comparing: hassurname -> whipple with hasparticipantroletype -> soldier, bodyguard (Subject: Prince Whipple)\n",
            "Comparing: hassurname -> whipple with roleprovidedby -> american revolution (Subject: Prince Whipple)\n",
            "No match found for truth row: Prince Whipple, Predicate: hassurname, Object: whipple\n",
            "Comparing: hasname -> prince whipple with haspreferrednamevariant -> prince whipple (Subject: Prince Whipple)\n",
            "Comparing: hasname -> prince whipple with fullnameasstring -> prince whipple (Subject: Prince Whipple)\n",
            "Comparing: hasname -> prince whipple with hassurnameasstring -> whipple (Subject: Prince Whipple)\n",
            "Comparing: hasname -> prince whipple with hasfirstnameasstring -> prince (Subject: Prince Whipple)\n",
            "Comparing: hasname -> prince whipple with hasdescription -> african american slave and later freedman. he was a soldier and a bodyguard during the american revolution under his slaveowner general william whipple of the new hampshire militia who formally manumitted him in 1784. (Subject: Prince Whipple)\n",
            "Comparing: hasname -> prince whipple with referstoplaceoforigin -> ambou, africa (Subject: Prince Whipple)\n",
            "Comparing: hasname -> prince whipple with has_race -> african-american (Subject: Prince Whipple)\n",
            "Comparing: hasname -> prince whipple with hasbirthdate -> 1750 (Subject: Prince Whipple)\n",
            "Comparing: hasname -> prince whipple with hasdeathdate -> 1796 (Subject: Prince Whipple)\n",
            "Comparing: hasname -> prince whipple with hassex -> male (Subject: Prince Whipple)\n",
            "Comparing: hasname -> prince whipple with hasstatusgeneratedevent -> manumission (Subject: Prince Whipple)\n",
            "Comparing: hasname -> prince whipple with hasvalue -> freedman (Subject: Prince Whipple)\n",
            "Comparing: hasname -> prince whipple with hasvalue -> soldier, bodyguard (Subject: Prince Whipple)\n",
            "Comparing: hasname -> prince whipple with hasinteragentrelationshiptype -> slaveowner (Subject: Prince Whipple)\n",
            "Comparing: hasname -> prince whipple with isrelationshipto -> general william whipple (Subject: Prince Whipple)\n",
            "Comparing: hasname -> prince whipple with isrelationshipfrom -> prince whipple (Subject: Prince Whipple)\n",
            "Comparing: hasname -> prince whipple with hasparticipantroletype -> soldier, bodyguard (Subject: Prince Whipple)\n",
            "Comparing: hasname -> prince whipple with roleprovidedby -> american revolution (Subject: Prince Whipple)\n",
            "No match found for truth row: Prince Whipple, Predicate: hasname, Object: prince whipple\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. after being enslaved and transported to north america, prince petitioned unsuccessfully in 1779 to end slavery and then participated in the american revolution. some credited him with crossing the delaware river with washington in december 1776, though this is unsubstantiated. with haspreferrednamevariant -> prince whipple (Subject: Prince Whipple)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. after being enslaved and transported to north america, prince petitioned unsuccessfully in 1779 to end slavery and then participated in the american revolution. some credited him with crossing the delaware river with washington in december 1776, though this is unsubstantiated. with fullnameasstring -> prince whipple (Subject: Prince Whipple)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. after being enslaved and transported to north america, prince petitioned unsuccessfully in 1779 to end slavery and then participated in the american revolution. some credited him with crossing the delaware river with washington in december 1776, though this is unsubstantiated. with hassurnameasstring -> whipple (Subject: Prince Whipple)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. after being enslaved and transported to north america, prince petitioned unsuccessfully in 1779 to end slavery and then participated in the american revolution. some credited him with crossing the delaware river with washington in december 1776, though this is unsubstantiated. with hasfirstnameasstring -> prince (Subject: Prince Whipple)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. after being enslaved and transported to north america, prince petitioned unsuccessfully in 1779 to end slavery and then participated in the american revolution. some credited him with crossing the delaware river with washington in december 1776, though this is unsubstantiated. with hasdescription -> african american slave and later freedman. he was a soldier and a bodyguard during the american revolution under his slaveowner general william whipple of the new hampshire militia who formally manumitted him in 1784. (Subject: Prince Whipple)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. after being enslaved and transported to north america, prince petitioned unsuccessfully in 1779 to end slavery and then participated in the american revolution. some credited him with crossing the delaware river with washington in december 1776, though this is unsubstantiated. with referstoplaceoforigin -> ambou, africa (Subject: Prince Whipple)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. after being enslaved and transported to north america, prince petitioned unsuccessfully in 1779 to end slavery and then participated in the american revolution. some credited him with crossing the delaware river with washington in december 1776, though this is unsubstantiated. with has_race -> african-american (Subject: Prince Whipple)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. after being enslaved and transported to north america, prince petitioned unsuccessfully in 1779 to end slavery and then participated in the american revolution. some credited him with crossing the delaware river with washington in december 1776, though this is unsubstantiated. with hasbirthdate -> 1750 (Subject: Prince Whipple)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. after being enslaved and transported to north america, prince petitioned unsuccessfully in 1779 to end slavery and then participated in the american revolution. some credited him with crossing the delaware river with washington in december 1776, though this is unsubstantiated. with hasdeathdate -> 1796 (Subject: Prince Whipple)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. after being enslaved and transported to north america, prince petitioned unsuccessfully in 1779 to end slavery and then participated in the american revolution. some credited him with crossing the delaware river with washington in december 1776, though this is unsubstantiated. with hassex -> male (Subject: Prince Whipple)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. after being enslaved and transported to north america, prince petitioned unsuccessfully in 1779 to end slavery and then participated in the american revolution. some credited him with crossing the delaware river with washington in december 1776, though this is unsubstantiated. with hasstatusgeneratedevent -> manumission (Subject: Prince Whipple)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. after being enslaved and transported to north america, prince petitioned unsuccessfully in 1779 to end slavery and then participated in the american revolution. some credited him with crossing the delaware river with washington in december 1776, though this is unsubstantiated. with hasvalue -> freedman (Subject: Prince Whipple)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. after being enslaved and transported to north america, prince petitioned unsuccessfully in 1779 to end slavery and then participated in the american revolution. some credited him with crossing the delaware river with washington in december 1776, though this is unsubstantiated. with hasvalue -> soldier, bodyguard (Subject: Prince Whipple)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. after being enslaved and transported to north america, prince petitioned unsuccessfully in 1779 to end slavery and then participated in the american revolution. some credited him with crossing the delaware river with washington in december 1776, though this is unsubstantiated. with hasinteragentrelationshiptype -> slaveowner (Subject: Prince Whipple)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. after being enslaved and transported to north america, prince petitioned unsuccessfully in 1779 to end slavery and then participated in the american revolution. some credited him with crossing the delaware river with washington in december 1776, though this is unsubstantiated. with isrelationshipto -> general william whipple (Subject: Prince Whipple)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. after being enslaved and transported to north america, prince petitioned unsuccessfully in 1779 to end slavery and then participated in the american revolution. some credited him with crossing the delaware river with washington in december 1776, though this is unsubstantiated. with isrelationshipfrom -> prince whipple (Subject: Prince Whipple)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. after being enslaved and transported to north america, prince petitioned unsuccessfully in 1779 to end slavery and then participated in the american revolution. some credited him with crossing the delaware river with washington in december 1776, though this is unsubstantiated. with hasparticipantroletype -> soldier, bodyguard (Subject: Prince Whipple)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. after being enslaved and transported to north america, prince petitioned unsuccessfully in 1779 to end slavery and then participated in the american revolution. some credited him with crossing the delaware river with washington in december 1776, though this is unsubstantiated. with roleprovidedby -> american revolution (Subject: Prince Whipple)\n",
            "No match found for truth row: Prince Whipple, Predicate: hasdescription, Object: enslaved. free before 13th amendment. after being enslaved and transported to north america, prince petitioned unsuccessfully in 1779 to end slavery and then participated in the american revolution. some credited him with crossing the delaware river with washington in december 1776, though this is unsubstantiated.\n",
            "\n",
            "Processing truth file: Elizabeth_Greenfield.tsv\n",
            "Truth DataFrame:\n",
            "                              0                         1  \\\n",
            "0   Elizabeth Taylor Greenfield               instance of   \n",
            "1   Elizabeth Taylor Greenfield                    hasSex   \n",
            "2   Elizabeth Taylor Greenfield        hasParticipantRole   \n",
            "3   Elizabeth Taylor Greenfield        hasParticipantRole   \n",
            "4   Elizabeth Taylor Greenfield  hasDescriptiveOccupation   \n",
            "5   Elizabeth Taylor Greenfield  hasDescriptiveOccupation   \n",
            "6   Elizabeth Taylor Greenfield  hasDescriptiveOccupation   \n",
            "7   Elizabeth Taylor Greenfield              hasFirstName   \n",
            "8   Elizabeth Taylor Greenfield                hasSurname   \n",
            "9   Elizabeth Taylor Greenfield                   hasName   \n",
            "10  Elizabeth Taylor Greenfield            hasDescription   \n",
            "\n",
            "                                                    2  \n",
            "0                                              Person  \n",
            "1                                              Female  \n",
            "2                                               Child  \n",
            "3                                     Deceased Person  \n",
            "4                                               Slave  \n",
            "5                                      Music Educator  \n",
            "6                                      Concert Singer  \n",
            "7                                    Elizabeth Taylor  \n",
            "8                                          Greenfield  \n",
            "9                         Elizabeth Taylor Greenfield  \n",
            "10  Born Enslaved. Free before 13th Amendment. A t...  \n",
            "\n",
            "Processing truth file: Timothy_Thomas_Fortune.tsv\n",
            "Truth DataFrame:\n",
            "                         0                         1  \\\n",
            "0   Timothy Thomas Fortune               instance of   \n",
            "1   Timothy Thomas Fortune                    hasSex   \n",
            "2   Timothy Thomas Fortune        hasParticipantRole   \n",
            "3   Timothy Thomas Fortune        hasParticipantRole   \n",
            "4   Timothy Thomas Fortune  hasDescriptiveOccupation   \n",
            "5   Timothy Thomas Fortune  hasDescriptiveOccupation   \n",
            "6   Timothy Thomas Fortune  hasDescriptiveOccupation   \n",
            "7   Timothy Thomas Fortune              hasFirstName   \n",
            "8   Timothy Thomas Fortune                hasSurname   \n",
            "9   Timothy Thomas Fortune                   hasName   \n",
            "10  Timothy Thomas Fortune            hasDescription   \n",
            "\n",
            "                                                    2  \n",
            "0                                              Person  \n",
            "1                                                Male  \n",
            "2                                               Child  \n",
            "3                                     Deceased Person  \n",
            "4                                               Slave  \n",
            "5                               Civil Rights Activist  \n",
            "6                        Newspaper Editor / Publisher  \n",
            "7                                      Timothy Thomas  \n",
            "8                                             Fortune  \n",
            "9                              Timothy Thomas Fortune  \n",
            "10  Enslaved. A prominent journalist who was affil...  \n",
            "\n",
            "Found matching generated file: Timothy_Thomas_Fortune_triples.tsv\n",
            "Generated DataFrame before transformation:\n",
            "                           Subject                      Predicate  \\\n",
            "1   Timothy_Thomas_Fortune_triples        hasPreferredNameVariant   \n",
            "2   Timothy_Thomas_Fortune_triples               fullNameAsString   \n",
            "3   Timothy_Thomas_Fortune_triples             hasSurnameAsString   \n",
            "4   Timothy_Thomas_Fortune_triples           hasFirstnameAsString   \n",
            "5   Timothy_Thomas_Fortune_triples                 hasDescription   \n",
            "6   Timothy_Thomas_Fortune_triples          refersToPlaceOfOrigin   \n",
            "7   Timothy_Thomas_Fortune_triples                       has_Race   \n",
            "8   Timothy_Thomas_Fortune_triples                   hasBirthDate   \n",
            "9   Timothy_Thomas_Fortune_triples                   hasDeathDate   \n",
            "10  Timothy_Thomas_Fortune_triples                         hasSex   \n",
            "11  Timothy_Thomas_Fortune_triples        hasStatusGeneratedEvent   \n",
            "12  Timothy_Thomas_Fortune_triples                       hasValue   \n",
            "13  Timothy_Thomas_Fortune_triples                       hasValue   \n",
            "14  Timothy_Thomas_Fortune_triples  hasInterAgentRelationshipType   \n",
            "15  Timothy_Thomas_Fortune_triples               isRelationshipTo   \n",
            "16  Timothy_Thomas_Fortune_triples             isRelationshipFrom   \n",
            "17  Timothy_Thomas_Fortune_triples         hasParticipantRoleType   \n",
            "18  Timothy_Thomas_Fortune_triples                 roleProvidedBy   \n",
            "\n",
            "                                               Object  \n",
            "1                              Timothy Thomas Fortune  \n",
            "2                              Timothy Thomas Fortune  \n",
            "3                                             Fortune  \n",
            "4                                      Timothy Thomas  \n",
            "5   American orator, civil rights leader, journali...  \n",
            "6                   Marianna, Jackson County, Florida  \n",
            "7                                    African-American  \n",
            "8                                     October 3, 1856  \n",
            "9                                        June 2, 1928  \n",
            "10                                               Male  \n",
            "11                              Civil Rights Movement  \n",
            "12  Civil Rights Leader, Journalist, Writer, Edito...  \n",
            "13  Orator, Civil Rights Leader, Journalist, Write...  \n",
            "14                                            Adviser  \n",
            "15                               Booker T. Washington  \n",
            "16                             Timothy Thomas Fortune  \n",
            "17                                             Editor  \n",
            "18                    Publication of The New York Age  \n",
            "Generated DataFrame after transformation:\n",
            "                   Subject                      Predicate  \\\n",
            "1   Timothy Thomas Fortune        haspreferrednamevariant   \n",
            "2   Timothy Thomas Fortune               fullnameasstring   \n",
            "3   Timothy Thomas Fortune             hassurnameasstring   \n",
            "4   Timothy Thomas Fortune           hasfirstnameasstring   \n",
            "5   Timothy Thomas Fortune                 hasdescription   \n",
            "6   Timothy Thomas Fortune          referstoplaceoforigin   \n",
            "7   Timothy Thomas Fortune                       has_race   \n",
            "8   Timothy Thomas Fortune                   hasbirthdate   \n",
            "9   Timothy Thomas Fortune                   hasdeathdate   \n",
            "10  Timothy Thomas Fortune                         hassex   \n",
            "11  Timothy Thomas Fortune        hasstatusgeneratedevent   \n",
            "12  Timothy Thomas Fortune                       hasvalue   \n",
            "13  Timothy Thomas Fortune                       hasvalue   \n",
            "14  Timothy Thomas Fortune  hasinteragentrelationshiptype   \n",
            "15  Timothy Thomas Fortune               isrelationshipto   \n",
            "16  Timothy Thomas Fortune             isrelationshipfrom   \n",
            "17  Timothy Thomas Fortune         hasparticipantroletype   \n",
            "18  Timothy Thomas Fortune                 roleprovidedby   \n",
            "\n",
            "                                               Object  \n",
            "1                              timothy thomas fortune  \n",
            "2                              timothy thomas fortune  \n",
            "3                                             fortune  \n",
            "4                                      timothy thomas  \n",
            "5   american orator, civil rights leader, journali...  \n",
            "6                   marianna, jackson county, florida  \n",
            "7                                    african-american  \n",
            "8                                     october 3, 1856  \n",
            "9                                        june 2, 1928  \n",
            "10                                               male  \n",
            "11                              civil rights movement  \n",
            "12  civil rights leader, journalist, writer, edito...  \n",
            "13  orator, civil rights leader, journalist, write...  \n",
            "14                                            adviser  \n",
            "15                               booker t. washington  \n",
            "16                             timothy thomas fortune  \n",
            "17                                             editor  \n",
            "18                    publication of the new york age  \n",
            "Comparing: instance of -> person with haspreferrednamevariant -> timothy thomas fortune (Subject: Timothy Thomas Fortune)\n",
            "Comparing: instance of -> person with fullnameasstring -> timothy thomas fortune (Subject: Timothy Thomas Fortune)\n",
            "Comparing: instance of -> person with hassurnameasstring -> fortune (Subject: Timothy Thomas Fortune)\n",
            "Comparing: instance of -> person with hasfirstnameasstring -> timothy thomas (Subject: Timothy Thomas Fortune)\n",
            "Comparing: instance of -> person with hasdescription -> american orator, civil rights leader, journalist, writer, editor and publisher. he was the highly influential editor of the nation's leading black newspaper the new york age and was the leading economist in the black community. (Subject: Timothy Thomas Fortune)\n",
            "Comparing: instance of -> person with referstoplaceoforigin -> marianna, jackson county, florida (Subject: Timothy Thomas Fortune)\n",
            "Comparing: instance of -> person with has_race -> african-american (Subject: Timothy Thomas Fortune)\n",
            "Comparing: instance of -> person with hasbirthdate -> october 3, 1856 (Subject: Timothy Thomas Fortune)\n",
            "Comparing: instance of -> person with hasdeathdate -> june 2, 1928 (Subject: Timothy Thomas Fortune)\n",
            "Comparing: instance of -> person with hassex -> male (Subject: Timothy Thomas Fortune)\n",
            "Comparing: instance of -> person with hasstatusgeneratedevent -> civil rights movement (Subject: Timothy Thomas Fortune)\n",
            "Comparing: instance of -> person with hasvalue -> civil rights leader, journalist, writer, editor, publisher (Subject: Timothy Thomas Fortune)\n",
            "Comparing: instance of -> person with hasvalue -> orator, civil rights leader, journalist, writer, editor, publisher (Subject: Timothy Thomas Fortune)\n",
            "Comparing: instance of -> person with hasinteragentrelationshiptype -> adviser (Subject: Timothy Thomas Fortune)\n",
            "Comparing: instance of -> person with isrelationshipto -> booker t. washington (Subject: Timothy Thomas Fortune)\n",
            "Comparing: instance of -> person with isrelationshipfrom -> timothy thomas fortune (Subject: Timothy Thomas Fortune)\n",
            "Comparing: instance of -> person with hasparticipantroletype -> editor (Subject: Timothy Thomas Fortune)\n",
            "Comparing: instance of -> person with roleprovidedby -> publication of the new york age (Subject: Timothy Thomas Fortune)\n",
            "No match found for truth row: Timothy Thomas Fortune, Predicate: instance of, Object: person\n",
            "Comparing: hassex -> male with haspreferrednamevariant -> timothy thomas fortune (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hassex -> male with fullnameasstring -> timothy thomas fortune (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hassex -> male with hassurnameasstring -> fortune (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hassex -> male with hasfirstnameasstring -> timothy thomas (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hassex -> male with hasdescription -> american orator, civil rights leader, journalist, writer, editor and publisher. he was the highly influential editor of the nation's leading black newspaper the new york age and was the leading economist in the black community. (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hassex -> male with referstoplaceoforigin -> marianna, jackson county, florida (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hassex -> male with has_race -> african-american (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hassex -> male with hasbirthdate -> october 3, 1856 (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hassex -> male with hasdeathdate -> june 2, 1928 (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hassex -> male with hassex -> male (Subject: Timothy Thomas Fortune)\n",
            "Match found for truth row Timothy Thomas Fortune with generated row Timothy Thomas Fortune\n",
            "Comparing: hassex -> male with hasstatusgeneratedevent -> civil rights movement (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hassex -> male with hasvalue -> civil rights leader, journalist, writer, editor, publisher (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hassex -> male with hasvalue -> orator, civil rights leader, journalist, writer, editor, publisher (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hassex -> male with hasinteragentrelationshiptype -> adviser (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hassex -> male with isrelationshipto -> booker t. washington (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hassex -> male with isrelationshipfrom -> timothy thomas fortune (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hassex -> male with hasparticipantroletype -> editor (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hassex -> male with roleprovidedby -> publication of the new york age (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasparticipantrole -> child with haspreferrednamevariant -> timothy thomas fortune (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasparticipantrole -> child with fullnameasstring -> timothy thomas fortune (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasparticipantrole -> child with hassurnameasstring -> fortune (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasparticipantrole -> child with hasfirstnameasstring -> timothy thomas (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasparticipantrole -> child with hasdescription -> american orator, civil rights leader, journalist, writer, editor and publisher. he was the highly influential editor of the nation's leading black newspaper the new york age and was the leading economist in the black community. (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasparticipantrole -> child with referstoplaceoforigin -> marianna, jackson county, florida (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasparticipantrole -> child with has_race -> african-american (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasparticipantrole -> child with hasbirthdate -> october 3, 1856 (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasparticipantrole -> child with hasdeathdate -> june 2, 1928 (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasparticipantrole -> child with hassex -> male (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasparticipantrole -> child with hasstatusgeneratedevent -> civil rights movement (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasparticipantrole -> child with hasvalue -> civil rights leader, journalist, writer, editor, publisher (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasparticipantrole -> child with hasvalue -> orator, civil rights leader, journalist, writer, editor, publisher (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasparticipantrole -> child with hasinteragentrelationshiptype -> adviser (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasparticipantrole -> child with isrelationshipto -> booker t. washington (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasparticipantrole -> child with isrelationshipfrom -> timothy thomas fortune (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasparticipantrole -> child with hasparticipantroletype -> editor (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasparticipantrole -> child with roleprovidedby -> publication of the new york age (Subject: Timothy Thomas Fortune)\n",
            "No match found for truth row: Timothy Thomas Fortune, Predicate: hasparticipantrole, Object: child\n",
            "Comparing: hasparticipantrole -> deceased person with haspreferrednamevariant -> timothy thomas fortune (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasparticipantrole -> deceased person with fullnameasstring -> timothy thomas fortune (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasparticipantrole -> deceased person with hassurnameasstring -> fortune (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasparticipantrole -> deceased person with hasfirstnameasstring -> timothy thomas (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasparticipantrole -> deceased person with hasdescription -> american orator, civil rights leader, journalist, writer, editor and publisher. he was the highly influential editor of the nation's leading black newspaper the new york age and was the leading economist in the black community. (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasparticipantrole -> deceased person with referstoplaceoforigin -> marianna, jackson county, florida (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasparticipantrole -> deceased person with has_race -> african-american (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasparticipantrole -> deceased person with hasbirthdate -> october 3, 1856 (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasparticipantrole -> deceased person with hasdeathdate -> june 2, 1928 (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasparticipantrole -> deceased person with hassex -> male (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasparticipantrole -> deceased person with hasstatusgeneratedevent -> civil rights movement (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasparticipantrole -> deceased person with hasvalue -> civil rights leader, journalist, writer, editor, publisher (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasparticipantrole -> deceased person with hasvalue -> orator, civil rights leader, journalist, writer, editor, publisher (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasparticipantrole -> deceased person with hasinteragentrelationshiptype -> adviser (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasparticipantrole -> deceased person with isrelationshipto -> booker t. washington (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasparticipantrole -> deceased person with isrelationshipfrom -> timothy thomas fortune (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasparticipantrole -> deceased person with hasparticipantroletype -> editor (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasparticipantrole -> deceased person with roleprovidedby -> publication of the new york age (Subject: Timothy Thomas Fortune)\n",
            "No match found for truth row: Timothy Thomas Fortune, Predicate: hasparticipantrole, Object: deceased person\n",
            "Comparing: hasdescriptiveoccupation -> slave with haspreferrednamevariant -> timothy thomas fortune (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescriptiveoccupation -> slave with fullnameasstring -> timothy thomas fortune (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hassurnameasstring -> fortune (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasfirstnameasstring -> timothy thomas (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasdescription -> american orator, civil rights leader, journalist, writer, editor and publisher. he was the highly influential editor of the nation's leading black newspaper the new york age and was the leading economist in the black community. (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescriptiveoccupation -> slave with referstoplaceoforigin -> marianna, jackson county, florida (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescriptiveoccupation -> slave with has_race -> african-american (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasbirthdate -> october 3, 1856 (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasdeathdate -> june 2, 1928 (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hassex -> male (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasstatusgeneratedevent -> civil rights movement (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasvalue -> civil rights leader, journalist, writer, editor, publisher (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasvalue -> orator, civil rights leader, journalist, writer, editor, publisher (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasinteragentrelationshiptype -> adviser (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescriptiveoccupation -> slave with isrelationshipto -> booker t. washington (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescriptiveoccupation -> slave with isrelationshipfrom -> timothy thomas fortune (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasparticipantroletype -> editor (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescriptiveoccupation -> slave with roleprovidedby -> publication of the new york age (Subject: Timothy Thomas Fortune)\n",
            "No match found for truth row: Timothy Thomas Fortune, Predicate: hasdescriptiveoccupation, Object: slave\n",
            "Comparing: hasdescriptiveoccupation -> civil rights activist with haspreferrednamevariant -> timothy thomas fortune (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescriptiveoccupation -> civil rights activist with fullnameasstring -> timothy thomas fortune (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescriptiveoccupation -> civil rights activist with hassurnameasstring -> fortune (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescriptiveoccupation -> civil rights activist with hasfirstnameasstring -> timothy thomas (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescriptiveoccupation -> civil rights activist with hasdescription -> american orator, civil rights leader, journalist, writer, editor and publisher. he was the highly influential editor of the nation's leading black newspaper the new york age and was the leading economist in the black community. (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescriptiveoccupation -> civil rights activist with referstoplaceoforigin -> marianna, jackson county, florida (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescriptiveoccupation -> civil rights activist with has_race -> african-american (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescriptiveoccupation -> civil rights activist with hasbirthdate -> october 3, 1856 (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescriptiveoccupation -> civil rights activist with hasdeathdate -> june 2, 1928 (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescriptiveoccupation -> civil rights activist with hassex -> male (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescriptiveoccupation -> civil rights activist with hasstatusgeneratedevent -> civil rights movement (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescriptiveoccupation -> civil rights activist with hasvalue -> civil rights leader, journalist, writer, editor, publisher (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescriptiveoccupation -> civil rights activist with hasvalue -> orator, civil rights leader, journalist, writer, editor, publisher (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescriptiveoccupation -> civil rights activist with hasinteragentrelationshiptype -> adviser (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescriptiveoccupation -> civil rights activist with isrelationshipto -> booker t. washington (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescriptiveoccupation -> civil rights activist with isrelationshipfrom -> timothy thomas fortune (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescriptiveoccupation -> civil rights activist with hasparticipantroletype -> editor (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescriptiveoccupation -> civil rights activist with roleprovidedby -> publication of the new york age (Subject: Timothy Thomas Fortune)\n",
            "No match found for truth row: Timothy Thomas Fortune, Predicate: hasdescriptiveoccupation, Object: civil rights activist\n",
            "Comparing: hasdescriptiveoccupation -> newspaper editor / publisher with haspreferrednamevariant -> timothy thomas fortune (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescriptiveoccupation -> newspaper editor / publisher with fullnameasstring -> timothy thomas fortune (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescriptiveoccupation -> newspaper editor / publisher with hassurnameasstring -> fortune (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescriptiveoccupation -> newspaper editor / publisher with hasfirstnameasstring -> timothy thomas (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescriptiveoccupation -> newspaper editor / publisher with hasdescription -> american orator, civil rights leader, journalist, writer, editor and publisher. he was the highly influential editor of the nation's leading black newspaper the new york age and was the leading economist in the black community. (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescriptiveoccupation -> newspaper editor / publisher with referstoplaceoforigin -> marianna, jackson county, florida (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescriptiveoccupation -> newspaper editor / publisher with has_race -> african-american (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescriptiveoccupation -> newspaper editor / publisher with hasbirthdate -> october 3, 1856 (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescriptiveoccupation -> newspaper editor / publisher with hasdeathdate -> june 2, 1928 (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescriptiveoccupation -> newspaper editor / publisher with hassex -> male (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescriptiveoccupation -> newspaper editor / publisher with hasstatusgeneratedevent -> civil rights movement (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescriptiveoccupation -> newspaper editor / publisher with hasvalue -> civil rights leader, journalist, writer, editor, publisher (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescriptiveoccupation -> newspaper editor / publisher with hasvalue -> orator, civil rights leader, journalist, writer, editor, publisher (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescriptiveoccupation -> newspaper editor / publisher with hasinteragentrelationshiptype -> adviser (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescriptiveoccupation -> newspaper editor / publisher with isrelationshipto -> booker t. washington (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescriptiveoccupation -> newspaper editor / publisher with isrelationshipfrom -> timothy thomas fortune (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescriptiveoccupation -> newspaper editor / publisher with hasparticipantroletype -> editor (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescriptiveoccupation -> newspaper editor / publisher with roleprovidedby -> publication of the new york age (Subject: Timothy Thomas Fortune)\n",
            "No match found for truth row: Timothy Thomas Fortune, Predicate: hasdescriptiveoccupation, Object: newspaper editor / publisher\n",
            "Comparing: hasfirstname -> timothy thomas with haspreferrednamevariant -> timothy thomas fortune (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasfirstname -> timothy thomas with fullnameasstring -> timothy thomas fortune (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasfirstname -> timothy thomas with hassurnameasstring -> fortune (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasfirstname -> timothy thomas with hasfirstnameasstring -> timothy thomas (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasfirstname -> timothy thomas with hasdescription -> american orator, civil rights leader, journalist, writer, editor and publisher. he was the highly influential editor of the nation's leading black newspaper the new york age and was the leading economist in the black community. (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasfirstname -> timothy thomas with referstoplaceoforigin -> marianna, jackson county, florida (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasfirstname -> timothy thomas with has_race -> african-american (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasfirstname -> timothy thomas with hasbirthdate -> october 3, 1856 (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasfirstname -> timothy thomas with hasdeathdate -> june 2, 1928 (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasfirstname -> timothy thomas with hassex -> male (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasfirstname -> timothy thomas with hasstatusgeneratedevent -> civil rights movement (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasfirstname -> timothy thomas with hasvalue -> civil rights leader, journalist, writer, editor, publisher (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasfirstname -> timothy thomas with hasvalue -> orator, civil rights leader, journalist, writer, editor, publisher (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasfirstname -> timothy thomas with hasinteragentrelationshiptype -> adviser (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasfirstname -> timothy thomas with isrelationshipto -> booker t. washington (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasfirstname -> timothy thomas with isrelationshipfrom -> timothy thomas fortune (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasfirstname -> timothy thomas with hasparticipantroletype -> editor (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasfirstname -> timothy thomas with roleprovidedby -> publication of the new york age (Subject: Timothy Thomas Fortune)\n",
            "No match found for truth row: Timothy Thomas Fortune, Predicate: hasfirstname, Object: timothy thomas\n",
            "Comparing: hassurname -> fortune with haspreferrednamevariant -> timothy thomas fortune (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hassurname -> fortune with fullnameasstring -> timothy thomas fortune (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hassurname -> fortune with hassurnameasstring -> fortune (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hassurname -> fortune with hasfirstnameasstring -> timothy thomas (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hassurname -> fortune with hasdescription -> american orator, civil rights leader, journalist, writer, editor and publisher. he was the highly influential editor of the nation's leading black newspaper the new york age and was the leading economist in the black community. (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hassurname -> fortune with referstoplaceoforigin -> marianna, jackson county, florida (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hassurname -> fortune with has_race -> african-american (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hassurname -> fortune with hasbirthdate -> october 3, 1856 (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hassurname -> fortune with hasdeathdate -> june 2, 1928 (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hassurname -> fortune with hassex -> male (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hassurname -> fortune with hasstatusgeneratedevent -> civil rights movement (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hassurname -> fortune with hasvalue -> civil rights leader, journalist, writer, editor, publisher (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hassurname -> fortune with hasvalue -> orator, civil rights leader, journalist, writer, editor, publisher (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hassurname -> fortune with hasinteragentrelationshiptype -> adviser (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hassurname -> fortune with isrelationshipto -> booker t. washington (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hassurname -> fortune with isrelationshipfrom -> timothy thomas fortune (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hassurname -> fortune with hasparticipantroletype -> editor (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hassurname -> fortune with roleprovidedby -> publication of the new york age (Subject: Timothy Thomas Fortune)\n",
            "No match found for truth row: Timothy Thomas Fortune, Predicate: hassurname, Object: fortune\n",
            "Comparing: hasname -> timothy thomas fortune with haspreferrednamevariant -> timothy thomas fortune (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasname -> timothy thomas fortune with fullnameasstring -> timothy thomas fortune (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasname -> timothy thomas fortune with hassurnameasstring -> fortune (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasname -> timothy thomas fortune with hasfirstnameasstring -> timothy thomas (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasname -> timothy thomas fortune with hasdescription -> american orator, civil rights leader, journalist, writer, editor and publisher. he was the highly influential editor of the nation's leading black newspaper the new york age and was the leading economist in the black community. (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasname -> timothy thomas fortune with referstoplaceoforigin -> marianna, jackson county, florida (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasname -> timothy thomas fortune with has_race -> african-american (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasname -> timothy thomas fortune with hasbirthdate -> october 3, 1856 (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasname -> timothy thomas fortune with hasdeathdate -> june 2, 1928 (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasname -> timothy thomas fortune with hassex -> male (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasname -> timothy thomas fortune with hasstatusgeneratedevent -> civil rights movement (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasname -> timothy thomas fortune with hasvalue -> civil rights leader, journalist, writer, editor, publisher (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasname -> timothy thomas fortune with hasvalue -> orator, civil rights leader, journalist, writer, editor, publisher (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasname -> timothy thomas fortune with hasinteragentrelationshiptype -> adviser (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasname -> timothy thomas fortune with isrelationshipto -> booker t. washington (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasname -> timothy thomas fortune with isrelationshipfrom -> timothy thomas fortune (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasname -> timothy thomas fortune with hasparticipantroletype -> editor (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasname -> timothy thomas fortune with roleprovidedby -> publication of the new york age (Subject: Timothy Thomas Fortune)\n",
            "No match found for truth row: Timothy Thomas Fortune, Predicate: hasname, Object: timothy thomas fortune\n",
            "Comparing: hasdescription -> enslaved. a prominent journalist who was affiliated with booker t. washington. among other causes, he called for use of the term \"afro-american\". with haspreferrednamevariant -> timothy thomas fortune (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescription -> enslaved. a prominent journalist who was affiliated with booker t. washington. among other causes, he called for use of the term \"afro-american\". with fullnameasstring -> timothy thomas fortune (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescription -> enslaved. a prominent journalist who was affiliated with booker t. washington. among other causes, he called for use of the term \"afro-american\". with hassurnameasstring -> fortune (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescription -> enslaved. a prominent journalist who was affiliated with booker t. washington. among other causes, he called for use of the term \"afro-american\". with hasfirstnameasstring -> timothy thomas (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescription -> enslaved. a prominent journalist who was affiliated with booker t. washington. among other causes, he called for use of the term \"afro-american\". with hasdescription -> american orator, civil rights leader, journalist, writer, editor and publisher. he was the highly influential editor of the nation's leading black newspaper the new york age and was the leading economist in the black community. (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescription -> enslaved. a prominent journalist who was affiliated with booker t. washington. among other causes, he called for use of the term \"afro-american\". with referstoplaceoforigin -> marianna, jackson county, florida (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescription -> enslaved. a prominent journalist who was affiliated with booker t. washington. among other causes, he called for use of the term \"afro-american\". with has_race -> african-american (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescription -> enslaved. a prominent journalist who was affiliated with booker t. washington. among other causes, he called for use of the term \"afro-american\". with hasbirthdate -> october 3, 1856 (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescription -> enslaved. a prominent journalist who was affiliated with booker t. washington. among other causes, he called for use of the term \"afro-american\". with hasdeathdate -> june 2, 1928 (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescription -> enslaved. a prominent journalist who was affiliated with booker t. washington. among other causes, he called for use of the term \"afro-american\". with hassex -> male (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescription -> enslaved. a prominent journalist who was affiliated with booker t. washington. among other causes, he called for use of the term \"afro-american\". with hasstatusgeneratedevent -> civil rights movement (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescription -> enslaved. a prominent journalist who was affiliated with booker t. washington. among other causes, he called for use of the term \"afro-american\". with hasvalue -> civil rights leader, journalist, writer, editor, publisher (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescription -> enslaved. a prominent journalist who was affiliated with booker t. washington. among other causes, he called for use of the term \"afro-american\". with hasvalue -> orator, civil rights leader, journalist, writer, editor, publisher (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescription -> enslaved. a prominent journalist who was affiliated with booker t. washington. among other causes, he called for use of the term \"afro-american\". with hasinteragentrelationshiptype -> adviser (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescription -> enslaved. a prominent journalist who was affiliated with booker t. washington. among other causes, he called for use of the term \"afro-american\". with isrelationshipto -> booker t. washington (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescription -> enslaved. a prominent journalist who was affiliated with booker t. washington. among other causes, he called for use of the term \"afro-american\". with isrelationshipfrom -> timothy thomas fortune (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescription -> enslaved. a prominent journalist who was affiliated with booker t. washington. among other causes, he called for use of the term \"afro-american\". with hasparticipantroletype -> editor (Subject: Timothy Thomas Fortune)\n",
            "Comparing: hasdescription -> enslaved. a prominent journalist who was affiliated with booker t. washington. among other causes, he called for use of the term \"afro-american\". with roleprovidedby -> publication of the new york age (Subject: Timothy Thomas Fortune)\n",
            "No match found for truth row: Timothy Thomas Fortune, Predicate: hasdescription, Object: enslaved. a prominent journalist who was affiliated with booker t. washington. among other causes, he called for use of the term \"afro-american\".\n",
            "\n",
            "Processing truth file: Paul_Jennings.tsv\n",
            "Truth DataFrame:\n",
            "                0                         1  \\\n",
            "0   Paul Jennings               instance of   \n",
            "1   Paul Jennings                    hasSex   \n",
            "2   Paul Jennings        hasParticipantRole   \n",
            "3   Paul Jennings        hasParticipantRole   \n",
            "4   Paul Jennings  hasDescriptiveOccupation   \n",
            "5   Paul Jennings  hasDescriptiveOccupation   \n",
            "6   Paul Jennings  hasDescriptiveOccupation   \n",
            "7   Paul Jennings              hasFirstName   \n",
            "8   Paul Jennings                hasSurname   \n",
            "9   Paul Jennings                   hasName   \n",
            "10  Paul Jennings            hasDescription   \n",
            "\n",
            "                                                    2  \n",
            "0                                              Person  \n",
            "1                                                Male  \n",
            "2                                               Child  \n",
            "3                                     Deceased Person  \n",
            "4                                    Domestic Servant  \n",
            "5                                               Slave  \n",
            "6                          Autobiographer / Memoirist  \n",
            "7                                                Paul  \n",
            "8                                            Jennings  \n",
            "9                                       Paul Jennings  \n",
            "10  Enslaved. Free before 13th Amendment. Formerly...  \n",
            "\n",
            "Found matching generated file: Paul_Jennings_(abolitionist)_triples.tsv\n",
            "Generated DataFrame before transformation:\n",
            "                                 Subject                      Predicate  \\\n",
            "1   Paul_Jennings_(abolitionist)_triples        hasPreferredNameVariant   \n",
            "2   Paul_Jennings_(abolitionist)_triples               fullNameAsString   \n",
            "3   Paul_Jennings_(abolitionist)_triples             hasSurnameAsString   \n",
            "4   Paul_Jennings_(abolitionist)_triples           hasFirstnameAsString   \n",
            "5   Paul_Jennings_(abolitionist)_triples                 hasDescription   \n",
            "6   Paul_Jennings_(abolitionist)_triples          refersToPlaceOfOrigin   \n",
            "7   Paul_Jennings_(abolitionist)_triples                       has_Race   \n",
            "8   Paul_Jennings_(abolitionist)_triples                    hasAgeValue   \n",
            "9   Paul_Jennings_(abolitionist)_triples                   hasBirthDate   \n",
            "10  Paul_Jennings_(abolitionist)_triples                   hasDeathDate   \n",
            "11  Paul_Jennings_(abolitionist)_triples                         hasSex   \n",
            "12  Paul_Jennings_(abolitionist)_triples        hasStatusGeneratedEvent   \n",
            "13  Paul_Jennings_(abolitionist)_triples                       hasValue   \n",
            "14  Paul_Jennings_(abolitionist)_triples                       hasValue   \n",
            "15  Paul_Jennings_(abolitionist)_triples  hasInterAgentRelationshipType   \n",
            "16  Paul_Jennings_(abolitionist)_triples               isRelationshipTo   \n",
            "17  Paul_Jennings_(abolitionist)_triples             isRelationshipFrom   \n",
            "18  Paul_Jennings_(abolitionist)_triples         hasParticipantRoleType   \n",
            "19  Paul_Jennings_(abolitionist)_triples                 roleProvidedBy   \n",
            "\n",
            "                                               Object  \n",
            "1                                       Paul Jennings  \n",
            "2                                       Paul Jennings  \n",
            "3                                            Jennings  \n",
            "4                                                Paul  \n",
            "5   An American abolitionist and writer. Enslaved ...  \n",
            "6                                    Washington, D.C.  \n",
            "7                                    African-American  \n",
            "8                                                 NaN  \n",
            "9                                             c. 1799  \n",
            "10                                               1874  \n",
            "11                                               Male  \n",
            "12                               Enslavement, Freedom  \n",
            "13                       Enslaved Person, Free Person  \n",
            "14                               Abolitionist, Writer  \n",
            "15                                        Enslaved by  \n",
            "16                                      James Madison  \n",
            "17                                      Paul Jennings  \n",
            "18                                      Memoir Writer  \n",
            "19        Publication of the first White House memoir  \n",
            "Generated DataFrame after transformation:\n",
            "                         Subject                      Predicate  \\\n",
            "1   Paul Jennings (abolitionist)        haspreferrednamevariant   \n",
            "2   Paul Jennings (abolitionist)               fullnameasstring   \n",
            "3   Paul Jennings (abolitionist)             hassurnameasstring   \n",
            "4   Paul Jennings (abolitionist)           hasfirstnameasstring   \n",
            "5   Paul Jennings (abolitionist)                 hasdescription   \n",
            "6   Paul Jennings (abolitionist)          referstoplaceoforigin   \n",
            "7   Paul Jennings (abolitionist)                       has_race   \n",
            "8   Paul Jennings (abolitionist)                    hasagevalue   \n",
            "9   Paul Jennings (abolitionist)                   hasbirthdate   \n",
            "10  Paul Jennings (abolitionist)                   hasdeathdate   \n",
            "11  Paul Jennings (abolitionist)                         hassex   \n",
            "12  Paul Jennings (abolitionist)        hasstatusgeneratedevent   \n",
            "13  Paul Jennings (abolitionist)                       hasvalue   \n",
            "14  Paul Jennings (abolitionist)                       hasvalue   \n",
            "15  Paul Jennings (abolitionist)  hasinteragentrelationshiptype   \n",
            "16  Paul Jennings (abolitionist)               isrelationshipto   \n",
            "17  Paul Jennings (abolitionist)             isrelationshipfrom   \n",
            "18  Paul Jennings (abolitionist)         hasparticipantroletype   \n",
            "19  Paul Jennings (abolitionist)                 roleprovidedby   \n",
            "\n",
            "                                               Object  \n",
            "1                                       paul jennings  \n",
            "2                                       paul jennings  \n",
            "3                                            jennings  \n",
            "4                                                paul  \n",
            "5   an american abolitionist and writer. enslaved ...  \n",
            "6                                    washington, d.c.  \n",
            "7                                    african-american  \n",
            "8                                                 NaN  \n",
            "9                                             c. 1799  \n",
            "10                                               1874  \n",
            "11                                               male  \n",
            "12                               enslavement, freedom  \n",
            "13                       enslaved person, free person  \n",
            "14                               abolitionist, writer  \n",
            "15                                        enslaved by  \n",
            "16                                      james madison  \n",
            "17                                      paul jennings  \n",
            "18                                      memoir writer  \n",
            "19        publication of the first white house memoir  \n",
            "Comparing: instance of -> person with haspreferrednamevariant -> paul jennings (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: instance of -> person with fullnameasstring -> paul jennings (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: instance of -> person with hassurnameasstring -> jennings (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: instance of -> person with hasfirstnameasstring -> paul (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: instance of -> person with hasdescription -> an american abolitionist and writer. enslaved as a young man by president james madison during and after his white house years, jennings published, in 1865, the first white house memoir. (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: instance of -> person with referstoplaceoforigin -> washington, d.c. (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: instance of -> person with has_race -> african-american (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: instance of -> person with hasagevalue -> nan (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: instance of -> person with hasbirthdate -> c. 1799 (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: instance of -> person with hasdeathdate -> 1874 (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: instance of -> person with hassex -> male (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: instance of -> person with hasstatusgeneratedevent -> enslavement, freedom (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: instance of -> person with hasvalue -> enslaved person, free person (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: instance of -> person with hasvalue -> abolitionist, writer (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: instance of -> person with hasinteragentrelationshiptype -> enslaved by (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: instance of -> person with isrelationshipto -> james madison (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: instance of -> person with isrelationshipfrom -> paul jennings (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: instance of -> person with hasparticipantroletype -> memoir writer (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: instance of -> person with roleprovidedby -> publication of the first white house memoir (Subject: Paul Jennings (abolitionist))\n",
            "No match found for truth row: Paul Jennings, Predicate: instance of, Object: person\n",
            "Comparing: hassex -> male with haspreferrednamevariant -> paul jennings (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hassex -> male with fullnameasstring -> paul jennings (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hassex -> male with hassurnameasstring -> jennings (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hassex -> male with hasfirstnameasstring -> paul (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hassex -> male with hasdescription -> an american abolitionist and writer. enslaved as a young man by president james madison during and after his white house years, jennings published, in 1865, the first white house memoir. (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hassex -> male with referstoplaceoforigin -> washington, d.c. (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hassex -> male with has_race -> african-american (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hassex -> male with hasagevalue -> nan (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hassex -> male with hasbirthdate -> c. 1799 (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hassex -> male with hasdeathdate -> 1874 (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hassex -> male with hassex -> male (Subject: Paul Jennings (abolitionist))\n",
            "Match found for truth row Paul Jennings with generated row Paul Jennings (abolitionist)\n",
            "Comparing: hassex -> male with hasstatusgeneratedevent -> enslavement, freedom (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hassex -> male with hasvalue -> enslaved person, free person (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hassex -> male with hasvalue -> abolitionist, writer (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hassex -> male with hasinteragentrelationshiptype -> enslaved by (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hassex -> male with isrelationshipto -> james madison (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hassex -> male with isrelationshipfrom -> paul jennings (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hassex -> male with hasparticipantroletype -> memoir writer (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hassex -> male with roleprovidedby -> publication of the first white house memoir (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasparticipantrole -> child with haspreferrednamevariant -> paul jennings (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasparticipantrole -> child with fullnameasstring -> paul jennings (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasparticipantrole -> child with hassurnameasstring -> jennings (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasparticipantrole -> child with hasfirstnameasstring -> paul (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasparticipantrole -> child with hasdescription -> an american abolitionist and writer. enslaved as a young man by president james madison during and after his white house years, jennings published, in 1865, the first white house memoir. (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasparticipantrole -> child with referstoplaceoforigin -> washington, d.c. (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasparticipantrole -> child with has_race -> african-american (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasparticipantrole -> child with hasagevalue -> nan (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasparticipantrole -> child with hasbirthdate -> c. 1799 (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasparticipantrole -> child with hasdeathdate -> 1874 (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasparticipantrole -> child with hassex -> male (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasparticipantrole -> child with hasstatusgeneratedevent -> enslavement, freedom (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasparticipantrole -> child with hasvalue -> enslaved person, free person (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasparticipantrole -> child with hasvalue -> abolitionist, writer (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasparticipantrole -> child with hasinteragentrelationshiptype -> enslaved by (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasparticipantrole -> child with isrelationshipto -> james madison (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasparticipantrole -> child with isrelationshipfrom -> paul jennings (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasparticipantrole -> child with hasparticipantroletype -> memoir writer (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasparticipantrole -> child with roleprovidedby -> publication of the first white house memoir (Subject: Paul Jennings (abolitionist))\n",
            "No match found for truth row: Paul Jennings, Predicate: hasparticipantrole, Object: child\n",
            "Comparing: hasparticipantrole -> deceased person with haspreferrednamevariant -> paul jennings (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasparticipantrole -> deceased person with fullnameasstring -> paul jennings (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasparticipantrole -> deceased person with hassurnameasstring -> jennings (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasparticipantrole -> deceased person with hasfirstnameasstring -> paul (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasparticipantrole -> deceased person with hasdescription -> an american abolitionist and writer. enslaved as a young man by president james madison during and after his white house years, jennings published, in 1865, the first white house memoir. (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasparticipantrole -> deceased person with referstoplaceoforigin -> washington, d.c. (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasparticipantrole -> deceased person with has_race -> african-american (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasparticipantrole -> deceased person with hasagevalue -> nan (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasparticipantrole -> deceased person with hasbirthdate -> c. 1799 (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasparticipantrole -> deceased person with hasdeathdate -> 1874 (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasparticipantrole -> deceased person with hassex -> male (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasparticipantrole -> deceased person with hasstatusgeneratedevent -> enslavement, freedom (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasparticipantrole -> deceased person with hasvalue -> enslaved person, free person (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasparticipantrole -> deceased person with hasvalue -> abolitionist, writer (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasparticipantrole -> deceased person with hasinteragentrelationshiptype -> enslaved by (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasparticipantrole -> deceased person with isrelationshipto -> james madison (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasparticipantrole -> deceased person with isrelationshipfrom -> paul jennings (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasparticipantrole -> deceased person with hasparticipantroletype -> memoir writer (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasparticipantrole -> deceased person with roleprovidedby -> publication of the first white house memoir (Subject: Paul Jennings (abolitionist))\n",
            "No match found for truth row: Paul Jennings, Predicate: hasparticipantrole, Object: deceased person\n",
            "Comparing: hasdescriptiveoccupation -> domestic servant with haspreferrednamevariant -> paul jennings (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> domestic servant with fullnameasstring -> paul jennings (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> domestic servant with hassurnameasstring -> jennings (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> domestic servant with hasfirstnameasstring -> paul (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> domestic servant with hasdescription -> an american abolitionist and writer. enslaved as a young man by president james madison during and after his white house years, jennings published, in 1865, the first white house memoir. (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> domestic servant with referstoplaceoforigin -> washington, d.c. (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> domestic servant with has_race -> african-american (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> domestic servant with hasagevalue -> nan (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> domestic servant with hasbirthdate -> c. 1799 (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> domestic servant with hasdeathdate -> 1874 (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> domestic servant with hassex -> male (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> domestic servant with hasstatusgeneratedevent -> enslavement, freedom (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> domestic servant with hasvalue -> enslaved person, free person (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> domestic servant with hasvalue -> abolitionist, writer (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> domestic servant with hasinteragentrelationshiptype -> enslaved by (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> domestic servant with isrelationshipto -> james madison (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> domestic servant with isrelationshipfrom -> paul jennings (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> domestic servant with hasparticipantroletype -> memoir writer (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> domestic servant with roleprovidedby -> publication of the first white house memoir (Subject: Paul Jennings (abolitionist))\n",
            "No match found for truth row: Paul Jennings, Predicate: hasdescriptiveoccupation, Object: domestic servant\n",
            "Comparing: hasdescriptiveoccupation -> slave with haspreferrednamevariant -> paul jennings (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> slave with fullnameasstring -> paul jennings (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> slave with hassurnameasstring -> jennings (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasfirstnameasstring -> paul (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasdescription -> an american abolitionist and writer. enslaved as a young man by president james madison during and after his white house years, jennings published, in 1865, the first white house memoir. (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> slave with referstoplaceoforigin -> washington, d.c. (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> slave with has_race -> african-american (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasagevalue -> nan (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasbirthdate -> c. 1799 (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasdeathdate -> 1874 (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> slave with hassex -> male (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasstatusgeneratedevent -> enslavement, freedom (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasvalue -> enslaved person, free person (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasvalue -> abolitionist, writer (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasinteragentrelationshiptype -> enslaved by (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> slave with isrelationshipto -> james madison (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> slave with isrelationshipfrom -> paul jennings (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasparticipantroletype -> memoir writer (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> slave with roleprovidedby -> publication of the first white house memoir (Subject: Paul Jennings (abolitionist))\n",
            "No match found for truth row: Paul Jennings, Predicate: hasdescriptiveoccupation, Object: slave\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with haspreferrednamevariant -> paul jennings (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with fullnameasstring -> paul jennings (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with hassurnameasstring -> jennings (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with hasfirstnameasstring -> paul (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with hasdescription -> an american abolitionist and writer. enslaved as a young man by president james madison during and after his white house years, jennings published, in 1865, the first white house memoir. (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with referstoplaceoforigin -> washington, d.c. (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with has_race -> african-american (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with hasagevalue -> nan (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with hasbirthdate -> c. 1799 (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with hasdeathdate -> 1874 (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with hassex -> male (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with hasstatusgeneratedevent -> enslavement, freedom (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with hasvalue -> enslaved person, free person (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with hasvalue -> abolitionist, writer (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with hasinteragentrelationshiptype -> enslaved by (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with isrelationshipto -> james madison (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with isrelationshipfrom -> paul jennings (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with hasparticipantroletype -> memoir writer (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with roleprovidedby -> publication of the first white house memoir (Subject: Paul Jennings (abolitionist))\n",
            "No match found for truth row: Paul Jennings, Predicate: hasdescriptiveoccupation, Object: autobiographer / memoirist\n",
            "Comparing: hasfirstname -> paul with haspreferrednamevariant -> paul jennings (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasfirstname -> paul with fullnameasstring -> paul jennings (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasfirstname -> paul with hassurnameasstring -> jennings (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasfirstname -> paul with hasfirstnameasstring -> paul (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasfirstname -> paul with hasdescription -> an american abolitionist and writer. enslaved as a young man by president james madison during and after his white house years, jennings published, in 1865, the first white house memoir. (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasfirstname -> paul with referstoplaceoforigin -> washington, d.c. (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasfirstname -> paul with has_race -> african-american (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasfirstname -> paul with hasagevalue -> nan (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasfirstname -> paul with hasbirthdate -> c. 1799 (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasfirstname -> paul with hasdeathdate -> 1874 (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasfirstname -> paul with hassex -> male (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasfirstname -> paul with hasstatusgeneratedevent -> enslavement, freedom (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasfirstname -> paul with hasvalue -> enslaved person, free person (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasfirstname -> paul with hasvalue -> abolitionist, writer (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasfirstname -> paul with hasinteragentrelationshiptype -> enslaved by (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasfirstname -> paul with isrelationshipto -> james madison (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasfirstname -> paul with isrelationshipfrom -> paul jennings (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasfirstname -> paul with hasparticipantroletype -> memoir writer (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasfirstname -> paul with roleprovidedby -> publication of the first white house memoir (Subject: Paul Jennings (abolitionist))\n",
            "No match found for truth row: Paul Jennings, Predicate: hasfirstname, Object: paul\n",
            "Comparing: hassurname -> jennings with haspreferrednamevariant -> paul jennings (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hassurname -> jennings with fullnameasstring -> paul jennings (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hassurname -> jennings with hassurnameasstring -> jennings (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hassurname -> jennings with hasfirstnameasstring -> paul (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hassurname -> jennings with hasdescription -> an american abolitionist and writer. enslaved as a young man by president james madison during and after his white house years, jennings published, in 1865, the first white house memoir. (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hassurname -> jennings with referstoplaceoforigin -> washington, d.c. (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hassurname -> jennings with has_race -> african-american (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hassurname -> jennings with hasagevalue -> nan (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hassurname -> jennings with hasbirthdate -> c. 1799 (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hassurname -> jennings with hasdeathdate -> 1874 (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hassurname -> jennings with hassex -> male (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hassurname -> jennings with hasstatusgeneratedevent -> enslavement, freedom (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hassurname -> jennings with hasvalue -> enslaved person, free person (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hassurname -> jennings with hasvalue -> abolitionist, writer (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hassurname -> jennings with hasinteragentrelationshiptype -> enslaved by (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hassurname -> jennings with isrelationshipto -> james madison (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hassurname -> jennings with isrelationshipfrom -> paul jennings (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hassurname -> jennings with hasparticipantroletype -> memoir writer (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hassurname -> jennings with roleprovidedby -> publication of the first white house memoir (Subject: Paul Jennings (abolitionist))\n",
            "No match found for truth row: Paul Jennings, Predicate: hassurname, Object: jennings\n",
            "Comparing: hasname -> paul jennings with haspreferrednamevariant -> paul jennings (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasname -> paul jennings with fullnameasstring -> paul jennings (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasname -> paul jennings with hassurnameasstring -> jennings (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasname -> paul jennings with hasfirstnameasstring -> paul (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasname -> paul jennings with hasdescription -> an american abolitionist and writer. enslaved as a young man by president james madison during and after his white house years, jennings published, in 1865, the first white house memoir. (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasname -> paul jennings with referstoplaceoforigin -> washington, d.c. (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasname -> paul jennings with has_race -> african-american (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasname -> paul jennings with hasagevalue -> nan (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasname -> paul jennings with hasbirthdate -> c. 1799 (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasname -> paul jennings with hasdeathdate -> 1874 (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasname -> paul jennings with hassex -> male (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasname -> paul jennings with hasstatusgeneratedevent -> enslavement, freedom (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasname -> paul jennings with hasvalue -> enslaved person, free person (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasname -> paul jennings with hasvalue -> abolitionist, writer (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasname -> paul jennings with hasinteragentrelationshiptype -> enslaved by (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasname -> paul jennings with isrelationshipto -> james madison (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasname -> paul jennings with isrelationshipfrom -> paul jennings (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasname -> paul jennings with hasparticipantroletype -> memoir writer (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasname -> paul jennings with roleprovidedby -> publication of the first white house memoir (Subject: Paul Jennings (abolitionist))\n",
            "No match found for truth row: Paul Jennings, Predicate: hasname, Object: paul jennings\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. formerly enslaved as a personal assistant to president james madison, jennings had a memoir about his time in the oval office published in book form in 1865. with haspreferrednamevariant -> paul jennings (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. formerly enslaved as a personal assistant to president james madison, jennings had a memoir about his time in the oval office published in book form in 1865. with fullnameasstring -> paul jennings (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. formerly enslaved as a personal assistant to president james madison, jennings had a memoir about his time in the oval office published in book form in 1865. with hassurnameasstring -> jennings (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. formerly enslaved as a personal assistant to president james madison, jennings had a memoir about his time in the oval office published in book form in 1865. with hasfirstnameasstring -> paul (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. formerly enslaved as a personal assistant to president james madison, jennings had a memoir about his time in the oval office published in book form in 1865. with hasdescription -> an american abolitionist and writer. enslaved as a young man by president james madison during and after his white house years, jennings published, in 1865, the first white house memoir. (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. formerly enslaved as a personal assistant to president james madison, jennings had a memoir about his time in the oval office published in book form in 1865. with referstoplaceoforigin -> washington, d.c. (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. formerly enslaved as a personal assistant to president james madison, jennings had a memoir about his time in the oval office published in book form in 1865. with has_race -> african-american (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. formerly enslaved as a personal assistant to president james madison, jennings had a memoir about his time in the oval office published in book form in 1865. with hasagevalue -> nan (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. formerly enslaved as a personal assistant to president james madison, jennings had a memoir about his time in the oval office published in book form in 1865. with hasbirthdate -> c. 1799 (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. formerly enslaved as a personal assistant to president james madison, jennings had a memoir about his time in the oval office published in book form in 1865. with hasdeathdate -> 1874 (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. formerly enslaved as a personal assistant to president james madison, jennings had a memoir about his time in the oval office published in book form in 1865. with hassex -> male (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. formerly enslaved as a personal assistant to president james madison, jennings had a memoir about his time in the oval office published in book form in 1865. with hasstatusgeneratedevent -> enslavement, freedom (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. formerly enslaved as a personal assistant to president james madison, jennings had a memoir about his time in the oval office published in book form in 1865. with hasvalue -> enslaved person, free person (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. formerly enslaved as a personal assistant to president james madison, jennings had a memoir about his time in the oval office published in book form in 1865. with hasvalue -> abolitionist, writer (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. formerly enslaved as a personal assistant to president james madison, jennings had a memoir about his time in the oval office published in book form in 1865. with hasinteragentrelationshiptype -> enslaved by (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. formerly enslaved as a personal assistant to president james madison, jennings had a memoir about his time in the oval office published in book form in 1865. with isrelationshipto -> james madison (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. formerly enslaved as a personal assistant to president james madison, jennings had a memoir about his time in the oval office published in book form in 1865. with isrelationshipfrom -> paul jennings (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. formerly enslaved as a personal assistant to president james madison, jennings had a memoir about his time in the oval office published in book form in 1865. with hasparticipantroletype -> memoir writer (Subject: Paul Jennings (abolitionist))\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. formerly enslaved as a personal assistant to president james madison, jennings had a memoir about his time in the oval office published in book form in 1865. with roleprovidedby -> publication of the first white house memoir (Subject: Paul Jennings (abolitionist))\n",
            "No match found for truth row: Paul Jennings, Predicate: hasdescription, Object: enslaved. free before 13th amendment. formerly enslaved as a personal assistant to president james madison, jennings had a memoir about his time in the oval office published in book form in 1865.\n",
            "\n",
            "Processing truth file: Thomas_Lewis_Johnson.tsv\n",
            "Truth DataFrame:\n",
            "                       0                         1  \\\n",
            "0   Thomas Lewis Johnson               instance of   \n",
            "1   Thomas Lewis Johnson                    hasSex   \n",
            "2   Thomas Lewis Johnson        hasParticipantRole   \n",
            "3   Thomas Lewis Johnson        hasParticipantRole   \n",
            "4   Thomas Lewis Johnson  hasDescriptiveOccupation   \n",
            "5   Thomas Lewis Johnson  hasDescriptiveOccupation   \n",
            "6   Thomas Lewis Johnson  hasDescriptiveOccupation   \n",
            "7   Thomas Lewis Johnson  hasDescriptiveOccupation   \n",
            "8   Thomas Lewis Johnson              hasFirstName   \n",
            "9   Thomas Lewis Johnson                hasSurname   \n",
            "10  Thomas Lewis Johnson                   hasName   \n",
            "11  Thomas Lewis Johnson            hasDescription   \n",
            "\n",
            "                                                    2  \n",
            "0                                              Person  \n",
            "1                                                Male  \n",
            "2                                               Child  \n",
            "3                                     Deceased Person  \n",
            "4                                               Slave  \n",
            "5                                          Missionary  \n",
            "6                              Slave Narrative Author  \n",
            "7                                      Baptist Clergy  \n",
            "8                                        Thomas Lewis  \n",
            "9                                             Johnson  \n",
            "10                               Thomas Lewis Johnson  \n",
            "11  Born Enslaved. A Baptist preacher and author w...  \n",
            "\n",
            "Found matching generated file: Thomas_Lewis_Johnson_triples.tsv\n",
            "Generated DataFrame before transformation:\n",
            "                         Subject                Predicate  \\\n",
            "1   Thomas_Lewis_Johnson_triples  hasPreferredNameVariant   \n",
            "2   Thomas_Lewis_Johnson_triples         fullNameAsString   \n",
            "3   Thomas_Lewis_Johnson_triples       hasSurnameAsString   \n",
            "4   Thomas_Lewis_Johnson_triples     hasFirstnameAsString   \n",
            "5   Thomas_Lewis_Johnson_triples           hasDescription   \n",
            "6   Thomas_Lewis_Johnson_triples    refersToPlaceOfOrigin   \n",
            "7   Thomas_Lewis_Johnson_triples                 has_Race   \n",
            "8   Thomas_Lewis_Johnson_triples              hasAgeValue   \n",
            "9   Thomas_Lewis_Johnson_triples             hasBirthDate   \n",
            "10  Thomas_Lewis_Johnson_triples                   hasSex   \n",
            "11  Thomas_Lewis_Johnson_triples  hasStatusGeneratedEvent   \n",
            "12  Thomas_Lewis_Johnson_triples                 hasValue   \n",
            "13  Thomas_Lewis_Johnson_triples                 hasValue   \n",
            "14  Thomas_Lewis_Johnson_triples   hasParticipantRoleType   \n",
            "15  Thomas_Lewis_Johnson_triples           roleProvidedBy   \n",
            "\n",
            "                                               Object  \n",
            "1                                        Lennox Lewis  \n",
            "2                                        Lennox Lewis  \n",
            "3                                               Lewis  \n",
            "4                                              Lennox  \n",
            "5   Boxing commentator and former professional box...  \n",
            "6                              United Kingdom, Canada  \n",
            "7                                               Black  \n",
            "8                                                 NaN  \n",
            "9                                    2 September 1965  \n",
            "10                                               Male  \n",
            "11                                      Boxing career  \n",
            "12                    Professional Boxer, Commentator  \n",
            "13                                 Boxer, Commentator  \n",
            "14                                              Boxer  \n",
            "15                                     Boxing Matches  \n",
            "Generated DataFrame after transformation:\n",
            "                 Subject                Predicate  \\\n",
            "1   Thomas Lewis Johnson  haspreferrednamevariant   \n",
            "2   Thomas Lewis Johnson         fullnameasstring   \n",
            "3   Thomas Lewis Johnson       hassurnameasstring   \n",
            "4   Thomas Lewis Johnson     hasfirstnameasstring   \n",
            "5   Thomas Lewis Johnson           hasdescription   \n",
            "6   Thomas Lewis Johnson    referstoplaceoforigin   \n",
            "7   Thomas Lewis Johnson                 has_race   \n",
            "8   Thomas Lewis Johnson              hasagevalue   \n",
            "9   Thomas Lewis Johnson             hasbirthdate   \n",
            "10  Thomas Lewis Johnson                   hassex   \n",
            "11  Thomas Lewis Johnson  hasstatusgeneratedevent   \n",
            "12  Thomas Lewis Johnson                 hasvalue   \n",
            "13  Thomas Lewis Johnson                 hasvalue   \n",
            "14  Thomas Lewis Johnson   hasparticipantroletype   \n",
            "15  Thomas Lewis Johnson           roleprovidedby   \n",
            "\n",
            "                                               Object  \n",
            "1                                        lennox lewis  \n",
            "2                                        lennox lewis  \n",
            "3                                               lewis  \n",
            "4                                              lennox  \n",
            "5   boxing commentator and former professional box...  \n",
            "6                              united kingdom, canada  \n",
            "7                                               black  \n",
            "8                                                 NaN  \n",
            "9                                    2 september 1965  \n",
            "10                                               male  \n",
            "11                                      boxing career  \n",
            "12                    professional boxer, commentator  \n",
            "13                                 boxer, commentator  \n",
            "14                                              boxer  \n",
            "15                                     boxing matches  \n",
            "Comparing: instance of -> person with haspreferrednamevariant -> lennox lewis (Subject: Thomas Lewis Johnson)\n",
            "Comparing: instance of -> person with fullnameasstring -> lennox lewis (Subject: Thomas Lewis Johnson)\n",
            "Comparing: instance of -> person with hassurnameasstring -> lewis (Subject: Thomas Lewis Johnson)\n",
            "Comparing: instance of -> person with hasfirstnameasstring -> lennox (Subject: Thomas Lewis Johnson)\n",
            "Comparing: instance of -> person with hasdescription -> boxing commentator and former professional boxer who competed from 1989 to 2003. he is a three-time world heavyweight champion, a two-time lineal champion, and held the undisputed championship. (Subject: Thomas Lewis Johnson)\n",
            "Comparing: instance of -> person with referstoplaceoforigin -> united kingdom, canada (Subject: Thomas Lewis Johnson)\n",
            "Comparing: instance of -> person with has_race -> black (Subject: Thomas Lewis Johnson)\n",
            "Comparing: instance of -> person with hasagevalue -> nan (Subject: Thomas Lewis Johnson)\n",
            "Comparing: instance of -> person with hasbirthdate -> 2 september 1965 (Subject: Thomas Lewis Johnson)\n",
            "Comparing: instance of -> person with hassex -> male (Subject: Thomas Lewis Johnson)\n",
            "Comparing: instance of -> person with hasstatusgeneratedevent -> boxing career (Subject: Thomas Lewis Johnson)\n",
            "Comparing: instance of -> person with hasvalue -> professional boxer, commentator (Subject: Thomas Lewis Johnson)\n",
            "Comparing: instance of -> person with hasvalue -> boxer, commentator (Subject: Thomas Lewis Johnson)\n",
            "Comparing: instance of -> person with hasparticipantroletype -> boxer (Subject: Thomas Lewis Johnson)\n",
            "Comparing: instance of -> person with roleprovidedby -> boxing matches (Subject: Thomas Lewis Johnson)\n",
            "No match found for truth row: Thomas Lewis Johnson, Predicate: instance of, Object: person\n",
            "Comparing: hassex -> male with haspreferrednamevariant -> lennox lewis (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hassex -> male with fullnameasstring -> lennox lewis (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hassex -> male with hassurnameasstring -> lewis (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hassex -> male with hasfirstnameasstring -> lennox (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hassex -> male with hasdescription -> boxing commentator and former professional boxer who competed from 1989 to 2003. he is a three-time world heavyweight champion, a two-time lineal champion, and held the undisputed championship. (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hassex -> male with referstoplaceoforigin -> united kingdom, canada (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hassex -> male with has_race -> black (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hassex -> male with hasagevalue -> nan (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hassex -> male with hasbirthdate -> 2 september 1965 (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hassex -> male with hassex -> male (Subject: Thomas Lewis Johnson)\n",
            "Match found for truth row Thomas Lewis Johnson with generated row Thomas Lewis Johnson\n",
            "Comparing: hassex -> male with hasstatusgeneratedevent -> boxing career (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hassex -> male with hasvalue -> professional boxer, commentator (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hassex -> male with hasvalue -> boxer, commentator (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hassex -> male with hasparticipantroletype -> boxer (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hassex -> male with roleprovidedby -> boxing matches (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasparticipantrole -> child with haspreferrednamevariant -> lennox lewis (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasparticipantrole -> child with fullnameasstring -> lennox lewis (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasparticipantrole -> child with hassurnameasstring -> lewis (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasparticipantrole -> child with hasfirstnameasstring -> lennox (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasparticipantrole -> child with hasdescription -> boxing commentator and former professional boxer who competed from 1989 to 2003. he is a three-time world heavyweight champion, a two-time lineal champion, and held the undisputed championship. (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasparticipantrole -> child with referstoplaceoforigin -> united kingdom, canada (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasparticipantrole -> child with has_race -> black (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasparticipantrole -> child with hasagevalue -> nan (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasparticipantrole -> child with hasbirthdate -> 2 september 1965 (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasparticipantrole -> child with hassex -> male (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasparticipantrole -> child with hasstatusgeneratedevent -> boxing career (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasparticipantrole -> child with hasvalue -> professional boxer, commentator (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasparticipantrole -> child with hasvalue -> boxer, commentator (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasparticipantrole -> child with hasparticipantroletype -> boxer (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasparticipantrole -> child with roleprovidedby -> boxing matches (Subject: Thomas Lewis Johnson)\n",
            "No match found for truth row: Thomas Lewis Johnson, Predicate: hasparticipantrole, Object: child\n",
            "Comparing: hasparticipantrole -> deceased person with haspreferrednamevariant -> lennox lewis (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasparticipantrole -> deceased person with fullnameasstring -> lennox lewis (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasparticipantrole -> deceased person with hassurnameasstring -> lewis (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasparticipantrole -> deceased person with hasfirstnameasstring -> lennox (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasparticipantrole -> deceased person with hasdescription -> boxing commentator and former professional boxer who competed from 1989 to 2003. he is a three-time world heavyweight champion, a two-time lineal champion, and held the undisputed championship. (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasparticipantrole -> deceased person with referstoplaceoforigin -> united kingdom, canada (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasparticipantrole -> deceased person with has_race -> black (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasparticipantrole -> deceased person with hasagevalue -> nan (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasparticipantrole -> deceased person with hasbirthdate -> 2 september 1965 (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasparticipantrole -> deceased person with hassex -> male (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasparticipantrole -> deceased person with hasstatusgeneratedevent -> boxing career (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasparticipantrole -> deceased person with hasvalue -> professional boxer, commentator (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasparticipantrole -> deceased person with hasvalue -> boxer, commentator (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasparticipantrole -> deceased person with hasparticipantroletype -> boxer (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasparticipantrole -> deceased person with roleprovidedby -> boxing matches (Subject: Thomas Lewis Johnson)\n",
            "No match found for truth row: Thomas Lewis Johnson, Predicate: hasparticipantrole, Object: deceased person\n",
            "Comparing: hasdescriptiveoccupation -> slave with haspreferrednamevariant -> lennox lewis (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> slave with fullnameasstring -> lennox lewis (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hassurnameasstring -> lewis (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasfirstnameasstring -> lennox (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasdescription -> boxing commentator and former professional boxer who competed from 1989 to 2003. he is a three-time world heavyweight champion, a two-time lineal champion, and held the undisputed championship. (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> slave with referstoplaceoforigin -> united kingdom, canada (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> slave with has_race -> black (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasagevalue -> nan (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasbirthdate -> 2 september 1965 (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hassex -> male (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasstatusgeneratedevent -> boxing career (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasvalue -> professional boxer, commentator (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasvalue -> boxer, commentator (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasparticipantroletype -> boxer (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> slave with roleprovidedby -> boxing matches (Subject: Thomas Lewis Johnson)\n",
            "No match found for truth row: Thomas Lewis Johnson, Predicate: hasdescriptiveoccupation, Object: slave\n",
            "Comparing: hasdescriptiveoccupation -> missionary with haspreferrednamevariant -> lennox lewis (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> missionary with fullnameasstring -> lennox lewis (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> missionary with hassurnameasstring -> lewis (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> missionary with hasfirstnameasstring -> lennox (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> missionary with hasdescription -> boxing commentator and former professional boxer who competed from 1989 to 2003. he is a three-time world heavyweight champion, a two-time lineal champion, and held the undisputed championship. (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> missionary with referstoplaceoforigin -> united kingdom, canada (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> missionary with has_race -> black (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> missionary with hasagevalue -> nan (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> missionary with hasbirthdate -> 2 september 1965 (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> missionary with hassex -> male (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> missionary with hasstatusgeneratedevent -> boxing career (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> missionary with hasvalue -> professional boxer, commentator (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> missionary with hasvalue -> boxer, commentator (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> missionary with hasparticipantroletype -> boxer (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> missionary with roleprovidedby -> boxing matches (Subject: Thomas Lewis Johnson)\n",
            "No match found for truth row: Thomas Lewis Johnson, Predicate: hasdescriptiveoccupation, Object: missionary\n",
            "Comparing: hasdescriptiveoccupation -> slave narrative author with haspreferrednamevariant -> lennox lewis (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> slave narrative author with fullnameasstring -> lennox lewis (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> slave narrative author with hassurnameasstring -> lewis (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> slave narrative author with hasfirstnameasstring -> lennox (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> slave narrative author with hasdescription -> boxing commentator and former professional boxer who competed from 1989 to 2003. he is a three-time world heavyweight champion, a two-time lineal champion, and held the undisputed championship. (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> slave narrative author with referstoplaceoforigin -> united kingdom, canada (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> slave narrative author with has_race -> black (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> slave narrative author with hasagevalue -> nan (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> slave narrative author with hasbirthdate -> 2 september 1965 (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> slave narrative author with hassex -> male (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> slave narrative author with hasstatusgeneratedevent -> boxing career (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> slave narrative author with hasvalue -> professional boxer, commentator (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> slave narrative author with hasvalue -> boxer, commentator (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> slave narrative author with hasparticipantroletype -> boxer (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> slave narrative author with roleprovidedby -> boxing matches (Subject: Thomas Lewis Johnson)\n",
            "No match found for truth row: Thomas Lewis Johnson, Predicate: hasdescriptiveoccupation, Object: slave narrative author\n",
            "Comparing: hasdescriptiveoccupation -> baptist clergy with haspreferrednamevariant -> lennox lewis (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> baptist clergy with fullnameasstring -> lennox lewis (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> baptist clergy with hassurnameasstring -> lewis (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> baptist clergy with hasfirstnameasstring -> lennox (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> baptist clergy with hasdescription -> boxing commentator and former professional boxer who competed from 1989 to 2003. he is a three-time world heavyweight champion, a two-time lineal champion, and held the undisputed championship. (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> baptist clergy with referstoplaceoforigin -> united kingdom, canada (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> baptist clergy with has_race -> black (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> baptist clergy with hasagevalue -> nan (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> baptist clergy with hasbirthdate -> 2 september 1965 (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> baptist clergy with hassex -> male (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> baptist clergy with hasstatusgeneratedevent -> boxing career (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> baptist clergy with hasvalue -> professional boxer, commentator (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> baptist clergy with hasvalue -> boxer, commentator (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> baptist clergy with hasparticipantroletype -> boxer (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescriptiveoccupation -> baptist clergy with roleprovidedby -> boxing matches (Subject: Thomas Lewis Johnson)\n",
            "No match found for truth row: Thomas Lewis Johnson, Predicate: hasdescriptiveoccupation, Object: baptist clergy\n",
            "Comparing: hasfirstname -> thomas lewis with haspreferrednamevariant -> lennox lewis (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasfirstname -> thomas lewis with fullnameasstring -> lennox lewis (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasfirstname -> thomas lewis with hassurnameasstring -> lewis (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasfirstname -> thomas lewis with hasfirstnameasstring -> lennox (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasfirstname -> thomas lewis with hasdescription -> boxing commentator and former professional boxer who competed from 1989 to 2003. he is a three-time world heavyweight champion, a two-time lineal champion, and held the undisputed championship. (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasfirstname -> thomas lewis with referstoplaceoforigin -> united kingdom, canada (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasfirstname -> thomas lewis with has_race -> black (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasfirstname -> thomas lewis with hasagevalue -> nan (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasfirstname -> thomas lewis with hasbirthdate -> 2 september 1965 (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasfirstname -> thomas lewis with hassex -> male (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasfirstname -> thomas lewis with hasstatusgeneratedevent -> boxing career (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasfirstname -> thomas lewis with hasvalue -> professional boxer, commentator (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasfirstname -> thomas lewis with hasvalue -> boxer, commentator (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasfirstname -> thomas lewis with hasparticipantroletype -> boxer (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasfirstname -> thomas lewis with roleprovidedby -> boxing matches (Subject: Thomas Lewis Johnson)\n",
            "No match found for truth row: Thomas Lewis Johnson, Predicate: hasfirstname, Object: thomas lewis\n",
            "Comparing: hassurname -> johnson with haspreferrednamevariant -> lennox lewis (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hassurname -> johnson with fullnameasstring -> lennox lewis (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hassurname -> johnson with hassurnameasstring -> lewis (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hassurname -> johnson with hasfirstnameasstring -> lennox (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hassurname -> johnson with hasdescription -> boxing commentator and former professional boxer who competed from 1989 to 2003. he is a three-time world heavyweight champion, a two-time lineal champion, and held the undisputed championship. (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hassurname -> johnson with referstoplaceoforigin -> united kingdom, canada (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hassurname -> johnson with has_race -> black (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hassurname -> johnson with hasagevalue -> nan (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hassurname -> johnson with hasbirthdate -> 2 september 1965 (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hassurname -> johnson with hassex -> male (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hassurname -> johnson with hasstatusgeneratedevent -> boxing career (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hassurname -> johnson with hasvalue -> professional boxer, commentator (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hassurname -> johnson with hasvalue -> boxer, commentator (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hassurname -> johnson with hasparticipantroletype -> boxer (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hassurname -> johnson with roleprovidedby -> boxing matches (Subject: Thomas Lewis Johnson)\n",
            "No match found for truth row: Thomas Lewis Johnson, Predicate: hassurname, Object: johnson\n",
            "Comparing: hasname -> thomas lewis johnson with haspreferrednamevariant -> lennox lewis (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasname -> thomas lewis johnson with fullnameasstring -> lennox lewis (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasname -> thomas lewis johnson with hassurnameasstring -> lewis (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasname -> thomas lewis johnson with hasfirstnameasstring -> lennox (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasname -> thomas lewis johnson with hasdescription -> boxing commentator and former professional boxer who competed from 1989 to 2003. he is a three-time world heavyweight champion, a two-time lineal champion, and held the undisputed championship. (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasname -> thomas lewis johnson with referstoplaceoforigin -> united kingdom, canada (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasname -> thomas lewis johnson with has_race -> black (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasname -> thomas lewis johnson with hasagevalue -> nan (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasname -> thomas lewis johnson with hasbirthdate -> 2 september 1965 (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasname -> thomas lewis johnson with hassex -> male (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasname -> thomas lewis johnson with hasstatusgeneratedevent -> boxing career (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasname -> thomas lewis johnson with hasvalue -> professional boxer, commentator (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasname -> thomas lewis johnson with hasvalue -> boxer, commentator (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasname -> thomas lewis johnson with hasparticipantroletype -> boxer (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasname -> thomas lewis johnson with roleprovidedby -> boxing matches (Subject: Thomas Lewis Johnson)\n",
            "No match found for truth row: Thomas Lewis Johnson, Predicate: hasname, Object: thomas lewis johnson\n",
            "Comparing: hasdescription -> born enslaved. a baptist preacher and author who was a missionary in cameroon, and worked and raised money for missionary purposes in both the u.s. and great britain. with haspreferrednamevariant -> lennox lewis (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescription -> born enslaved. a baptist preacher and author who was a missionary in cameroon, and worked and raised money for missionary purposes in both the u.s. and great britain. with fullnameasstring -> lennox lewis (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescription -> born enslaved. a baptist preacher and author who was a missionary in cameroon, and worked and raised money for missionary purposes in both the u.s. and great britain. with hassurnameasstring -> lewis (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescription -> born enslaved. a baptist preacher and author who was a missionary in cameroon, and worked and raised money for missionary purposes in both the u.s. and great britain. with hasfirstnameasstring -> lennox (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescription -> born enslaved. a baptist preacher and author who was a missionary in cameroon, and worked and raised money for missionary purposes in both the u.s. and great britain. with hasdescription -> boxing commentator and former professional boxer who competed from 1989 to 2003. he is a three-time world heavyweight champion, a two-time lineal champion, and held the undisputed championship. (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescription -> born enslaved. a baptist preacher and author who was a missionary in cameroon, and worked and raised money for missionary purposes in both the u.s. and great britain. with referstoplaceoforigin -> united kingdom, canada (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescription -> born enslaved. a baptist preacher and author who was a missionary in cameroon, and worked and raised money for missionary purposes in both the u.s. and great britain. with has_race -> black (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescription -> born enslaved. a baptist preacher and author who was a missionary in cameroon, and worked and raised money for missionary purposes in both the u.s. and great britain. with hasagevalue -> nan (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescription -> born enslaved. a baptist preacher and author who was a missionary in cameroon, and worked and raised money for missionary purposes in both the u.s. and great britain. with hasbirthdate -> 2 september 1965 (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescription -> born enslaved. a baptist preacher and author who was a missionary in cameroon, and worked and raised money for missionary purposes in both the u.s. and great britain. with hassex -> male (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescription -> born enslaved. a baptist preacher and author who was a missionary in cameroon, and worked and raised money for missionary purposes in both the u.s. and great britain. with hasstatusgeneratedevent -> boxing career (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescription -> born enslaved. a baptist preacher and author who was a missionary in cameroon, and worked and raised money for missionary purposes in both the u.s. and great britain. with hasvalue -> professional boxer, commentator (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescription -> born enslaved. a baptist preacher and author who was a missionary in cameroon, and worked and raised money for missionary purposes in both the u.s. and great britain. with hasvalue -> boxer, commentator (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescription -> born enslaved. a baptist preacher and author who was a missionary in cameroon, and worked and raised money for missionary purposes in both the u.s. and great britain. with hasparticipantroletype -> boxer (Subject: Thomas Lewis Johnson)\n",
            "Comparing: hasdescription -> born enslaved. a baptist preacher and author who was a missionary in cameroon, and worked and raised money for missionary purposes in both the u.s. and great britain. with roleprovidedby -> boxing matches (Subject: Thomas Lewis Johnson)\n",
            "No match found for truth row: Thomas Lewis Johnson, Predicate: hasdescription, Object: born enslaved. a baptist preacher and author who was a missionary in cameroon, and worked and raised money for missionary purposes in both the u.s. and great britain.\n",
            "\n",
            "Processing truth file: James_W.C._Pennington.tsv\n",
            "Truth DataFrame:\n",
            "                                   0                         1  \\\n",
            "0   James William Charles Pennington               instance of   \n",
            "1   James William Charles Pennington                    hasSex   \n",
            "2   James William Charles Pennington        hasParticipantRole   \n",
            "3   James William Charles Pennington        hasParticipantRole   \n",
            "4   James William Charles Pennington  hasDescriptiveOccupation   \n",
            "5   James William Charles Pennington  hasDescriptiveOccupation   \n",
            "6   James William Charles Pennington  hasDescriptiveOccupation   \n",
            "7   James William Charles Pennington              hasFirstName   \n",
            "8   James William Charles Pennington                hasSurname   \n",
            "9   James William Charles Pennington                   hasName   \n",
            "10  James William Charles Pennington            hasDescription   \n",
            "\n",
            "                                                    2  \n",
            "0                                              Person  \n",
            "1                                                Male  \n",
            "2                                               Child  \n",
            "3                                     Deceased Person  \n",
            "4                                               Slave  \n",
            "5                                        Abolitionist  \n",
            "6                               Congregational Clergy  \n",
            "7                               James William Charles  \n",
            "8                                          Pennington  \n",
            "9                    James William Charles Pennington  \n",
            "10  Enslaved. Free before 13th Amendment. Penningt...  \n",
            "\n",
            "Processing truth file: Lott_Cary.tsv\n",
            "Truth DataFrame:\n",
            "            0                         1  \\\n",
            "0   Lott Cary               instance of   \n",
            "1   Lott Cary                    hasSex   \n",
            "2   Lott Cary        hasParticipantRole   \n",
            "3   Lott Cary        hasParticipantRole   \n",
            "4   Lott Cary  hasDescriptiveOccupation   \n",
            "5   Lott Cary  hasDescriptiveOccupation   \n",
            "6   Lott Cary  hasDescriptiveOccupation   \n",
            "7   Lott Cary              hasFirstName   \n",
            "8   Lott Cary                hasSurname   \n",
            "9   Lott Cary                   hasName   \n",
            "10  Lott Cary            hasDescription   \n",
            "\n",
            "                                                    2  \n",
            "0                                              Person  \n",
            "1                                                Male  \n",
            "2                                               Child  \n",
            "3                                     Deceased Person  \n",
            "4                                               Slave  \n",
            "5                                          Missionary  \n",
            "6                                      Baptist Clergy  \n",
            "7                                                Lott  \n",
            "8                                                Cary  \n",
            "9                                           Lott Cary  \n",
            "10  Enslaved. Free before 13th Amendment. A Baptis...  \n",
            "\n",
            "Processing truth file: Pierre_Caliste_Landry.tsv\n",
            "Truth DataFrame:\n",
            "                        0                         1  \\\n",
            "0   Pierre Caliste Landry               instance of   \n",
            "1   Pierre Caliste Landry                    hasSex   \n",
            "2   Pierre Caliste Landry        hasParticipantRole   \n",
            "3   Pierre Caliste Landry        hasParticipantRole   \n",
            "4   Pierre Caliste Landry  hasDescriptiveOccupation   \n",
            "5   Pierre Caliste Landry  hasDescriptiveOccupation   \n",
            "6   Pierre Caliste Landry  hasDescriptiveOccupation   \n",
            "7   Pierre Caliste Landry  hasDescriptiveOccupation   \n",
            "8   Pierre Caliste Landry  hasDescriptiveOccupation   \n",
            "9   Pierre Caliste Landry              hasFirstName   \n",
            "10  Pierre Caliste Landry                hasSurname   \n",
            "11  Pierre Caliste Landry                   hasName   \n",
            "12  Pierre Caliste Landry            hasDescription   \n",
            "\n",
            "                                                    2  \n",
            "0                                              Person  \n",
            "1                                                Male  \n",
            "2                                               Child  \n",
            "3                                     Deceased Person  \n",
            "4                                               Slave  \n",
            "5                                    State Legislator  \n",
            "6                                              Lawyer  \n",
            "7                  Institutional Founder / Benefactor  \n",
            "8                          Methodist Episcopal Clergy  \n",
            "9                                      Pierre Caliste  \n",
            "10                                             Landry  \n",
            "11                              Pierre Caliste Landry  \n",
            "12  Enslaved. When elected mayor of Donaldsonville...  \n",
            "\n",
            "Found matching generated file: Pierre_Caliste_Landry_triples.tsv\n",
            "Generated DataFrame before transformation:\n",
            "                          Subject                Predicate  \\\n",
            "1   Pierre_Caliste_Landry_triples  hasPreferredNameVariant   \n",
            "2   Pierre_Caliste_Landry_triples         fullNameAsString   \n",
            "3   Pierre_Caliste_Landry_triples       hasSurnameAsString   \n",
            "4   Pierre_Caliste_Landry_triples     hasFirstnameAsString   \n",
            "5   Pierre_Caliste_Landry_triples           hasDescription   \n",
            "6   Pierre_Caliste_Landry_triples    refersToPlaceOfOrigin   \n",
            "7   Pierre_Caliste_Landry_triples                 has_Race   \n",
            "8   Pierre_Caliste_Landry_triples             hasBirthDate   \n",
            "9   Pierre_Caliste_Landry_triples             hasDeathDate   \n",
            "10  Pierre_Caliste_Landry_triples                   hasSex   \n",
            "11  Pierre_Caliste_Landry_triples  hasStatusGeneratedEvent   \n",
            "12  Pierre_Caliste_Landry_triples                 hasValue   \n",
            "13  Pierre_Caliste_Landry_triples                 hasValue   \n",
            "14  Pierre_Caliste_Landry_triples         isRelationshipTo   \n",
            "15  Pierre_Caliste_Landry_triples       isRelationshipFrom   \n",
            "16  Pierre_Caliste_Landry_triples   hasParticipantRoleType   \n",
            "17  Pierre_Caliste_Landry_triples           roleProvidedBy   \n",
            "\n",
            "                                               Object  \n",
            "1                               Pierre Caliste Landry  \n",
            "2                               Pierre Caliste Landry  \n",
            "3                                              Landry  \n",
            "4                                      Pierre Caliste  \n",
            "5   Born into slavery, became an attorney, Methodi...  \n",
            "6   Prevost sugar cane plantation in Ascension Parish  \n",
            "7                                    African-American  \n",
            "8                                      April 19, 1841  \n",
            "9                                   December 22, 1921  \n",
            "10                                               Male  \n",
            "11  End of Civil War, Conversion to Methodist Epis...  \n",
            "12  Former Slave, Mayor, Minister, Attorney, Edito...  \n",
            "13      Attorney, Minister, Mayor, Editor, Legislator  \n",
            "14                  Amanda Grigsby, Florence Simpkins  \n",
            "15                              Pierre Caliste Landry  \n",
            "16                        Mayor, Minister, Legislator  \n",
            "17  Election as Mayor, Ministerial Duties, Legisla...  \n",
            "Generated DataFrame after transformation:\n",
            "                  Subject                Predicate  \\\n",
            "1   Pierre Caliste Landry  haspreferrednamevariant   \n",
            "2   Pierre Caliste Landry         fullnameasstring   \n",
            "3   Pierre Caliste Landry       hassurnameasstring   \n",
            "4   Pierre Caliste Landry     hasfirstnameasstring   \n",
            "5   Pierre Caliste Landry           hasdescription   \n",
            "6   Pierre Caliste Landry    referstoplaceoforigin   \n",
            "7   Pierre Caliste Landry                 has_race   \n",
            "8   Pierre Caliste Landry             hasbirthdate   \n",
            "9   Pierre Caliste Landry             hasdeathdate   \n",
            "10  Pierre Caliste Landry                   hassex   \n",
            "11  Pierre Caliste Landry  hasstatusgeneratedevent   \n",
            "12  Pierre Caliste Landry                 hasvalue   \n",
            "13  Pierre Caliste Landry                 hasvalue   \n",
            "14  Pierre Caliste Landry         isrelationshipto   \n",
            "15  Pierre Caliste Landry       isrelationshipfrom   \n",
            "16  Pierre Caliste Landry   hasparticipantroletype   \n",
            "17  Pierre Caliste Landry           roleprovidedby   \n",
            "\n",
            "                                               Object  \n",
            "1                               pierre caliste landry  \n",
            "2                               pierre caliste landry  \n",
            "3                                              landry  \n",
            "4                                      pierre caliste  \n",
            "5   born into slavery, became an attorney, methodi...  \n",
            "6   prevost sugar cane plantation in ascension parish  \n",
            "7                                    african-american  \n",
            "8                                      april 19, 1841  \n",
            "9                                   december 22, 1921  \n",
            "10                                               male  \n",
            "11  end of civil war, conversion to methodist epis...  \n",
            "12  former slave, mayor, minister, attorney, edito...  \n",
            "13      attorney, minister, mayor, editor, legislator  \n",
            "14                  amanda grigsby, florence simpkins  \n",
            "15                              pierre caliste landry  \n",
            "16                        mayor, minister, legislator  \n",
            "17  election as mayor, ministerial duties, legisla...  \n",
            "Comparing: instance of -> person with haspreferrednamevariant -> pierre caliste landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: instance of -> person with fullnameasstring -> pierre caliste landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: instance of -> person with hassurnameasstring -> landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: instance of -> person with hasfirstnameasstring -> pierre caliste (Subject: Pierre Caliste Landry)\n",
            "Comparing: instance of -> person with hasdescription -> born into slavery, became an attorney, methodist episcopal minister, mayor, newspaper editor, and state legislator in louisiana. (Subject: Pierre Caliste Landry)\n",
            "Comparing: instance of -> person with referstoplaceoforigin -> prevost sugar cane plantation in ascension parish (Subject: Pierre Caliste Landry)\n",
            "Comparing: instance of -> person with has_race -> african-american (Subject: Pierre Caliste Landry)\n",
            "Comparing: instance of -> person with hasbirthdate -> april 19, 1841 (Subject: Pierre Caliste Landry)\n",
            "Comparing: instance of -> person with hasdeathdate -> december 22, 1921 (Subject: Pierre Caliste Landry)\n",
            "Comparing: instance of -> person with hassex -> male (Subject: Pierre Caliste Landry)\n",
            "Comparing: instance of -> person with hasstatusgeneratedevent -> end of civil war, conversion to methodist episcopal faith (Subject: Pierre Caliste Landry)\n",
            "Comparing: instance of -> person with hasvalue -> former slave, mayor, minister, attorney, editor, legislator (Subject: Pierre Caliste Landry)\n",
            "Comparing: instance of -> person with hasvalue -> attorney, minister, mayor, editor, legislator (Subject: Pierre Caliste Landry)\n",
            "Comparing: instance of -> person with isrelationshipto -> amanda grigsby, florence simpkins (Subject: Pierre Caliste Landry)\n",
            "Comparing: instance of -> person with isrelationshipfrom -> pierre caliste landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: instance of -> person with hasparticipantroletype -> mayor, minister, legislator (Subject: Pierre Caliste Landry)\n",
            "Comparing: instance of -> person with roleprovidedby -> election as mayor, ministerial duties, legislative duties (Subject: Pierre Caliste Landry)\n",
            "No match found for truth row: Pierre Caliste Landry, Predicate: instance of, Object: person\n",
            "Comparing: hassex -> male with haspreferrednamevariant -> pierre caliste landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: hassex -> male with fullnameasstring -> pierre caliste landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: hassex -> male with hassurnameasstring -> landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: hassex -> male with hasfirstnameasstring -> pierre caliste (Subject: Pierre Caliste Landry)\n",
            "Comparing: hassex -> male with hasdescription -> born into slavery, became an attorney, methodist episcopal minister, mayor, newspaper editor, and state legislator in louisiana. (Subject: Pierre Caliste Landry)\n",
            "Comparing: hassex -> male with referstoplaceoforigin -> prevost sugar cane plantation in ascension parish (Subject: Pierre Caliste Landry)\n",
            "Comparing: hassex -> male with has_race -> african-american (Subject: Pierre Caliste Landry)\n",
            "Comparing: hassex -> male with hasbirthdate -> april 19, 1841 (Subject: Pierre Caliste Landry)\n",
            "Comparing: hassex -> male with hasdeathdate -> december 22, 1921 (Subject: Pierre Caliste Landry)\n",
            "Comparing: hassex -> male with hassex -> male (Subject: Pierre Caliste Landry)\n",
            "Match found for truth row Pierre Caliste Landry with generated row Pierre Caliste Landry\n",
            "Comparing: hassex -> male with hasstatusgeneratedevent -> end of civil war, conversion to methodist episcopal faith (Subject: Pierre Caliste Landry)\n",
            "Comparing: hassex -> male with hasvalue -> former slave, mayor, minister, attorney, editor, legislator (Subject: Pierre Caliste Landry)\n",
            "Comparing: hassex -> male with hasvalue -> attorney, minister, mayor, editor, legislator (Subject: Pierre Caliste Landry)\n",
            "Comparing: hassex -> male with isrelationshipto -> amanda grigsby, florence simpkins (Subject: Pierre Caliste Landry)\n",
            "Comparing: hassex -> male with isrelationshipfrom -> pierre caliste landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: hassex -> male with hasparticipantroletype -> mayor, minister, legislator (Subject: Pierre Caliste Landry)\n",
            "Comparing: hassex -> male with roleprovidedby -> election as mayor, ministerial duties, legislative duties (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasparticipantrole -> child with haspreferrednamevariant -> pierre caliste landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasparticipantrole -> child with fullnameasstring -> pierre caliste landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasparticipantrole -> child with hassurnameasstring -> landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasparticipantrole -> child with hasfirstnameasstring -> pierre caliste (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasparticipantrole -> child with hasdescription -> born into slavery, became an attorney, methodist episcopal minister, mayor, newspaper editor, and state legislator in louisiana. (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasparticipantrole -> child with referstoplaceoforigin -> prevost sugar cane plantation in ascension parish (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasparticipantrole -> child with has_race -> african-american (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasparticipantrole -> child with hasbirthdate -> april 19, 1841 (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasparticipantrole -> child with hasdeathdate -> december 22, 1921 (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasparticipantrole -> child with hassex -> male (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasparticipantrole -> child with hasstatusgeneratedevent -> end of civil war, conversion to methodist episcopal faith (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasparticipantrole -> child with hasvalue -> former slave, mayor, minister, attorney, editor, legislator (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasparticipantrole -> child with hasvalue -> attorney, minister, mayor, editor, legislator (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasparticipantrole -> child with isrelationshipto -> amanda grigsby, florence simpkins (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasparticipantrole -> child with isrelationshipfrom -> pierre caliste landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasparticipantrole -> child with hasparticipantroletype -> mayor, minister, legislator (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasparticipantrole -> child with roleprovidedby -> election as mayor, ministerial duties, legislative duties (Subject: Pierre Caliste Landry)\n",
            "No match found for truth row: Pierre Caliste Landry, Predicate: hasparticipantrole, Object: child\n",
            "Comparing: hasparticipantrole -> deceased person with haspreferrednamevariant -> pierre caliste landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasparticipantrole -> deceased person with fullnameasstring -> pierre caliste landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasparticipantrole -> deceased person with hassurnameasstring -> landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasparticipantrole -> deceased person with hasfirstnameasstring -> pierre caliste (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasparticipantrole -> deceased person with hasdescription -> born into slavery, became an attorney, methodist episcopal minister, mayor, newspaper editor, and state legislator in louisiana. (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasparticipantrole -> deceased person with referstoplaceoforigin -> prevost sugar cane plantation in ascension parish (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasparticipantrole -> deceased person with has_race -> african-american (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasparticipantrole -> deceased person with hasbirthdate -> april 19, 1841 (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasparticipantrole -> deceased person with hasdeathdate -> december 22, 1921 (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasparticipantrole -> deceased person with hassex -> male (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasparticipantrole -> deceased person with hasstatusgeneratedevent -> end of civil war, conversion to methodist episcopal faith (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasparticipantrole -> deceased person with hasvalue -> former slave, mayor, minister, attorney, editor, legislator (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasparticipantrole -> deceased person with hasvalue -> attorney, minister, mayor, editor, legislator (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasparticipantrole -> deceased person with isrelationshipto -> amanda grigsby, florence simpkins (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasparticipantrole -> deceased person with isrelationshipfrom -> pierre caliste landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasparticipantrole -> deceased person with hasparticipantroletype -> mayor, minister, legislator (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasparticipantrole -> deceased person with roleprovidedby -> election as mayor, ministerial duties, legislative duties (Subject: Pierre Caliste Landry)\n",
            "No match found for truth row: Pierre Caliste Landry, Predicate: hasparticipantrole, Object: deceased person\n",
            "Comparing: hasdescriptiveoccupation -> slave with haspreferrednamevariant -> pierre caliste landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> slave with fullnameasstring -> pierre caliste landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hassurnameasstring -> landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasfirstnameasstring -> pierre caliste (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasdescription -> born into slavery, became an attorney, methodist episcopal minister, mayor, newspaper editor, and state legislator in louisiana. (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> slave with referstoplaceoforigin -> prevost sugar cane plantation in ascension parish (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> slave with has_race -> african-american (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasbirthdate -> april 19, 1841 (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasdeathdate -> december 22, 1921 (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hassex -> male (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasstatusgeneratedevent -> end of civil war, conversion to methodist episcopal faith (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasvalue -> former slave, mayor, minister, attorney, editor, legislator (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasvalue -> attorney, minister, mayor, editor, legislator (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> slave with isrelationshipto -> amanda grigsby, florence simpkins (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> slave with isrelationshipfrom -> pierre caliste landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasparticipantroletype -> mayor, minister, legislator (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> slave with roleprovidedby -> election as mayor, ministerial duties, legislative duties (Subject: Pierre Caliste Landry)\n",
            "No match found for truth row: Pierre Caliste Landry, Predicate: hasdescriptiveoccupation, Object: slave\n",
            "Comparing: hasdescriptiveoccupation -> state legislator with haspreferrednamevariant -> pierre caliste landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> state legislator with fullnameasstring -> pierre caliste landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> state legislator with hassurnameasstring -> landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> state legislator with hasfirstnameasstring -> pierre caliste (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> state legislator with hasdescription -> born into slavery, became an attorney, methodist episcopal minister, mayor, newspaper editor, and state legislator in louisiana. (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> state legislator with referstoplaceoforigin -> prevost sugar cane plantation in ascension parish (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> state legislator with has_race -> african-american (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> state legislator with hasbirthdate -> april 19, 1841 (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> state legislator with hasdeathdate -> december 22, 1921 (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> state legislator with hassex -> male (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> state legislator with hasstatusgeneratedevent -> end of civil war, conversion to methodist episcopal faith (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> state legislator with hasvalue -> former slave, mayor, minister, attorney, editor, legislator (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> state legislator with hasvalue -> attorney, minister, mayor, editor, legislator (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> state legislator with isrelationshipto -> amanda grigsby, florence simpkins (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> state legislator with isrelationshipfrom -> pierre caliste landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> state legislator with hasparticipantroletype -> mayor, minister, legislator (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> state legislator with roleprovidedby -> election as mayor, ministerial duties, legislative duties (Subject: Pierre Caliste Landry)\n",
            "No match found for truth row: Pierre Caliste Landry, Predicate: hasdescriptiveoccupation, Object: state legislator\n",
            "Comparing: hasdescriptiveoccupation -> lawyer with haspreferrednamevariant -> pierre caliste landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> lawyer with fullnameasstring -> pierre caliste landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> lawyer with hassurnameasstring -> landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> lawyer with hasfirstnameasstring -> pierre caliste (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> lawyer with hasdescription -> born into slavery, became an attorney, methodist episcopal minister, mayor, newspaper editor, and state legislator in louisiana. (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> lawyer with referstoplaceoforigin -> prevost sugar cane plantation in ascension parish (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> lawyer with has_race -> african-american (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> lawyer with hasbirthdate -> april 19, 1841 (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> lawyer with hasdeathdate -> december 22, 1921 (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> lawyer with hassex -> male (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> lawyer with hasstatusgeneratedevent -> end of civil war, conversion to methodist episcopal faith (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> lawyer with hasvalue -> former slave, mayor, minister, attorney, editor, legislator (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> lawyer with hasvalue -> attorney, minister, mayor, editor, legislator (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> lawyer with isrelationshipto -> amanda grigsby, florence simpkins (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> lawyer with isrelationshipfrom -> pierre caliste landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> lawyer with hasparticipantroletype -> mayor, minister, legislator (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> lawyer with roleprovidedby -> election as mayor, ministerial duties, legislative duties (Subject: Pierre Caliste Landry)\n",
            "No match found for truth row: Pierre Caliste Landry, Predicate: hasdescriptiveoccupation, Object: lawyer\n",
            "Comparing: hasdescriptiveoccupation -> institutional founder / benefactor with haspreferrednamevariant -> pierre caliste landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> institutional founder / benefactor with fullnameasstring -> pierre caliste landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> institutional founder / benefactor with hassurnameasstring -> landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> institutional founder / benefactor with hasfirstnameasstring -> pierre caliste (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> institutional founder / benefactor with hasdescription -> born into slavery, became an attorney, methodist episcopal minister, mayor, newspaper editor, and state legislator in louisiana. (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> institutional founder / benefactor with referstoplaceoforigin -> prevost sugar cane plantation in ascension parish (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> institutional founder / benefactor with has_race -> african-american (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> institutional founder / benefactor with hasbirthdate -> april 19, 1841 (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> institutional founder / benefactor with hasdeathdate -> december 22, 1921 (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> institutional founder / benefactor with hassex -> male (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> institutional founder / benefactor with hasstatusgeneratedevent -> end of civil war, conversion to methodist episcopal faith (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> institutional founder / benefactor with hasvalue -> former slave, mayor, minister, attorney, editor, legislator (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> institutional founder / benefactor with hasvalue -> attorney, minister, mayor, editor, legislator (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> institutional founder / benefactor with isrelationshipto -> amanda grigsby, florence simpkins (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> institutional founder / benefactor with isrelationshipfrom -> pierre caliste landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> institutional founder / benefactor with hasparticipantroletype -> mayor, minister, legislator (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> institutional founder / benefactor with roleprovidedby -> election as mayor, ministerial duties, legislative duties (Subject: Pierre Caliste Landry)\n",
            "No match found for truth row: Pierre Caliste Landry, Predicate: hasdescriptiveoccupation, Object: institutional founder / benefactor\n",
            "Comparing: hasdescriptiveoccupation -> methodist episcopal clergy with haspreferrednamevariant -> pierre caliste landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> methodist episcopal clergy with fullnameasstring -> pierre caliste landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> methodist episcopal clergy with hassurnameasstring -> landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> methodist episcopal clergy with hasfirstnameasstring -> pierre caliste (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> methodist episcopal clergy with hasdescription -> born into slavery, became an attorney, methodist episcopal minister, mayor, newspaper editor, and state legislator in louisiana. (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> methodist episcopal clergy with referstoplaceoforigin -> prevost sugar cane plantation in ascension parish (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> methodist episcopal clergy with has_race -> african-american (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> methodist episcopal clergy with hasbirthdate -> april 19, 1841 (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> methodist episcopal clergy with hasdeathdate -> december 22, 1921 (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> methodist episcopal clergy with hassex -> male (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> methodist episcopal clergy with hasstatusgeneratedevent -> end of civil war, conversion to methodist episcopal faith (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> methodist episcopal clergy with hasvalue -> former slave, mayor, minister, attorney, editor, legislator (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> methodist episcopal clergy with hasvalue -> attorney, minister, mayor, editor, legislator (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> methodist episcopal clergy with isrelationshipto -> amanda grigsby, florence simpkins (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> methodist episcopal clergy with isrelationshipfrom -> pierre caliste landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> methodist episcopal clergy with hasparticipantroletype -> mayor, minister, legislator (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescriptiveoccupation -> methodist episcopal clergy with roleprovidedby -> election as mayor, ministerial duties, legislative duties (Subject: Pierre Caliste Landry)\n",
            "No match found for truth row: Pierre Caliste Landry, Predicate: hasdescriptiveoccupation, Object: methodist episcopal clergy\n",
            "Comparing: hasfirstname -> pierre caliste with haspreferrednamevariant -> pierre caliste landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasfirstname -> pierre caliste with fullnameasstring -> pierre caliste landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasfirstname -> pierre caliste with hassurnameasstring -> landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasfirstname -> pierre caliste with hasfirstnameasstring -> pierre caliste (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasfirstname -> pierre caliste with hasdescription -> born into slavery, became an attorney, methodist episcopal minister, mayor, newspaper editor, and state legislator in louisiana. (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasfirstname -> pierre caliste with referstoplaceoforigin -> prevost sugar cane plantation in ascension parish (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasfirstname -> pierre caliste with has_race -> african-american (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasfirstname -> pierre caliste with hasbirthdate -> april 19, 1841 (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasfirstname -> pierre caliste with hasdeathdate -> december 22, 1921 (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasfirstname -> pierre caliste with hassex -> male (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasfirstname -> pierre caliste with hasstatusgeneratedevent -> end of civil war, conversion to methodist episcopal faith (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasfirstname -> pierre caliste with hasvalue -> former slave, mayor, minister, attorney, editor, legislator (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasfirstname -> pierre caliste with hasvalue -> attorney, minister, mayor, editor, legislator (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasfirstname -> pierre caliste with isrelationshipto -> amanda grigsby, florence simpkins (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasfirstname -> pierre caliste with isrelationshipfrom -> pierre caliste landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasfirstname -> pierre caliste with hasparticipantroletype -> mayor, minister, legislator (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasfirstname -> pierre caliste with roleprovidedby -> election as mayor, ministerial duties, legislative duties (Subject: Pierre Caliste Landry)\n",
            "No match found for truth row: Pierre Caliste Landry, Predicate: hasfirstname, Object: pierre caliste\n",
            "Comparing: hassurname -> landry with haspreferrednamevariant -> pierre caliste landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: hassurname -> landry with fullnameasstring -> pierre caliste landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: hassurname -> landry with hassurnameasstring -> landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: hassurname -> landry with hasfirstnameasstring -> pierre caliste (Subject: Pierre Caliste Landry)\n",
            "Comparing: hassurname -> landry with hasdescription -> born into slavery, became an attorney, methodist episcopal minister, mayor, newspaper editor, and state legislator in louisiana. (Subject: Pierre Caliste Landry)\n",
            "Comparing: hassurname -> landry with referstoplaceoforigin -> prevost sugar cane plantation in ascension parish (Subject: Pierre Caliste Landry)\n",
            "Comparing: hassurname -> landry with has_race -> african-american (Subject: Pierre Caliste Landry)\n",
            "Comparing: hassurname -> landry with hasbirthdate -> april 19, 1841 (Subject: Pierre Caliste Landry)\n",
            "Comparing: hassurname -> landry with hasdeathdate -> december 22, 1921 (Subject: Pierre Caliste Landry)\n",
            "Comparing: hassurname -> landry with hassex -> male (Subject: Pierre Caliste Landry)\n",
            "Comparing: hassurname -> landry with hasstatusgeneratedevent -> end of civil war, conversion to methodist episcopal faith (Subject: Pierre Caliste Landry)\n",
            "Comparing: hassurname -> landry with hasvalue -> former slave, mayor, minister, attorney, editor, legislator (Subject: Pierre Caliste Landry)\n",
            "Comparing: hassurname -> landry with hasvalue -> attorney, minister, mayor, editor, legislator (Subject: Pierre Caliste Landry)\n",
            "Comparing: hassurname -> landry with isrelationshipto -> amanda grigsby, florence simpkins (Subject: Pierre Caliste Landry)\n",
            "Comparing: hassurname -> landry with isrelationshipfrom -> pierre caliste landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: hassurname -> landry with hasparticipantroletype -> mayor, minister, legislator (Subject: Pierre Caliste Landry)\n",
            "Comparing: hassurname -> landry with roleprovidedby -> election as mayor, ministerial duties, legislative duties (Subject: Pierre Caliste Landry)\n",
            "No match found for truth row: Pierre Caliste Landry, Predicate: hassurname, Object: landry\n",
            "Comparing: hasname -> pierre caliste landry with haspreferrednamevariant -> pierre caliste landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasname -> pierre caliste landry with fullnameasstring -> pierre caliste landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasname -> pierre caliste landry with hassurnameasstring -> landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasname -> pierre caliste landry with hasfirstnameasstring -> pierre caliste (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasname -> pierre caliste landry with hasdescription -> born into slavery, became an attorney, methodist episcopal minister, mayor, newspaper editor, and state legislator in louisiana. (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasname -> pierre caliste landry with referstoplaceoforigin -> prevost sugar cane plantation in ascension parish (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasname -> pierre caliste landry with has_race -> african-american (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasname -> pierre caliste landry with hasbirthdate -> april 19, 1841 (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasname -> pierre caliste landry with hasdeathdate -> december 22, 1921 (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasname -> pierre caliste landry with hassex -> male (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasname -> pierre caliste landry with hasstatusgeneratedevent -> end of civil war, conversion to methodist episcopal faith (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasname -> pierre caliste landry with hasvalue -> former slave, mayor, minister, attorney, editor, legislator (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasname -> pierre caliste landry with hasvalue -> attorney, minister, mayor, editor, legislator (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasname -> pierre caliste landry with isrelationshipto -> amanda grigsby, florence simpkins (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasname -> pierre caliste landry with isrelationshipfrom -> pierre caliste landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasname -> pierre caliste landry with hasparticipantroletype -> mayor, minister, legislator (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasname -> pierre caliste landry with roleprovidedby -> election as mayor, ministerial duties, legislative duties (Subject: Pierre Caliste Landry)\n",
            "No match found for truth row: Pierre Caliste Landry, Predicate: hasname, Object: pierre caliste landry\n",
            "Comparing: hasdescription -> enslaved. when elected mayor of donaldsonville in 1868, he became the first african american mayor in the united states. also served as a state representative, senator, university founder, and methodist elder. with haspreferrednamevariant -> pierre caliste landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescription -> enslaved. when elected mayor of donaldsonville in 1868, he became the first african american mayor in the united states. also served as a state representative, senator, university founder, and methodist elder. with fullnameasstring -> pierre caliste landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescription -> enslaved. when elected mayor of donaldsonville in 1868, he became the first african american mayor in the united states. also served as a state representative, senator, university founder, and methodist elder. with hassurnameasstring -> landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescription -> enslaved. when elected mayor of donaldsonville in 1868, he became the first african american mayor in the united states. also served as a state representative, senator, university founder, and methodist elder. with hasfirstnameasstring -> pierre caliste (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescription -> enslaved. when elected mayor of donaldsonville in 1868, he became the first african american mayor in the united states. also served as a state representative, senator, university founder, and methodist elder. with hasdescription -> born into slavery, became an attorney, methodist episcopal minister, mayor, newspaper editor, and state legislator in louisiana. (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescription -> enslaved. when elected mayor of donaldsonville in 1868, he became the first african american mayor in the united states. also served as a state representative, senator, university founder, and methodist elder. with referstoplaceoforigin -> prevost sugar cane plantation in ascension parish (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescription -> enslaved. when elected mayor of donaldsonville in 1868, he became the first african american mayor in the united states. also served as a state representative, senator, university founder, and methodist elder. with has_race -> african-american (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescription -> enslaved. when elected mayor of donaldsonville in 1868, he became the first african american mayor in the united states. also served as a state representative, senator, university founder, and methodist elder. with hasbirthdate -> april 19, 1841 (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescription -> enslaved. when elected mayor of donaldsonville in 1868, he became the first african american mayor in the united states. also served as a state representative, senator, university founder, and methodist elder. with hasdeathdate -> december 22, 1921 (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescription -> enslaved. when elected mayor of donaldsonville in 1868, he became the first african american mayor in the united states. also served as a state representative, senator, university founder, and methodist elder. with hassex -> male (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescription -> enslaved. when elected mayor of donaldsonville in 1868, he became the first african american mayor in the united states. also served as a state representative, senator, university founder, and methodist elder. with hasstatusgeneratedevent -> end of civil war, conversion to methodist episcopal faith (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescription -> enslaved. when elected mayor of donaldsonville in 1868, he became the first african american mayor in the united states. also served as a state representative, senator, university founder, and methodist elder. with hasvalue -> former slave, mayor, minister, attorney, editor, legislator (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescription -> enslaved. when elected mayor of donaldsonville in 1868, he became the first african american mayor in the united states. also served as a state representative, senator, university founder, and methodist elder. with hasvalue -> attorney, minister, mayor, editor, legislator (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescription -> enslaved. when elected mayor of donaldsonville in 1868, he became the first african american mayor in the united states. also served as a state representative, senator, university founder, and methodist elder. with isrelationshipto -> amanda grigsby, florence simpkins (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescription -> enslaved. when elected mayor of donaldsonville in 1868, he became the first african american mayor in the united states. also served as a state representative, senator, university founder, and methodist elder. with isrelationshipfrom -> pierre caliste landry (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescription -> enslaved. when elected mayor of donaldsonville in 1868, he became the first african american mayor in the united states. also served as a state representative, senator, university founder, and methodist elder. with hasparticipantroletype -> mayor, minister, legislator (Subject: Pierre Caliste Landry)\n",
            "Comparing: hasdescription -> enslaved. when elected mayor of donaldsonville in 1868, he became the first african american mayor in the united states. also served as a state representative, senator, university founder, and methodist elder. with roleprovidedby -> election as mayor, ministerial duties, legislative duties (Subject: Pierre Caliste Landry)\n",
            "No match found for truth row: Pierre Caliste Landry, Predicate: hasdescription, Object: enslaved. when elected mayor of donaldsonville in 1868, he became the first african american mayor in the united states. also served as a state representative, senator, university founder, and methodist elder.\n",
            "\n",
            "Processing truth file: Peter_Salem.tsv\n",
            "Truth DataFrame:\n",
            "             0                         1  \\\n",
            "0  Peter Salem               instance of   \n",
            "1  Peter Salem                    hasSex   \n",
            "2  Peter Salem        hasParticipantRole   \n",
            "3  Peter Salem        hasParticipantRole   \n",
            "4  Peter Salem  hasDescriptiveOccupation   \n",
            "5  Peter Salem  hasDescriptiveOccupation   \n",
            "6  Peter Salem              hasFirstName   \n",
            "7  Peter Salem                hasSurname   \n",
            "8  Peter Salem                   hasName   \n",
            "9  Peter Salem            hasDescription   \n",
            "\n",
            "                                                   2  \n",
            "0                                             Person  \n",
            "1                                               Male  \n",
            "2                                              Child  \n",
            "3                                    Deceased Person  \n",
            "4                                              Slave  \n",
            "5                                Colonial Militiaman  \n",
            "6                                              Peter  \n",
            "7                                              Salem  \n",
            "8                                        Peter Salem  \n",
            "9  Born Enslaved. Free before 13th Amendment. A s...  \n",
            "\n",
            "Found matching generated file: Peter_Salem_triples.tsv\n",
            "Generated DataFrame before transformation:\n",
            "                Subject                      Predicate  \\\n",
            "1   Peter_Salem_triples        hasPreferredNameVariant   \n",
            "2   Peter_Salem_triples               fullNameAsString   \n",
            "3   Peter_Salem_triples             hasSurnameAsString   \n",
            "4   Peter_Salem_triples           hasFirstnameAsString   \n",
            "5   Peter_Salem_triples                 hasDescription   \n",
            "6   Peter_Salem_triples          refersToPlaceOfOrigin   \n",
            "7   Peter_Salem_triples                       has_Race   \n",
            "8   Peter_Salem_triples                   hasBirthDate   \n",
            "9   Peter_Salem_triples                   hasDeathDate   \n",
            "10  Peter_Salem_triples                         hasSex   \n",
            "11  Peter_Salem_triples        hasStatusGeneratedEvent   \n",
            "12  Peter_Salem_triples                       hasValue   \n",
            "13  Peter_Salem_triples                       hasValue   \n",
            "14  Peter_Salem_triples  hasInterAgentRelationshipType   \n",
            "15  Peter_Salem_triples               isRelationshipTo   \n",
            "16  Peter_Salem_triples             isRelationshipFrom   \n",
            "17  Peter_Salem_triples         hasParticipantRoleType   \n",
            "18  Peter_Salem_triples                 roleProvidedBy   \n",
            "\n",
            "                                               Object  \n",
            "1                                         Peter Salem  \n",
            "2                                         Peter Salem  \n",
            "3                                               Salem  \n",
            "4                                               Peter  \n",
            "5   Peter Salem was an African-American from Massa...  \n",
            "6           Framingham, Province of Massachusetts Bay  \n",
            "7                                    African-American  \n",
            "8                                     October 1, 1750  \n",
            "9                                     August 16, 1816  \n",
            "10                                               Male  \n",
            "11  Emancipation, Enlistment in the Continental Ar...  \n",
            "12                  Freed Slave, Soldier, Cane Weaver  \n",
            "13                               Soldier, Cane Weaver  \n",
            "14                                           Enslaver  \n",
            "15                           Major Lawson Buckminster  \n",
            "16                                        Peter Salem  \n",
            "17                                            Soldier  \n",
            "18                         American Revolutionary War  \n",
            "Generated DataFrame after transformation:\n",
            "        Subject                      Predicate  \\\n",
            "1   Peter Salem        haspreferrednamevariant   \n",
            "2   Peter Salem               fullnameasstring   \n",
            "3   Peter Salem             hassurnameasstring   \n",
            "4   Peter Salem           hasfirstnameasstring   \n",
            "5   Peter Salem                 hasdescription   \n",
            "6   Peter Salem          referstoplaceoforigin   \n",
            "7   Peter Salem                       has_race   \n",
            "8   Peter Salem                   hasbirthdate   \n",
            "9   Peter Salem                   hasdeathdate   \n",
            "10  Peter Salem                         hassex   \n",
            "11  Peter Salem        hasstatusgeneratedevent   \n",
            "12  Peter Salem                       hasvalue   \n",
            "13  Peter Salem                       hasvalue   \n",
            "14  Peter Salem  hasinteragentrelationshiptype   \n",
            "15  Peter Salem               isrelationshipto   \n",
            "16  Peter Salem             isrelationshipfrom   \n",
            "17  Peter Salem         hasparticipantroletype   \n",
            "18  Peter Salem                 roleprovidedby   \n",
            "\n",
            "                                               Object  \n",
            "1                                         peter salem  \n",
            "2                                         peter salem  \n",
            "3                                               salem  \n",
            "4                                               peter  \n",
            "5   peter salem was an african-american from massa...  \n",
            "6           framingham, province of massachusetts bay  \n",
            "7                                    african-american  \n",
            "8                                     october 1, 1750  \n",
            "9                                     august 16, 1816  \n",
            "10                                               male  \n",
            "11  emancipation, enlistment in the continental ar...  \n",
            "12                  freed slave, soldier, cane weaver  \n",
            "13                               soldier, cane weaver  \n",
            "14                                           enslaver  \n",
            "15                           major lawson buckminster  \n",
            "16                                        peter salem  \n",
            "17                                            soldier  \n",
            "18                         american revolutionary war  \n",
            "Comparing: instance of -> person with haspreferrednamevariant -> peter salem (Subject: Peter Salem)\n",
            "Comparing: instance of -> person with fullnameasstring -> peter salem (Subject: Peter Salem)\n",
            "Comparing: instance of -> person with hassurnameasstring -> salem (Subject: Peter Salem)\n",
            "Comparing: instance of -> person with hasfirstnameasstring -> peter (Subject: Peter Salem)\n",
            "Comparing: instance of -> person with hasdescription -> peter salem was an african-american from massachusetts who served as a u.s. soldier in the american revolutionary war. born into slavery in framingham, he was freed by a later master, major lawson buckminster, to serve in the local militia. he then enlisted in the continental army, serving for nearly five years during the war. afterwards, he married and worked as a cane weaver. (Subject: Peter Salem)\n",
            "Comparing: instance of -> person with referstoplaceoforigin -> framingham, province of massachusetts bay (Subject: Peter Salem)\n",
            "Comparing: instance of -> person with has_race -> african-american (Subject: Peter Salem)\n",
            "Comparing: instance of -> person with hasbirthdate -> october 1, 1750 (Subject: Peter Salem)\n",
            "Comparing: instance of -> person with hasdeathdate -> august 16, 1816 (Subject: Peter Salem)\n",
            "Comparing: instance of -> person with hassex -> male (Subject: Peter Salem)\n",
            "Comparing: instance of -> person with hasstatusgeneratedevent -> emancipation, enlistment in the continental army, marriage (Subject: Peter Salem)\n",
            "Comparing: instance of -> person with hasvalue -> freed slave, soldier, cane weaver (Subject: Peter Salem)\n",
            "Comparing: instance of -> person with hasvalue -> soldier, cane weaver (Subject: Peter Salem)\n",
            "Comparing: instance of -> person with hasinteragentrelationshiptype -> enslaver (Subject: Peter Salem)\n",
            "Comparing: instance of -> person with isrelationshipto -> major lawson buckminster (Subject: Peter Salem)\n",
            "Comparing: instance of -> person with isrelationshipfrom -> peter salem (Subject: Peter Salem)\n",
            "Comparing: instance of -> person with hasparticipantroletype -> soldier (Subject: Peter Salem)\n",
            "Comparing: instance of -> person with roleprovidedby -> american revolutionary war (Subject: Peter Salem)\n",
            "No match found for truth row: Peter Salem, Predicate: instance of, Object: person\n",
            "Comparing: hassex -> male with haspreferrednamevariant -> peter salem (Subject: Peter Salem)\n",
            "Comparing: hassex -> male with fullnameasstring -> peter salem (Subject: Peter Salem)\n",
            "Comparing: hassex -> male with hassurnameasstring -> salem (Subject: Peter Salem)\n",
            "Comparing: hassex -> male with hasfirstnameasstring -> peter (Subject: Peter Salem)\n",
            "Comparing: hassex -> male with hasdescription -> peter salem was an african-american from massachusetts who served as a u.s. soldier in the american revolutionary war. born into slavery in framingham, he was freed by a later master, major lawson buckminster, to serve in the local militia. he then enlisted in the continental army, serving for nearly five years during the war. afterwards, he married and worked as a cane weaver. (Subject: Peter Salem)\n",
            "Comparing: hassex -> male with referstoplaceoforigin -> framingham, province of massachusetts bay (Subject: Peter Salem)\n",
            "Comparing: hassex -> male with has_race -> african-american (Subject: Peter Salem)\n",
            "Comparing: hassex -> male with hasbirthdate -> october 1, 1750 (Subject: Peter Salem)\n",
            "Comparing: hassex -> male with hasdeathdate -> august 16, 1816 (Subject: Peter Salem)\n",
            "Comparing: hassex -> male with hassex -> male (Subject: Peter Salem)\n",
            "Match found for truth row Peter Salem with generated row Peter Salem\n",
            "Comparing: hassex -> male with hasstatusgeneratedevent -> emancipation, enlistment in the continental army, marriage (Subject: Peter Salem)\n",
            "Comparing: hassex -> male with hasvalue -> freed slave, soldier, cane weaver (Subject: Peter Salem)\n",
            "Comparing: hassex -> male with hasvalue -> soldier, cane weaver (Subject: Peter Salem)\n",
            "Comparing: hassex -> male with hasinteragentrelationshiptype -> enslaver (Subject: Peter Salem)\n",
            "Comparing: hassex -> male with isrelationshipto -> major lawson buckminster (Subject: Peter Salem)\n",
            "Comparing: hassex -> male with isrelationshipfrom -> peter salem (Subject: Peter Salem)\n",
            "Comparing: hassex -> male with hasparticipantroletype -> soldier (Subject: Peter Salem)\n",
            "Comparing: hassex -> male with roleprovidedby -> american revolutionary war (Subject: Peter Salem)\n",
            "Comparing: hasparticipantrole -> child with haspreferrednamevariant -> peter salem (Subject: Peter Salem)\n",
            "Comparing: hasparticipantrole -> child with fullnameasstring -> peter salem (Subject: Peter Salem)\n",
            "Comparing: hasparticipantrole -> child with hassurnameasstring -> salem (Subject: Peter Salem)\n",
            "Comparing: hasparticipantrole -> child with hasfirstnameasstring -> peter (Subject: Peter Salem)\n",
            "Comparing: hasparticipantrole -> child with hasdescription -> peter salem was an african-american from massachusetts who served as a u.s. soldier in the american revolutionary war. born into slavery in framingham, he was freed by a later master, major lawson buckminster, to serve in the local militia. he then enlisted in the continental army, serving for nearly five years during the war. afterwards, he married and worked as a cane weaver. (Subject: Peter Salem)\n",
            "Comparing: hasparticipantrole -> child with referstoplaceoforigin -> framingham, province of massachusetts bay (Subject: Peter Salem)\n",
            "Comparing: hasparticipantrole -> child with has_race -> african-american (Subject: Peter Salem)\n",
            "Comparing: hasparticipantrole -> child with hasbirthdate -> october 1, 1750 (Subject: Peter Salem)\n",
            "Comparing: hasparticipantrole -> child with hasdeathdate -> august 16, 1816 (Subject: Peter Salem)\n",
            "Comparing: hasparticipantrole -> child with hassex -> male (Subject: Peter Salem)\n",
            "Comparing: hasparticipantrole -> child with hasstatusgeneratedevent -> emancipation, enlistment in the continental army, marriage (Subject: Peter Salem)\n",
            "Comparing: hasparticipantrole -> child with hasvalue -> freed slave, soldier, cane weaver (Subject: Peter Salem)\n",
            "Comparing: hasparticipantrole -> child with hasvalue -> soldier, cane weaver (Subject: Peter Salem)\n",
            "Comparing: hasparticipantrole -> child with hasinteragentrelationshiptype -> enslaver (Subject: Peter Salem)\n",
            "Comparing: hasparticipantrole -> child with isrelationshipto -> major lawson buckminster (Subject: Peter Salem)\n",
            "Comparing: hasparticipantrole -> child with isrelationshipfrom -> peter salem (Subject: Peter Salem)\n",
            "Comparing: hasparticipantrole -> child with hasparticipantroletype -> soldier (Subject: Peter Salem)\n",
            "Comparing: hasparticipantrole -> child with roleprovidedby -> american revolutionary war (Subject: Peter Salem)\n",
            "No match found for truth row: Peter Salem, Predicate: hasparticipantrole, Object: child\n",
            "Comparing: hasparticipantrole -> deceased person with haspreferrednamevariant -> peter salem (Subject: Peter Salem)\n",
            "Comparing: hasparticipantrole -> deceased person with fullnameasstring -> peter salem (Subject: Peter Salem)\n",
            "Comparing: hasparticipantrole -> deceased person with hassurnameasstring -> salem (Subject: Peter Salem)\n",
            "Comparing: hasparticipantrole -> deceased person with hasfirstnameasstring -> peter (Subject: Peter Salem)\n",
            "Comparing: hasparticipantrole -> deceased person with hasdescription -> peter salem was an african-american from massachusetts who served as a u.s. soldier in the american revolutionary war. born into slavery in framingham, he was freed by a later master, major lawson buckminster, to serve in the local militia. he then enlisted in the continental army, serving for nearly five years during the war. afterwards, he married and worked as a cane weaver. (Subject: Peter Salem)\n",
            "Comparing: hasparticipantrole -> deceased person with referstoplaceoforigin -> framingham, province of massachusetts bay (Subject: Peter Salem)\n",
            "Comparing: hasparticipantrole -> deceased person with has_race -> african-american (Subject: Peter Salem)\n",
            "Comparing: hasparticipantrole -> deceased person with hasbirthdate -> october 1, 1750 (Subject: Peter Salem)\n",
            "Comparing: hasparticipantrole -> deceased person with hasdeathdate -> august 16, 1816 (Subject: Peter Salem)\n",
            "Comparing: hasparticipantrole -> deceased person with hassex -> male (Subject: Peter Salem)\n",
            "Comparing: hasparticipantrole -> deceased person with hasstatusgeneratedevent -> emancipation, enlistment in the continental army, marriage (Subject: Peter Salem)\n",
            "Comparing: hasparticipantrole -> deceased person with hasvalue -> freed slave, soldier, cane weaver (Subject: Peter Salem)\n",
            "Comparing: hasparticipantrole -> deceased person with hasvalue -> soldier, cane weaver (Subject: Peter Salem)\n",
            "Comparing: hasparticipantrole -> deceased person with hasinteragentrelationshiptype -> enslaver (Subject: Peter Salem)\n",
            "Comparing: hasparticipantrole -> deceased person with isrelationshipto -> major lawson buckminster (Subject: Peter Salem)\n",
            "Comparing: hasparticipantrole -> deceased person with isrelationshipfrom -> peter salem (Subject: Peter Salem)\n",
            "Comparing: hasparticipantrole -> deceased person with hasparticipantroletype -> soldier (Subject: Peter Salem)\n",
            "Comparing: hasparticipantrole -> deceased person with roleprovidedby -> american revolutionary war (Subject: Peter Salem)\n",
            "No match found for truth row: Peter Salem, Predicate: hasparticipantrole, Object: deceased person\n",
            "Comparing: hasdescriptiveoccupation -> slave with haspreferrednamevariant -> peter salem (Subject: Peter Salem)\n",
            "Comparing: hasdescriptiveoccupation -> slave with fullnameasstring -> peter salem (Subject: Peter Salem)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hassurnameasstring -> salem (Subject: Peter Salem)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasfirstnameasstring -> peter (Subject: Peter Salem)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasdescription -> peter salem was an african-american from massachusetts who served as a u.s. soldier in the american revolutionary war. born into slavery in framingham, he was freed by a later master, major lawson buckminster, to serve in the local militia. he then enlisted in the continental army, serving for nearly five years during the war. afterwards, he married and worked as a cane weaver. (Subject: Peter Salem)\n",
            "Comparing: hasdescriptiveoccupation -> slave with referstoplaceoforigin -> framingham, province of massachusetts bay (Subject: Peter Salem)\n",
            "Comparing: hasdescriptiveoccupation -> slave with has_race -> african-american (Subject: Peter Salem)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasbirthdate -> october 1, 1750 (Subject: Peter Salem)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasdeathdate -> august 16, 1816 (Subject: Peter Salem)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hassex -> male (Subject: Peter Salem)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasstatusgeneratedevent -> emancipation, enlistment in the continental army, marriage (Subject: Peter Salem)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasvalue -> freed slave, soldier, cane weaver (Subject: Peter Salem)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasvalue -> soldier, cane weaver (Subject: Peter Salem)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasinteragentrelationshiptype -> enslaver (Subject: Peter Salem)\n",
            "Comparing: hasdescriptiveoccupation -> slave with isrelationshipto -> major lawson buckminster (Subject: Peter Salem)\n",
            "Comparing: hasdescriptiveoccupation -> slave with isrelationshipfrom -> peter salem (Subject: Peter Salem)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasparticipantroletype -> soldier (Subject: Peter Salem)\n",
            "Comparing: hasdescriptiveoccupation -> slave with roleprovidedby -> american revolutionary war (Subject: Peter Salem)\n",
            "No match found for truth row: Peter Salem, Predicate: hasdescriptiveoccupation, Object: slave\n",
            "Comparing: hasdescriptiveoccupation -> colonial militiaman with haspreferrednamevariant -> peter salem (Subject: Peter Salem)\n",
            "Comparing: hasdescriptiveoccupation -> colonial militiaman with fullnameasstring -> peter salem (Subject: Peter Salem)\n",
            "Comparing: hasdescriptiveoccupation -> colonial militiaman with hassurnameasstring -> salem (Subject: Peter Salem)\n",
            "Comparing: hasdescriptiveoccupation -> colonial militiaman with hasfirstnameasstring -> peter (Subject: Peter Salem)\n",
            "Comparing: hasdescriptiveoccupation -> colonial militiaman with hasdescription -> peter salem was an african-american from massachusetts who served as a u.s. soldier in the american revolutionary war. born into slavery in framingham, he was freed by a later master, major lawson buckminster, to serve in the local militia. he then enlisted in the continental army, serving for nearly five years during the war. afterwards, he married and worked as a cane weaver. (Subject: Peter Salem)\n",
            "Comparing: hasdescriptiveoccupation -> colonial militiaman with referstoplaceoforigin -> framingham, province of massachusetts bay (Subject: Peter Salem)\n",
            "Comparing: hasdescriptiveoccupation -> colonial militiaman with has_race -> african-american (Subject: Peter Salem)\n",
            "Comparing: hasdescriptiveoccupation -> colonial militiaman with hasbirthdate -> october 1, 1750 (Subject: Peter Salem)\n",
            "Comparing: hasdescriptiveoccupation -> colonial militiaman with hasdeathdate -> august 16, 1816 (Subject: Peter Salem)\n",
            "Comparing: hasdescriptiveoccupation -> colonial militiaman with hassex -> male (Subject: Peter Salem)\n",
            "Comparing: hasdescriptiveoccupation -> colonial militiaman with hasstatusgeneratedevent -> emancipation, enlistment in the continental army, marriage (Subject: Peter Salem)\n",
            "Comparing: hasdescriptiveoccupation -> colonial militiaman with hasvalue -> freed slave, soldier, cane weaver (Subject: Peter Salem)\n",
            "Comparing: hasdescriptiveoccupation -> colonial militiaman with hasvalue -> soldier, cane weaver (Subject: Peter Salem)\n",
            "Comparing: hasdescriptiveoccupation -> colonial militiaman with hasinteragentrelationshiptype -> enslaver (Subject: Peter Salem)\n",
            "Comparing: hasdescriptiveoccupation -> colonial militiaman with isrelationshipto -> major lawson buckminster (Subject: Peter Salem)\n",
            "Comparing: hasdescriptiveoccupation -> colonial militiaman with isrelationshipfrom -> peter salem (Subject: Peter Salem)\n",
            "Comparing: hasdescriptiveoccupation -> colonial militiaman with hasparticipantroletype -> soldier (Subject: Peter Salem)\n",
            "Comparing: hasdescriptiveoccupation -> colonial militiaman with roleprovidedby -> american revolutionary war (Subject: Peter Salem)\n",
            "No match found for truth row: Peter Salem, Predicate: hasdescriptiveoccupation, Object: colonial militiaman\n",
            "Comparing: hasfirstname -> peter with haspreferrednamevariant -> peter salem (Subject: Peter Salem)\n",
            "Comparing: hasfirstname -> peter with fullnameasstring -> peter salem (Subject: Peter Salem)\n",
            "Comparing: hasfirstname -> peter with hassurnameasstring -> salem (Subject: Peter Salem)\n",
            "Comparing: hasfirstname -> peter with hasfirstnameasstring -> peter (Subject: Peter Salem)\n",
            "Comparing: hasfirstname -> peter with hasdescription -> peter salem was an african-american from massachusetts who served as a u.s. soldier in the american revolutionary war. born into slavery in framingham, he was freed by a later master, major lawson buckminster, to serve in the local militia. he then enlisted in the continental army, serving for nearly five years during the war. afterwards, he married and worked as a cane weaver. (Subject: Peter Salem)\n",
            "Comparing: hasfirstname -> peter with referstoplaceoforigin -> framingham, province of massachusetts bay (Subject: Peter Salem)\n",
            "Comparing: hasfirstname -> peter with has_race -> african-american (Subject: Peter Salem)\n",
            "Comparing: hasfirstname -> peter with hasbirthdate -> october 1, 1750 (Subject: Peter Salem)\n",
            "Comparing: hasfirstname -> peter with hasdeathdate -> august 16, 1816 (Subject: Peter Salem)\n",
            "Comparing: hasfirstname -> peter with hassex -> male (Subject: Peter Salem)\n",
            "Comparing: hasfirstname -> peter with hasstatusgeneratedevent -> emancipation, enlistment in the continental army, marriage (Subject: Peter Salem)\n",
            "Comparing: hasfirstname -> peter with hasvalue -> freed slave, soldier, cane weaver (Subject: Peter Salem)\n",
            "Comparing: hasfirstname -> peter with hasvalue -> soldier, cane weaver (Subject: Peter Salem)\n",
            "Comparing: hasfirstname -> peter with hasinteragentrelationshiptype -> enslaver (Subject: Peter Salem)\n",
            "Comparing: hasfirstname -> peter with isrelationshipto -> major lawson buckminster (Subject: Peter Salem)\n",
            "Comparing: hasfirstname -> peter with isrelationshipfrom -> peter salem (Subject: Peter Salem)\n",
            "Comparing: hasfirstname -> peter with hasparticipantroletype -> soldier (Subject: Peter Salem)\n",
            "Comparing: hasfirstname -> peter with roleprovidedby -> american revolutionary war (Subject: Peter Salem)\n",
            "No match found for truth row: Peter Salem, Predicate: hasfirstname, Object: peter\n",
            "Comparing: hassurname -> salem with haspreferrednamevariant -> peter salem (Subject: Peter Salem)\n",
            "Comparing: hassurname -> salem with fullnameasstring -> peter salem (Subject: Peter Salem)\n",
            "Comparing: hassurname -> salem with hassurnameasstring -> salem (Subject: Peter Salem)\n",
            "Comparing: hassurname -> salem with hasfirstnameasstring -> peter (Subject: Peter Salem)\n",
            "Comparing: hassurname -> salem with hasdescription -> peter salem was an african-american from massachusetts who served as a u.s. soldier in the american revolutionary war. born into slavery in framingham, he was freed by a later master, major lawson buckminster, to serve in the local militia. he then enlisted in the continental army, serving for nearly five years during the war. afterwards, he married and worked as a cane weaver. (Subject: Peter Salem)\n",
            "Comparing: hassurname -> salem with referstoplaceoforigin -> framingham, province of massachusetts bay (Subject: Peter Salem)\n",
            "Comparing: hassurname -> salem with has_race -> african-american (Subject: Peter Salem)\n",
            "Comparing: hassurname -> salem with hasbirthdate -> october 1, 1750 (Subject: Peter Salem)\n",
            "Comparing: hassurname -> salem with hasdeathdate -> august 16, 1816 (Subject: Peter Salem)\n",
            "Comparing: hassurname -> salem with hassex -> male (Subject: Peter Salem)\n",
            "Comparing: hassurname -> salem with hasstatusgeneratedevent -> emancipation, enlistment in the continental army, marriage (Subject: Peter Salem)\n",
            "Comparing: hassurname -> salem with hasvalue -> freed slave, soldier, cane weaver (Subject: Peter Salem)\n",
            "Comparing: hassurname -> salem with hasvalue -> soldier, cane weaver (Subject: Peter Salem)\n",
            "Comparing: hassurname -> salem with hasinteragentrelationshiptype -> enslaver (Subject: Peter Salem)\n",
            "Comparing: hassurname -> salem with isrelationshipto -> major lawson buckminster (Subject: Peter Salem)\n",
            "Comparing: hassurname -> salem with isrelationshipfrom -> peter salem (Subject: Peter Salem)\n",
            "Comparing: hassurname -> salem with hasparticipantroletype -> soldier (Subject: Peter Salem)\n",
            "Comparing: hassurname -> salem with roleprovidedby -> american revolutionary war (Subject: Peter Salem)\n",
            "No match found for truth row: Peter Salem, Predicate: hassurname, Object: salem\n",
            "Comparing: hasname -> peter salem with haspreferrednamevariant -> peter salem (Subject: Peter Salem)\n",
            "Comparing: hasname -> peter salem with fullnameasstring -> peter salem (Subject: Peter Salem)\n",
            "Comparing: hasname -> peter salem with hassurnameasstring -> salem (Subject: Peter Salem)\n",
            "Comparing: hasname -> peter salem with hasfirstnameasstring -> peter (Subject: Peter Salem)\n",
            "Comparing: hasname -> peter salem with hasdescription -> peter salem was an african-american from massachusetts who served as a u.s. soldier in the american revolutionary war. born into slavery in framingham, he was freed by a later master, major lawson buckminster, to serve in the local militia. he then enlisted in the continental army, serving for nearly five years during the war. afterwards, he married and worked as a cane weaver. (Subject: Peter Salem)\n",
            "Comparing: hasname -> peter salem with referstoplaceoforigin -> framingham, province of massachusetts bay (Subject: Peter Salem)\n",
            "Comparing: hasname -> peter salem with has_race -> african-american (Subject: Peter Salem)\n",
            "Comparing: hasname -> peter salem with hasbirthdate -> october 1, 1750 (Subject: Peter Salem)\n",
            "Comparing: hasname -> peter salem with hasdeathdate -> august 16, 1816 (Subject: Peter Salem)\n",
            "Comparing: hasname -> peter salem with hassex -> male (Subject: Peter Salem)\n",
            "Comparing: hasname -> peter salem with hasstatusgeneratedevent -> emancipation, enlistment in the continental army, marriage (Subject: Peter Salem)\n",
            "Comparing: hasname -> peter salem with hasvalue -> freed slave, soldier, cane weaver (Subject: Peter Salem)\n",
            "Comparing: hasname -> peter salem with hasvalue -> soldier, cane weaver (Subject: Peter Salem)\n",
            "Comparing: hasname -> peter salem with hasinteragentrelationshiptype -> enslaver (Subject: Peter Salem)\n",
            "Comparing: hasname -> peter salem with isrelationshipto -> major lawson buckminster (Subject: Peter Salem)\n",
            "Comparing: hasname -> peter salem with isrelationshipfrom -> peter salem (Subject: Peter Salem)\n",
            "Comparing: hasname -> peter salem with hasparticipantroletype -> soldier (Subject: Peter Salem)\n",
            "Comparing: hasname -> peter salem with roleprovidedby -> american revolutionary war (Subject: Peter Salem)\n",
            "No match found for truth row: Peter Salem, Predicate: hasname, Object: peter salem\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. a soldier in the revolutionary war who was honored for killing major pitcairn, the leader of the british charge in the battle of bunker hill . with haspreferrednamevariant -> peter salem (Subject: Peter Salem)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. a soldier in the revolutionary war who was honored for killing major pitcairn, the leader of the british charge in the battle of bunker hill . with fullnameasstring -> peter salem (Subject: Peter Salem)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. a soldier in the revolutionary war who was honored for killing major pitcairn, the leader of the british charge in the battle of bunker hill . with hassurnameasstring -> salem (Subject: Peter Salem)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. a soldier in the revolutionary war who was honored for killing major pitcairn, the leader of the british charge in the battle of bunker hill . with hasfirstnameasstring -> peter (Subject: Peter Salem)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. a soldier in the revolutionary war who was honored for killing major pitcairn, the leader of the british charge in the battle of bunker hill . with hasdescription -> peter salem was an african-american from massachusetts who served as a u.s. soldier in the american revolutionary war. born into slavery in framingham, he was freed by a later master, major lawson buckminster, to serve in the local militia. he then enlisted in the continental army, serving for nearly five years during the war. afterwards, he married and worked as a cane weaver. (Subject: Peter Salem)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. a soldier in the revolutionary war who was honored for killing major pitcairn, the leader of the british charge in the battle of bunker hill . with referstoplaceoforigin -> framingham, province of massachusetts bay (Subject: Peter Salem)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. a soldier in the revolutionary war who was honored for killing major pitcairn, the leader of the british charge in the battle of bunker hill . with has_race -> african-american (Subject: Peter Salem)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. a soldier in the revolutionary war who was honored for killing major pitcairn, the leader of the british charge in the battle of bunker hill . with hasbirthdate -> october 1, 1750 (Subject: Peter Salem)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. a soldier in the revolutionary war who was honored for killing major pitcairn, the leader of the british charge in the battle of bunker hill . with hasdeathdate -> august 16, 1816 (Subject: Peter Salem)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. a soldier in the revolutionary war who was honored for killing major pitcairn, the leader of the british charge in the battle of bunker hill . with hassex -> male (Subject: Peter Salem)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. a soldier in the revolutionary war who was honored for killing major pitcairn, the leader of the british charge in the battle of bunker hill . with hasstatusgeneratedevent -> emancipation, enlistment in the continental army, marriage (Subject: Peter Salem)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. a soldier in the revolutionary war who was honored for killing major pitcairn, the leader of the british charge in the battle of bunker hill . with hasvalue -> freed slave, soldier, cane weaver (Subject: Peter Salem)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. a soldier in the revolutionary war who was honored for killing major pitcairn, the leader of the british charge in the battle of bunker hill . with hasvalue -> soldier, cane weaver (Subject: Peter Salem)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. a soldier in the revolutionary war who was honored for killing major pitcairn, the leader of the british charge in the battle of bunker hill . with hasinteragentrelationshiptype -> enslaver (Subject: Peter Salem)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. a soldier in the revolutionary war who was honored for killing major pitcairn, the leader of the british charge in the battle of bunker hill . with isrelationshipto -> major lawson buckminster (Subject: Peter Salem)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. a soldier in the revolutionary war who was honored for killing major pitcairn, the leader of the british charge in the battle of bunker hill . with isrelationshipfrom -> peter salem (Subject: Peter Salem)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. a soldier in the revolutionary war who was honored for killing major pitcairn, the leader of the british charge in the battle of bunker hill . with hasparticipantroletype -> soldier (Subject: Peter Salem)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. a soldier in the revolutionary war who was honored for killing major pitcairn, the leader of the british charge in the battle of bunker hill . with roleprovidedby -> american revolutionary war (Subject: Peter Salem)\n",
            "No match found for truth row: Peter Salem, Predicate: hasdescription, Object: born enslaved. free before 13th amendment. a soldier in the revolutionary war who was honored for killing major pitcairn, the leader of the british charge in the battle of bunker hill .\n",
            "\n",
            "Processing truth file: J._Vance_Lewis.tsv\n",
            "Truth DataFrame:\n",
            "                     0                         1  \\\n",
            "0   Joseph Vance Lewis               instance of   \n",
            "1   Joseph Vance Lewis                    hasSex   \n",
            "2   Joseph Vance Lewis        hasParticipantRole   \n",
            "3   Joseph Vance Lewis        hasParticipantRole   \n",
            "4   Joseph Vance Lewis  hasDescriptiveOccupation   \n",
            "5   Joseph Vance Lewis  hasDescriptiveOccupation   \n",
            "6   Joseph Vance Lewis  hasDescriptiveOccupation   \n",
            "7   Joseph Vance Lewis  hasDescriptiveOccupation   \n",
            "8   Joseph Vance Lewis  hasDescriptiveOccupation   \n",
            "9   Joseph Vance Lewis              hasFirstName   \n",
            "10  Joseph Vance Lewis                hasSurname   \n",
            "11  Joseph Vance Lewis                   hasName   \n",
            "12  Joseph Vance Lewis            hasDescription   \n",
            "\n",
            "                                                    2  \n",
            "0                                              Person  \n",
            "1                                                Male  \n",
            "2                                               Child  \n",
            "3                                     Deceased Person  \n",
            "4                                               Slave  \n",
            "5                                            Educator  \n",
            "6                                              Lawyer  \n",
            "7                          Autobiographer / Memoirist  \n",
            "8                              Slave Narrative Author  \n",
            "9                                        Joseph Vance  \n",
            "10                                              Lewis  \n",
            "11                                 Joseph Vance Lewis  \n",
            "12  Born Enslaved. An attorney who took many high-...  \n",
            "\n",
            "Processing truth file: Elizabeth_Keckley.tsv\n",
            "Truth DataFrame:\n",
            "                          0                         1  \\\n",
            "0   Elizabeth Hobbs Keckley               instance of   \n",
            "1   Elizabeth Hobbs Keckley                    hasSex   \n",
            "2   Elizabeth Hobbs Keckley        hasParticipantRole   \n",
            "3   Elizabeth Hobbs Keckley        hasParticipantRole   \n",
            "4   Elizabeth Hobbs Keckley  hasDescriptiveOccupation   \n",
            "5   Elizabeth Hobbs Keckley  hasDescriptiveOccupation   \n",
            "6   Elizabeth Hobbs Keckley  hasDescriptiveOccupation   \n",
            "7   Elizabeth Hobbs Keckley  hasDescriptiveOccupation   \n",
            "8   Elizabeth Hobbs Keckley  hasDescriptiveOccupation   \n",
            "9   Elizabeth Hobbs Keckley              hasFirstName   \n",
            "10  Elizabeth Hobbs Keckley                hasSurname   \n",
            "11  Elizabeth Hobbs Keckley                   hasName   \n",
            "12  Elizabeth Hobbs Keckley            hasDescription   \n",
            "\n",
            "                                                    2  \n",
            "0                                              Person  \n",
            "1                                              Female  \n",
            "2                                               Child  \n",
            "3                                     Deceased Person  \n",
            "4                                          Seamstress  \n",
            "5                                               Slave  \n",
            "6                                          Confidante  \n",
            "7                          Autobiographer / Memoirist  \n",
            "8                                   White House Staff  \n",
            "9                                     Elizabeth Hobbs  \n",
            "10                                            Keckley  \n",
            "11                            Elizabeth Hobbs Keckley  \n",
            "12  Enslaved. Free before 13th Amendment. A dressm...  \n",
            "\n",
            "Found matching generated file: Elizabeth_Keckley_triples.tsv\n",
            "Generated DataFrame before transformation:\n",
            "                      Subject                      Predicate  \\\n",
            "1   Elizabeth_Keckley_triples        hasPreferredNameVariant   \n",
            "2   Elizabeth_Keckley_triples               fullNameAsString   \n",
            "3   Elizabeth_Keckley_triples             hasSurnameAsString   \n",
            "4   Elizabeth_Keckley_triples           hasFirstnameAsString   \n",
            "5   Elizabeth_Keckley_triples                 hasDescription   \n",
            "6   Elizabeth_Keckley_triples          refersToPlaceOfOrigin   \n",
            "7   Elizabeth_Keckley_triples                       has_Race   \n",
            "8   Elizabeth_Keckley_triples                         hasSex   \n",
            "9   Elizabeth_Keckley_triples        hasStatusGeneratedEvent   \n",
            "10  Elizabeth_Keckley_triples                       hasValue   \n",
            "11  Elizabeth_Keckley_triples                       hasValue   \n",
            "12  Elizabeth_Keckley_triples  hasInterAgentRelationshipType   \n",
            "13  Elizabeth_Keckley_triples               isRelationshipTo   \n",
            "14  Elizabeth_Keckley_triples             isRelationshipFrom   \n",
            "15  Elizabeth_Keckley_triples         hasParticipantRoleType   \n",
            "16  Elizabeth_Keckley_triples                 roleProvidedBy   \n",
            "\n",
            "                                               Object  \n",
            "1                                   Elizabeth Keckley  \n",
            "2                                   Elizabeth Keckley  \n",
            "3                                             Keckley  \n",
            "4                                           Elizabeth  \n",
            "5   Formerly enslaved person, seamstress, author, ...  \n",
            "6                                            Virginia  \n",
            "7                                    African-American  \n",
            "8                                              Female  \n",
            "9                                         manumission  \n",
            "10              Formerly Enslaved Person, Free Person  \n",
            "11                                 Seamstress, Author  \n",
            "12                                              Owner  \n",
            "13                              Alexander M. Kirkland  \n",
            "14                                  Elizabeth Keckley  \n",
            "15                                           Employee  \n",
            "16                      Employment at the White House  \n",
            "Generated DataFrame after transformation:\n",
            "              Subject                      Predicate  \\\n",
            "1   Elizabeth Keckley        haspreferrednamevariant   \n",
            "2   Elizabeth Keckley               fullnameasstring   \n",
            "3   Elizabeth Keckley             hassurnameasstring   \n",
            "4   Elizabeth Keckley           hasfirstnameasstring   \n",
            "5   Elizabeth Keckley                 hasdescription   \n",
            "6   Elizabeth Keckley          referstoplaceoforigin   \n",
            "7   Elizabeth Keckley                       has_race   \n",
            "8   Elizabeth Keckley                         hassex   \n",
            "9   Elizabeth Keckley        hasstatusgeneratedevent   \n",
            "10  Elizabeth Keckley                       hasvalue   \n",
            "11  Elizabeth Keckley                       hasvalue   \n",
            "12  Elizabeth Keckley  hasinteragentrelationshiptype   \n",
            "13  Elizabeth Keckley               isrelationshipto   \n",
            "14  Elizabeth Keckley             isrelationshipfrom   \n",
            "15  Elizabeth Keckley         hasparticipantroletype   \n",
            "16  Elizabeth Keckley                 roleprovidedby   \n",
            "\n",
            "                                               Object  \n",
            "1                                   elizabeth keckley  \n",
            "2                                   elizabeth keckley  \n",
            "3                                             keckley  \n",
            "4                                           elizabeth  \n",
            "5   formerly enslaved person, seamstress, author, ...  \n",
            "6                                            virginia  \n",
            "7                                    african-american  \n",
            "8                                              female  \n",
            "9                                         manumission  \n",
            "10              formerly enslaved person, free person  \n",
            "11                                 seamstress, author  \n",
            "12                                              owner  \n",
            "13                              alexander m. kirkland  \n",
            "14                                  elizabeth keckley  \n",
            "15                                           employee  \n",
            "16                      employment at the white house  \n",
            "Comparing: instance of -> person with haspreferrednamevariant -> elizabeth keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: instance of -> person with fullnameasstring -> elizabeth keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: instance of -> person with hassurnameasstring -> keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: instance of -> person with hasfirstnameasstring -> elizabeth (Subject: Elizabeth Keckley)\n",
            "Comparing: instance of -> person with hasdescription -> formerly enslaved person, seamstress, author, and friend of mary todd lincoln. (Subject: Elizabeth Keckley)\n",
            "Comparing: instance of -> person with referstoplaceoforigin -> virginia (Subject: Elizabeth Keckley)\n",
            "Comparing: instance of -> person with has_race -> african-american (Subject: Elizabeth Keckley)\n",
            "Comparing: instance of -> person with hassex -> female (Subject: Elizabeth Keckley)\n",
            "Comparing: instance of -> person with hasstatusgeneratedevent -> manumission (Subject: Elizabeth Keckley)\n",
            "Comparing: instance of -> person with hasvalue -> formerly enslaved person, free person (Subject: Elizabeth Keckley)\n",
            "Comparing: instance of -> person with hasvalue -> seamstress, author (Subject: Elizabeth Keckley)\n",
            "Comparing: instance of -> person with hasinteragentrelationshiptype -> owner (Subject: Elizabeth Keckley)\n",
            "Comparing: instance of -> person with isrelationshipto -> alexander m. kirkland (Subject: Elizabeth Keckley)\n",
            "Comparing: instance of -> person with isrelationshipfrom -> elizabeth keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: instance of -> person with hasparticipantroletype -> employee (Subject: Elizabeth Keckley)\n",
            "Comparing: instance of -> person with roleprovidedby -> employment at the white house (Subject: Elizabeth Keckley)\n",
            "No match found for truth row: Elizabeth Hobbs Keckley, Predicate: instance of, Object: person\n",
            "Comparing: hassex -> female with haspreferrednamevariant -> elizabeth keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: hassex -> female with fullnameasstring -> elizabeth keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: hassex -> female with hassurnameasstring -> keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: hassex -> female with hasfirstnameasstring -> elizabeth (Subject: Elizabeth Keckley)\n",
            "Comparing: hassex -> female with hasdescription -> formerly enslaved person, seamstress, author, and friend of mary todd lincoln. (Subject: Elizabeth Keckley)\n",
            "Comparing: hassex -> female with referstoplaceoforigin -> virginia (Subject: Elizabeth Keckley)\n",
            "Comparing: hassex -> female with has_race -> african-american (Subject: Elizabeth Keckley)\n",
            "Comparing: hassex -> female with hassex -> female (Subject: Elizabeth Keckley)\n",
            "Match found for truth row Elizabeth Hobbs Keckley with generated row Elizabeth Keckley\n",
            "Comparing: hassex -> female with hasstatusgeneratedevent -> manumission (Subject: Elizabeth Keckley)\n",
            "Comparing: hassex -> female with hasvalue -> formerly enslaved person, free person (Subject: Elizabeth Keckley)\n",
            "Comparing: hassex -> female with hasvalue -> seamstress, author (Subject: Elizabeth Keckley)\n",
            "Comparing: hassex -> female with hasinteragentrelationshiptype -> owner (Subject: Elizabeth Keckley)\n",
            "Comparing: hassex -> female with isrelationshipto -> alexander m. kirkland (Subject: Elizabeth Keckley)\n",
            "Comparing: hassex -> female with isrelationshipfrom -> elizabeth keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: hassex -> female with hasparticipantroletype -> employee (Subject: Elizabeth Keckley)\n",
            "Comparing: hassex -> female with roleprovidedby -> employment at the white house (Subject: Elizabeth Keckley)\n",
            "Comparing: hasparticipantrole -> child with haspreferrednamevariant -> elizabeth keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: hasparticipantrole -> child with fullnameasstring -> elizabeth keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: hasparticipantrole -> child with hassurnameasstring -> keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: hasparticipantrole -> child with hasfirstnameasstring -> elizabeth (Subject: Elizabeth Keckley)\n",
            "Comparing: hasparticipantrole -> child with hasdescription -> formerly enslaved person, seamstress, author, and friend of mary todd lincoln. (Subject: Elizabeth Keckley)\n",
            "Comparing: hasparticipantrole -> child with referstoplaceoforigin -> virginia (Subject: Elizabeth Keckley)\n",
            "Comparing: hasparticipantrole -> child with has_race -> african-american (Subject: Elizabeth Keckley)\n",
            "Comparing: hasparticipantrole -> child with hassex -> female (Subject: Elizabeth Keckley)\n",
            "Comparing: hasparticipantrole -> child with hasstatusgeneratedevent -> manumission (Subject: Elizabeth Keckley)\n",
            "Comparing: hasparticipantrole -> child with hasvalue -> formerly enslaved person, free person (Subject: Elizabeth Keckley)\n",
            "Comparing: hasparticipantrole -> child with hasvalue -> seamstress, author (Subject: Elizabeth Keckley)\n",
            "Comparing: hasparticipantrole -> child with hasinteragentrelationshiptype -> owner (Subject: Elizabeth Keckley)\n",
            "Comparing: hasparticipantrole -> child with isrelationshipto -> alexander m. kirkland (Subject: Elizabeth Keckley)\n",
            "Comparing: hasparticipantrole -> child with isrelationshipfrom -> elizabeth keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: hasparticipantrole -> child with hasparticipantroletype -> employee (Subject: Elizabeth Keckley)\n",
            "Comparing: hasparticipantrole -> child with roleprovidedby -> employment at the white house (Subject: Elizabeth Keckley)\n",
            "No match found for truth row: Elizabeth Hobbs Keckley, Predicate: hasparticipantrole, Object: child\n",
            "Comparing: hasparticipantrole -> deceased person with haspreferrednamevariant -> elizabeth keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: hasparticipantrole -> deceased person with fullnameasstring -> elizabeth keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: hasparticipantrole -> deceased person with hassurnameasstring -> keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: hasparticipantrole -> deceased person with hasfirstnameasstring -> elizabeth (Subject: Elizabeth Keckley)\n",
            "Comparing: hasparticipantrole -> deceased person with hasdescription -> formerly enslaved person, seamstress, author, and friend of mary todd lincoln. (Subject: Elizabeth Keckley)\n",
            "Comparing: hasparticipantrole -> deceased person with referstoplaceoforigin -> virginia (Subject: Elizabeth Keckley)\n",
            "Comparing: hasparticipantrole -> deceased person with has_race -> african-american (Subject: Elizabeth Keckley)\n",
            "Comparing: hasparticipantrole -> deceased person with hassex -> female (Subject: Elizabeth Keckley)\n",
            "Comparing: hasparticipantrole -> deceased person with hasstatusgeneratedevent -> manumission (Subject: Elizabeth Keckley)\n",
            "Comparing: hasparticipantrole -> deceased person with hasvalue -> formerly enslaved person, free person (Subject: Elizabeth Keckley)\n",
            "Comparing: hasparticipantrole -> deceased person with hasvalue -> seamstress, author (Subject: Elizabeth Keckley)\n",
            "Comparing: hasparticipantrole -> deceased person with hasinteragentrelationshiptype -> owner (Subject: Elizabeth Keckley)\n",
            "Comparing: hasparticipantrole -> deceased person with isrelationshipto -> alexander m. kirkland (Subject: Elizabeth Keckley)\n",
            "Comparing: hasparticipantrole -> deceased person with isrelationshipfrom -> elizabeth keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: hasparticipantrole -> deceased person with hasparticipantroletype -> employee (Subject: Elizabeth Keckley)\n",
            "Comparing: hasparticipantrole -> deceased person with roleprovidedby -> employment at the white house (Subject: Elizabeth Keckley)\n",
            "No match found for truth row: Elizabeth Hobbs Keckley, Predicate: hasparticipantrole, Object: deceased person\n",
            "Comparing: hasdescriptiveoccupation -> seamstress with haspreferrednamevariant -> elizabeth keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> seamstress with fullnameasstring -> elizabeth keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> seamstress with hassurnameasstring -> keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> seamstress with hasfirstnameasstring -> elizabeth (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> seamstress with hasdescription -> formerly enslaved person, seamstress, author, and friend of mary todd lincoln. (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> seamstress with referstoplaceoforigin -> virginia (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> seamstress with has_race -> african-american (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> seamstress with hassex -> female (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> seamstress with hasstatusgeneratedevent -> manumission (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> seamstress with hasvalue -> formerly enslaved person, free person (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> seamstress with hasvalue -> seamstress, author (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> seamstress with hasinteragentrelationshiptype -> owner (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> seamstress with isrelationshipto -> alexander m. kirkland (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> seamstress with isrelationshipfrom -> elizabeth keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> seamstress with hasparticipantroletype -> employee (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> seamstress with roleprovidedby -> employment at the white house (Subject: Elizabeth Keckley)\n",
            "No match found for truth row: Elizabeth Hobbs Keckley, Predicate: hasdescriptiveoccupation, Object: seamstress\n",
            "Comparing: hasdescriptiveoccupation -> slave with haspreferrednamevariant -> elizabeth keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> slave with fullnameasstring -> elizabeth keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hassurnameasstring -> keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasfirstnameasstring -> elizabeth (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasdescription -> formerly enslaved person, seamstress, author, and friend of mary todd lincoln. (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> slave with referstoplaceoforigin -> virginia (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> slave with has_race -> african-american (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hassex -> female (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasstatusgeneratedevent -> manumission (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasvalue -> formerly enslaved person, free person (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasvalue -> seamstress, author (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasinteragentrelationshiptype -> owner (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> slave with isrelationshipto -> alexander m. kirkland (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> slave with isrelationshipfrom -> elizabeth keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasparticipantroletype -> employee (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> slave with roleprovidedby -> employment at the white house (Subject: Elizabeth Keckley)\n",
            "No match found for truth row: Elizabeth Hobbs Keckley, Predicate: hasdescriptiveoccupation, Object: slave\n",
            "Comparing: hasdescriptiveoccupation -> confidante with haspreferrednamevariant -> elizabeth keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> confidante with fullnameasstring -> elizabeth keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> confidante with hassurnameasstring -> keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> confidante with hasfirstnameasstring -> elizabeth (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> confidante with hasdescription -> formerly enslaved person, seamstress, author, and friend of mary todd lincoln. (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> confidante with referstoplaceoforigin -> virginia (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> confidante with has_race -> african-american (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> confidante with hassex -> female (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> confidante with hasstatusgeneratedevent -> manumission (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> confidante with hasvalue -> formerly enslaved person, free person (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> confidante with hasvalue -> seamstress, author (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> confidante with hasinteragentrelationshiptype -> owner (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> confidante with isrelationshipto -> alexander m. kirkland (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> confidante with isrelationshipfrom -> elizabeth keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> confidante with hasparticipantroletype -> employee (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> confidante with roleprovidedby -> employment at the white house (Subject: Elizabeth Keckley)\n",
            "No match found for truth row: Elizabeth Hobbs Keckley, Predicate: hasdescriptiveoccupation, Object: confidante\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with haspreferrednamevariant -> elizabeth keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with fullnameasstring -> elizabeth keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with hassurnameasstring -> keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with hasfirstnameasstring -> elizabeth (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with hasdescription -> formerly enslaved person, seamstress, author, and friend of mary todd lincoln. (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with referstoplaceoforigin -> virginia (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with has_race -> african-american (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with hassex -> female (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with hasstatusgeneratedevent -> manumission (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with hasvalue -> formerly enslaved person, free person (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with hasvalue -> seamstress, author (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with hasinteragentrelationshiptype -> owner (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with isrelationshipto -> alexander m. kirkland (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with isrelationshipfrom -> elizabeth keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with hasparticipantroletype -> employee (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> autobiographer / memoirist with roleprovidedby -> employment at the white house (Subject: Elizabeth Keckley)\n",
            "No match found for truth row: Elizabeth Hobbs Keckley, Predicate: hasdescriptiveoccupation, Object: autobiographer / memoirist\n",
            "Comparing: hasdescriptiveoccupation -> white house staff with haspreferrednamevariant -> elizabeth keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> white house staff with fullnameasstring -> elizabeth keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> white house staff with hassurnameasstring -> keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> white house staff with hasfirstnameasstring -> elizabeth (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> white house staff with hasdescription -> formerly enslaved person, seamstress, author, and friend of mary todd lincoln. (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> white house staff with referstoplaceoforigin -> virginia (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> white house staff with has_race -> african-american (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> white house staff with hassex -> female (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> white house staff with hasstatusgeneratedevent -> manumission (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> white house staff with hasvalue -> formerly enslaved person, free person (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> white house staff with hasvalue -> seamstress, author (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> white house staff with hasinteragentrelationshiptype -> owner (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> white house staff with isrelationshipto -> alexander m. kirkland (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> white house staff with isrelationshipfrom -> elizabeth keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> white house staff with hasparticipantroletype -> employee (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescriptiveoccupation -> white house staff with roleprovidedby -> employment at the white house (Subject: Elizabeth Keckley)\n",
            "No match found for truth row: Elizabeth Hobbs Keckley, Predicate: hasdescriptiveoccupation, Object: white house staff\n",
            "Comparing: hasfirstname -> elizabeth hobbs with haspreferrednamevariant -> elizabeth keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: hasfirstname -> elizabeth hobbs with fullnameasstring -> elizabeth keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: hasfirstname -> elizabeth hobbs with hassurnameasstring -> keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: hasfirstname -> elizabeth hobbs with hasfirstnameasstring -> elizabeth (Subject: Elizabeth Keckley)\n",
            "Comparing: hasfirstname -> elizabeth hobbs with hasdescription -> formerly enslaved person, seamstress, author, and friend of mary todd lincoln. (Subject: Elizabeth Keckley)\n",
            "Comparing: hasfirstname -> elizabeth hobbs with referstoplaceoforigin -> virginia (Subject: Elizabeth Keckley)\n",
            "Comparing: hasfirstname -> elizabeth hobbs with has_race -> african-american (Subject: Elizabeth Keckley)\n",
            "Comparing: hasfirstname -> elizabeth hobbs with hassex -> female (Subject: Elizabeth Keckley)\n",
            "Comparing: hasfirstname -> elizabeth hobbs with hasstatusgeneratedevent -> manumission (Subject: Elizabeth Keckley)\n",
            "Comparing: hasfirstname -> elizabeth hobbs with hasvalue -> formerly enslaved person, free person (Subject: Elizabeth Keckley)\n",
            "Comparing: hasfirstname -> elizabeth hobbs with hasvalue -> seamstress, author (Subject: Elizabeth Keckley)\n",
            "Comparing: hasfirstname -> elizabeth hobbs with hasinteragentrelationshiptype -> owner (Subject: Elizabeth Keckley)\n",
            "Comparing: hasfirstname -> elizabeth hobbs with isrelationshipto -> alexander m. kirkland (Subject: Elizabeth Keckley)\n",
            "Comparing: hasfirstname -> elizabeth hobbs with isrelationshipfrom -> elizabeth keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: hasfirstname -> elizabeth hobbs with hasparticipantroletype -> employee (Subject: Elizabeth Keckley)\n",
            "Comparing: hasfirstname -> elizabeth hobbs with roleprovidedby -> employment at the white house (Subject: Elizabeth Keckley)\n",
            "No match found for truth row: Elizabeth Hobbs Keckley, Predicate: hasfirstname, Object: elizabeth hobbs\n",
            "Comparing: hassurname -> keckley with haspreferrednamevariant -> elizabeth keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: hassurname -> keckley with fullnameasstring -> elizabeth keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: hassurname -> keckley with hassurnameasstring -> keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: hassurname -> keckley with hasfirstnameasstring -> elizabeth (Subject: Elizabeth Keckley)\n",
            "Comparing: hassurname -> keckley with hasdescription -> formerly enslaved person, seamstress, author, and friend of mary todd lincoln. (Subject: Elizabeth Keckley)\n",
            "Comparing: hassurname -> keckley with referstoplaceoforigin -> virginia (Subject: Elizabeth Keckley)\n",
            "Comparing: hassurname -> keckley with has_race -> african-american (Subject: Elizabeth Keckley)\n",
            "Comparing: hassurname -> keckley with hassex -> female (Subject: Elizabeth Keckley)\n",
            "Comparing: hassurname -> keckley with hasstatusgeneratedevent -> manumission (Subject: Elizabeth Keckley)\n",
            "Comparing: hassurname -> keckley with hasvalue -> formerly enslaved person, free person (Subject: Elizabeth Keckley)\n",
            "Comparing: hassurname -> keckley with hasvalue -> seamstress, author (Subject: Elizabeth Keckley)\n",
            "Comparing: hassurname -> keckley with hasinteragentrelationshiptype -> owner (Subject: Elizabeth Keckley)\n",
            "Comparing: hassurname -> keckley with isrelationshipto -> alexander m. kirkland (Subject: Elizabeth Keckley)\n",
            "Comparing: hassurname -> keckley with isrelationshipfrom -> elizabeth keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: hassurname -> keckley with hasparticipantroletype -> employee (Subject: Elizabeth Keckley)\n",
            "Comparing: hassurname -> keckley with roleprovidedby -> employment at the white house (Subject: Elizabeth Keckley)\n",
            "No match found for truth row: Elizabeth Hobbs Keckley, Predicate: hassurname, Object: keckley\n",
            "Comparing: hasname -> elizabeth hobbs keckley with haspreferrednamevariant -> elizabeth keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: hasname -> elizabeth hobbs keckley with fullnameasstring -> elizabeth keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: hasname -> elizabeth hobbs keckley with hassurnameasstring -> keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: hasname -> elizabeth hobbs keckley with hasfirstnameasstring -> elizabeth (Subject: Elizabeth Keckley)\n",
            "Comparing: hasname -> elizabeth hobbs keckley with hasdescription -> formerly enslaved person, seamstress, author, and friend of mary todd lincoln. (Subject: Elizabeth Keckley)\n",
            "Comparing: hasname -> elizabeth hobbs keckley with referstoplaceoforigin -> virginia (Subject: Elizabeth Keckley)\n",
            "Comparing: hasname -> elizabeth hobbs keckley with has_race -> african-american (Subject: Elizabeth Keckley)\n",
            "Comparing: hasname -> elizabeth hobbs keckley with hassex -> female (Subject: Elizabeth Keckley)\n",
            "Comparing: hasname -> elizabeth hobbs keckley with hasstatusgeneratedevent -> manumission (Subject: Elizabeth Keckley)\n",
            "Comparing: hasname -> elizabeth hobbs keckley with hasvalue -> formerly enslaved person, free person (Subject: Elizabeth Keckley)\n",
            "Comparing: hasname -> elizabeth hobbs keckley with hasvalue -> seamstress, author (Subject: Elizabeth Keckley)\n",
            "Comparing: hasname -> elizabeth hobbs keckley with hasinteragentrelationshiptype -> owner (Subject: Elizabeth Keckley)\n",
            "Comparing: hasname -> elizabeth hobbs keckley with isrelationshipto -> alexander m. kirkland (Subject: Elizabeth Keckley)\n",
            "Comparing: hasname -> elizabeth hobbs keckley with isrelationshipfrom -> elizabeth keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: hasname -> elizabeth hobbs keckley with hasparticipantroletype -> employee (Subject: Elizabeth Keckley)\n",
            "Comparing: hasname -> elizabeth hobbs keckley with roleprovidedby -> employment at the white house (Subject: Elizabeth Keckley)\n",
            "No match found for truth row: Elizabeth Hobbs Keckley, Predicate: hasname, Object: elizabeth hobbs keckley\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. a dressmaker for mary lincoln, activist for \"contraband\" (newly freed slaves), and an autobiographer. with haspreferrednamevariant -> elizabeth keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. a dressmaker for mary lincoln, activist for \"contraband\" (newly freed slaves), and an autobiographer. with fullnameasstring -> elizabeth keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. a dressmaker for mary lincoln, activist for \"contraband\" (newly freed slaves), and an autobiographer. with hassurnameasstring -> keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. a dressmaker for mary lincoln, activist for \"contraband\" (newly freed slaves), and an autobiographer. with hasfirstnameasstring -> elizabeth (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. a dressmaker for mary lincoln, activist for \"contraband\" (newly freed slaves), and an autobiographer. with hasdescription -> formerly enslaved person, seamstress, author, and friend of mary todd lincoln. (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. a dressmaker for mary lincoln, activist for \"contraband\" (newly freed slaves), and an autobiographer. with referstoplaceoforigin -> virginia (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. a dressmaker for mary lincoln, activist for \"contraband\" (newly freed slaves), and an autobiographer. with has_race -> african-american (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. a dressmaker for mary lincoln, activist for \"contraband\" (newly freed slaves), and an autobiographer. with hassex -> female (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. a dressmaker for mary lincoln, activist for \"contraband\" (newly freed slaves), and an autobiographer. with hasstatusgeneratedevent -> manumission (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. a dressmaker for mary lincoln, activist for \"contraband\" (newly freed slaves), and an autobiographer. with hasvalue -> formerly enslaved person, free person (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. a dressmaker for mary lincoln, activist for \"contraband\" (newly freed slaves), and an autobiographer. with hasvalue -> seamstress, author (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. a dressmaker for mary lincoln, activist for \"contraband\" (newly freed slaves), and an autobiographer. with hasinteragentrelationshiptype -> owner (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. a dressmaker for mary lincoln, activist for \"contraband\" (newly freed slaves), and an autobiographer. with isrelationshipto -> alexander m. kirkland (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. a dressmaker for mary lincoln, activist for \"contraband\" (newly freed slaves), and an autobiographer. with isrelationshipfrom -> elizabeth keckley (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. a dressmaker for mary lincoln, activist for \"contraband\" (newly freed slaves), and an autobiographer. with hasparticipantroletype -> employee (Subject: Elizabeth Keckley)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. a dressmaker for mary lincoln, activist for \"contraband\" (newly freed slaves), and an autobiographer. with roleprovidedby -> employment at the white house (Subject: Elizabeth Keckley)\n",
            "No match found for truth row: Elizabeth Hobbs Keckley, Predicate: hasdescription, Object: enslaved. free before 13th amendment. a dressmaker for mary lincoln, activist for \"contraband\" (newly freed slaves), and an autobiographer.\n",
            "\n",
            "Processing truth file: George_W._Albright.tsv\n",
            "Truth DataFrame:\n",
            "                            0                         1  \\\n",
            "0  George Washington Albright               instance of   \n",
            "1  George Washington Albright                    hasSex   \n",
            "2  George Washington Albright        hasParticipantRole   \n",
            "3  George Washington Albright        hasParticipantRole   \n",
            "4  George Washington Albright  hasDescriptiveOccupation   \n",
            "5  George Washington Albright  hasDescriptiveOccupation   \n",
            "6  George Washington Albright              hasFirstName   \n",
            "7  George Washington Albright                hasSurname   \n",
            "8  George Washington Albright                   hasName   \n",
            "9  George Washington Albright            hasDescription   \n",
            "\n",
            "                                                   2  \n",
            "0                                             Person  \n",
            "1                                               Male  \n",
            "2                                              Child  \n",
            "3                                    Deceased Person  \n",
            "4                                              Slave  \n",
            "5                                   State Legislator  \n",
            "6                                  George Washington  \n",
            "7                                           Albright  \n",
            "8                         George Washington Albright  \n",
            "9  Enslaved. A member of the Mississippi legislat...  \n",
            "\n",
            "Processing truth file: Daniel_Coker.tsv\n",
            "Truth DataFrame:\n",
            "                 0                         1  \\\n",
            "0   Daniel T Coker               instance of   \n",
            "1   Daniel T Coker                    hasSex   \n",
            "2   Daniel T Coker        hasParticipantRole   \n",
            "3   Daniel T Coker        hasParticipantRole   \n",
            "4   Daniel T Coker  hasDescriptiveOccupation   \n",
            "5   Daniel T Coker  hasDescriptiveOccupation   \n",
            "6   Daniel T Coker  hasDescriptiveOccupation   \n",
            "7   Daniel T Coker              hasFirstName   \n",
            "8   Daniel T Coker                hasSurname   \n",
            "9   Daniel T Coker                   hasName   \n",
            "10  Daniel T Coker            hasDescription   \n",
            "\n",
            "                                                    2  \n",
            "0                                              Person  \n",
            "1                                                Male  \n",
            "2                                               Child  \n",
            "3                                     Deceased Person  \n",
            "4                                               Slave  \n",
            "5                                            Educator  \n",
            "6                  African Methodist Episcopal Bishop  \n",
            "7                                           Daniel T.  \n",
            "8                                               Coker  \n",
            "9                                     Daniel T. Coker  \n",
            "10  Enslaved. Free before 13th Amendment. Publishe...  \n",
            "\n",
            "Found matching generated file: Daniel_Coker_triples.tsv\n",
            "Generated DataFrame before transformation:\n",
            "                 Subject                      Predicate  \\\n",
            "1   Daniel_Coker_triples        hasPreferredNameVariant   \n",
            "2   Daniel_Coker_triples               fullNameAsString   \n",
            "3   Daniel_Coker_triples             hasSurnameAsString   \n",
            "4   Daniel_Coker_triples           hasFirstnameAsString   \n",
            "5   Daniel_Coker_triples                 hasDescription   \n",
            "6   Daniel_Coker_triples          refersToPlaceOfOrigin   \n",
            "7   Daniel_Coker_triples                       has_Race   \n",
            "8   Daniel_Coker_triples                         hasSex   \n",
            "9   Daniel_Coker_triples        hasStatusGeneratedEvent   \n",
            "10  Daniel_Coker_triples                       hasValue   \n",
            "11  Daniel_Coker_triples                       hasValue   \n",
            "12  Daniel_Coker_triples  hasInterAgentRelationshipType   \n",
            "13  Daniel_Coker_triples               isRelationshipTo   \n",
            "14  Daniel_Coker_triples             isRelationshipFrom   \n",
            "15  Daniel_Coker_triples         hasParticipantRoleType   \n",
            "16  Daniel_Coker_triples                 roleProvidedBy   \n",
            "\n",
            "                                               Object  \n",
            "1                                        Daniel Coker  \n",
            "2                                        Daniel Coker  \n",
            "3                                               Coker  \n",
            "4                                              Daniel  \n",
            "5   Daniel Coker was a mixed-race man who was born...  \n",
            "6                                            Maryland  \n",
            "7                                          Mixed-Race  \n",
            "8                                                Male  \n",
            "9   Escape to New York, Change of Name, License to...  \n",
            "10                 Enslaved Person, Preacher, Settler  \n",
            "11                                  Preacher, Settler  \n",
            "12                                       Half-Brother  \n",
            "13                                Coker Half-Brothers  \n",
            "14                                       Daniel Coker  \n",
            "15                                             Leader  \n",
            "16             Settlement in Sierra Leone and Liberia  \n",
            "Generated DataFrame after transformation:\n",
            "         Subject                      Predicate  \\\n",
            "1   Daniel Coker        haspreferrednamevariant   \n",
            "2   Daniel Coker               fullnameasstring   \n",
            "3   Daniel Coker             hassurnameasstring   \n",
            "4   Daniel Coker           hasfirstnameasstring   \n",
            "5   Daniel Coker                 hasdescription   \n",
            "6   Daniel Coker          referstoplaceoforigin   \n",
            "7   Daniel Coker                       has_race   \n",
            "8   Daniel Coker                         hassex   \n",
            "9   Daniel Coker        hasstatusgeneratedevent   \n",
            "10  Daniel Coker                       hasvalue   \n",
            "11  Daniel Coker                       hasvalue   \n",
            "12  Daniel Coker  hasinteragentrelationshiptype   \n",
            "13  Daniel Coker               isrelationshipto   \n",
            "14  Daniel Coker             isrelationshipfrom   \n",
            "15  Daniel Coker         hasparticipantroletype   \n",
            "16  Daniel Coker                 roleprovidedby   \n",
            "\n",
            "                                               Object  \n",
            "1                                        daniel coker  \n",
            "2                                        daniel coker  \n",
            "3                                               coker  \n",
            "4                                              daniel  \n",
            "5   daniel coker was a mixed-race man who was born...  \n",
            "6                                            maryland  \n",
            "7                                          mixed-race  \n",
            "8                                                male  \n",
            "9   escape to new york, change of name, license to...  \n",
            "10                 enslaved person, preacher, settler  \n",
            "11                                  preacher, settler  \n",
            "12                                       half-brother  \n",
            "13                                coker half-brothers  \n",
            "14                                       daniel coker  \n",
            "15                                             leader  \n",
            "16             settlement in sierra leone and liberia  \n",
            "Comparing: instance of -> person with haspreferrednamevariant -> daniel coker (Subject: Daniel Coker)\n",
            "Comparing: instance of -> person with fullnameasstring -> daniel coker (Subject: Daniel Coker)\n",
            "Comparing: instance of -> person with hassurnameasstring -> coker (Subject: Daniel Coker)\n",
            "Comparing: instance of -> person with hasfirstnameasstring -> daniel (Subject: Daniel Coker)\n",
            "Comparing: instance of -> person with hasdescription -> daniel coker was a mixed-race man who was born into slavery, escaped to new york, changed his name, and became a licensed preacher. he later played a significant role in the early settlement of sierra leone and liberia. (Subject: Daniel Coker)\n",
            "Comparing: instance of -> person with referstoplaceoforigin -> maryland (Subject: Daniel Coker)\n",
            "Comparing: instance of -> person with has_race -> mixed-race (Subject: Daniel Coker)\n",
            "Comparing: instance of -> person with hassex -> male (Subject: Daniel Coker)\n",
            "Comparing: instance of -> person with hasstatusgeneratedevent -> escape to new york, change of name, license to preach, settlement in sierra leone and liberia (Subject: Daniel Coker)\n",
            "Comparing: instance of -> person with hasvalue -> enslaved person, preacher, settler (Subject: Daniel Coker)\n",
            "Comparing: instance of -> person with hasvalue -> preacher, settler (Subject: Daniel Coker)\n",
            "Comparing: instance of -> person with hasinteragentrelationshiptype -> half-brother (Subject: Daniel Coker)\n",
            "Comparing: instance of -> person with isrelationshipto -> coker half-brothers (Subject: Daniel Coker)\n",
            "Comparing: instance of -> person with isrelationshipfrom -> daniel coker (Subject: Daniel Coker)\n",
            "Comparing: instance of -> person with hasparticipantroletype -> leader (Subject: Daniel Coker)\n",
            "Comparing: instance of -> person with roleprovidedby -> settlement in sierra leone and liberia (Subject: Daniel Coker)\n",
            "No match found for truth row: Daniel T Coker, Predicate: instance of, Object: person\n",
            "Comparing: hassex -> male with haspreferrednamevariant -> daniel coker (Subject: Daniel Coker)\n",
            "Comparing: hassex -> male with fullnameasstring -> daniel coker (Subject: Daniel Coker)\n",
            "Comparing: hassex -> male with hassurnameasstring -> coker (Subject: Daniel Coker)\n",
            "Comparing: hassex -> male with hasfirstnameasstring -> daniel (Subject: Daniel Coker)\n",
            "Comparing: hassex -> male with hasdescription -> daniel coker was a mixed-race man who was born into slavery, escaped to new york, changed his name, and became a licensed preacher. he later played a significant role in the early settlement of sierra leone and liberia. (Subject: Daniel Coker)\n",
            "Comparing: hassex -> male with referstoplaceoforigin -> maryland (Subject: Daniel Coker)\n",
            "Comparing: hassex -> male with has_race -> mixed-race (Subject: Daniel Coker)\n",
            "Comparing: hassex -> male with hassex -> male (Subject: Daniel Coker)\n",
            "Match found for truth row Daniel T Coker with generated row Daniel Coker\n",
            "Comparing: hassex -> male with hasstatusgeneratedevent -> escape to new york, change of name, license to preach, settlement in sierra leone and liberia (Subject: Daniel Coker)\n",
            "Comparing: hassex -> male with hasvalue -> enslaved person, preacher, settler (Subject: Daniel Coker)\n",
            "Comparing: hassex -> male with hasvalue -> preacher, settler (Subject: Daniel Coker)\n",
            "Comparing: hassex -> male with hasinteragentrelationshiptype -> half-brother (Subject: Daniel Coker)\n",
            "Comparing: hassex -> male with isrelationshipto -> coker half-brothers (Subject: Daniel Coker)\n",
            "Comparing: hassex -> male with isrelationshipfrom -> daniel coker (Subject: Daniel Coker)\n",
            "Comparing: hassex -> male with hasparticipantroletype -> leader (Subject: Daniel Coker)\n",
            "Comparing: hassex -> male with roleprovidedby -> settlement in sierra leone and liberia (Subject: Daniel Coker)\n",
            "Comparing: hasparticipantrole -> child with haspreferrednamevariant -> daniel coker (Subject: Daniel Coker)\n",
            "Comparing: hasparticipantrole -> child with fullnameasstring -> daniel coker (Subject: Daniel Coker)\n",
            "Comparing: hasparticipantrole -> child with hassurnameasstring -> coker (Subject: Daniel Coker)\n",
            "Comparing: hasparticipantrole -> child with hasfirstnameasstring -> daniel (Subject: Daniel Coker)\n",
            "Comparing: hasparticipantrole -> child with hasdescription -> daniel coker was a mixed-race man who was born into slavery, escaped to new york, changed his name, and became a licensed preacher. he later played a significant role in the early settlement of sierra leone and liberia. (Subject: Daniel Coker)\n",
            "Comparing: hasparticipantrole -> child with referstoplaceoforigin -> maryland (Subject: Daniel Coker)\n",
            "Comparing: hasparticipantrole -> child with has_race -> mixed-race (Subject: Daniel Coker)\n",
            "Comparing: hasparticipantrole -> child with hassex -> male (Subject: Daniel Coker)\n",
            "Comparing: hasparticipantrole -> child with hasstatusgeneratedevent -> escape to new york, change of name, license to preach, settlement in sierra leone and liberia (Subject: Daniel Coker)\n",
            "Comparing: hasparticipantrole -> child with hasvalue -> enslaved person, preacher, settler (Subject: Daniel Coker)\n",
            "Comparing: hasparticipantrole -> child with hasvalue -> preacher, settler (Subject: Daniel Coker)\n",
            "Comparing: hasparticipantrole -> child with hasinteragentrelationshiptype -> half-brother (Subject: Daniel Coker)\n",
            "Comparing: hasparticipantrole -> child with isrelationshipto -> coker half-brothers (Subject: Daniel Coker)\n",
            "Comparing: hasparticipantrole -> child with isrelationshipfrom -> daniel coker (Subject: Daniel Coker)\n",
            "Comparing: hasparticipantrole -> child with hasparticipantroletype -> leader (Subject: Daniel Coker)\n",
            "Comparing: hasparticipantrole -> child with roleprovidedby -> settlement in sierra leone and liberia (Subject: Daniel Coker)\n",
            "No match found for truth row: Daniel T Coker, Predicate: hasparticipantrole, Object: child\n",
            "Comparing: hasparticipantrole -> deceased person with haspreferrednamevariant -> daniel coker (Subject: Daniel Coker)\n",
            "Comparing: hasparticipantrole -> deceased person with fullnameasstring -> daniel coker (Subject: Daniel Coker)\n",
            "Comparing: hasparticipantrole -> deceased person with hassurnameasstring -> coker (Subject: Daniel Coker)\n",
            "Comparing: hasparticipantrole -> deceased person with hasfirstnameasstring -> daniel (Subject: Daniel Coker)\n",
            "Comparing: hasparticipantrole -> deceased person with hasdescription -> daniel coker was a mixed-race man who was born into slavery, escaped to new york, changed his name, and became a licensed preacher. he later played a significant role in the early settlement of sierra leone and liberia. (Subject: Daniel Coker)\n",
            "Comparing: hasparticipantrole -> deceased person with referstoplaceoforigin -> maryland (Subject: Daniel Coker)\n",
            "Comparing: hasparticipantrole -> deceased person with has_race -> mixed-race (Subject: Daniel Coker)\n",
            "Comparing: hasparticipantrole -> deceased person with hassex -> male (Subject: Daniel Coker)\n",
            "Comparing: hasparticipantrole -> deceased person with hasstatusgeneratedevent -> escape to new york, change of name, license to preach, settlement in sierra leone and liberia (Subject: Daniel Coker)\n",
            "Comparing: hasparticipantrole -> deceased person with hasvalue -> enslaved person, preacher, settler (Subject: Daniel Coker)\n",
            "Comparing: hasparticipantrole -> deceased person with hasvalue -> preacher, settler (Subject: Daniel Coker)\n",
            "Comparing: hasparticipantrole -> deceased person with hasinteragentrelationshiptype -> half-brother (Subject: Daniel Coker)\n",
            "Comparing: hasparticipantrole -> deceased person with isrelationshipto -> coker half-brothers (Subject: Daniel Coker)\n",
            "Comparing: hasparticipantrole -> deceased person with isrelationshipfrom -> daniel coker (Subject: Daniel Coker)\n",
            "Comparing: hasparticipantrole -> deceased person with hasparticipantroletype -> leader (Subject: Daniel Coker)\n",
            "Comparing: hasparticipantrole -> deceased person with roleprovidedby -> settlement in sierra leone and liberia (Subject: Daniel Coker)\n",
            "No match found for truth row: Daniel T Coker, Predicate: hasparticipantrole, Object: deceased person\n",
            "Comparing: hasdescriptiveoccupation -> slave with haspreferrednamevariant -> daniel coker (Subject: Daniel Coker)\n",
            "Comparing: hasdescriptiveoccupation -> slave with fullnameasstring -> daniel coker (Subject: Daniel Coker)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hassurnameasstring -> coker (Subject: Daniel Coker)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasfirstnameasstring -> daniel (Subject: Daniel Coker)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasdescription -> daniel coker was a mixed-race man who was born into slavery, escaped to new york, changed his name, and became a licensed preacher. he later played a significant role in the early settlement of sierra leone and liberia. (Subject: Daniel Coker)\n",
            "Comparing: hasdescriptiveoccupation -> slave with referstoplaceoforigin -> maryland (Subject: Daniel Coker)\n",
            "Comparing: hasdescriptiveoccupation -> slave with has_race -> mixed-race (Subject: Daniel Coker)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hassex -> male (Subject: Daniel Coker)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasstatusgeneratedevent -> escape to new york, change of name, license to preach, settlement in sierra leone and liberia (Subject: Daniel Coker)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasvalue -> enslaved person, preacher, settler (Subject: Daniel Coker)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasvalue -> preacher, settler (Subject: Daniel Coker)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasinteragentrelationshiptype -> half-brother (Subject: Daniel Coker)\n",
            "Comparing: hasdescriptiveoccupation -> slave with isrelationshipto -> coker half-brothers (Subject: Daniel Coker)\n",
            "Comparing: hasdescriptiveoccupation -> slave with isrelationshipfrom -> daniel coker (Subject: Daniel Coker)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasparticipantroletype -> leader (Subject: Daniel Coker)\n",
            "Comparing: hasdescriptiveoccupation -> slave with roleprovidedby -> settlement in sierra leone and liberia (Subject: Daniel Coker)\n",
            "No match found for truth row: Daniel T Coker, Predicate: hasdescriptiveoccupation, Object: slave\n",
            "Comparing: hasdescriptiveoccupation -> educator with haspreferrednamevariant -> daniel coker (Subject: Daniel Coker)\n",
            "Comparing: hasdescriptiveoccupation -> educator with fullnameasstring -> daniel coker (Subject: Daniel Coker)\n",
            "Comparing: hasdescriptiveoccupation -> educator with hassurnameasstring -> coker (Subject: Daniel Coker)\n",
            "Comparing: hasdescriptiveoccupation -> educator with hasfirstnameasstring -> daniel (Subject: Daniel Coker)\n",
            "Comparing: hasdescriptiveoccupation -> educator with hasdescription -> daniel coker was a mixed-race man who was born into slavery, escaped to new york, changed his name, and became a licensed preacher. he later played a significant role in the early settlement of sierra leone and liberia. (Subject: Daniel Coker)\n",
            "Comparing: hasdescriptiveoccupation -> educator with referstoplaceoforigin -> maryland (Subject: Daniel Coker)\n",
            "Comparing: hasdescriptiveoccupation -> educator with has_race -> mixed-race (Subject: Daniel Coker)\n",
            "Comparing: hasdescriptiveoccupation -> educator with hassex -> male (Subject: Daniel Coker)\n",
            "Comparing: hasdescriptiveoccupation -> educator with hasstatusgeneratedevent -> escape to new york, change of name, license to preach, settlement in sierra leone and liberia (Subject: Daniel Coker)\n",
            "Comparing: hasdescriptiveoccupation -> educator with hasvalue -> enslaved person, preacher, settler (Subject: Daniel Coker)\n",
            "Comparing: hasdescriptiveoccupation -> educator with hasvalue -> preacher, settler (Subject: Daniel Coker)\n",
            "Comparing: hasdescriptiveoccupation -> educator with hasinteragentrelationshiptype -> half-brother (Subject: Daniel Coker)\n",
            "Comparing: hasdescriptiveoccupation -> educator with isrelationshipto -> coker half-brothers (Subject: Daniel Coker)\n",
            "Comparing: hasdescriptiveoccupation -> educator with isrelationshipfrom -> daniel coker (Subject: Daniel Coker)\n",
            "Comparing: hasdescriptiveoccupation -> educator with hasparticipantroletype -> leader (Subject: Daniel Coker)\n",
            "Comparing: hasdescriptiveoccupation -> educator with roleprovidedby -> settlement in sierra leone and liberia (Subject: Daniel Coker)\n",
            "No match found for truth row: Daniel T Coker, Predicate: hasdescriptiveoccupation, Object: educator\n",
            "Comparing: hasdescriptiveoccupation -> african methodist episcopal bishop with haspreferrednamevariant -> daniel coker (Subject: Daniel Coker)\n",
            "Comparing: hasdescriptiveoccupation -> african methodist episcopal bishop with fullnameasstring -> daniel coker (Subject: Daniel Coker)\n",
            "Comparing: hasdescriptiveoccupation -> african methodist episcopal bishop with hassurnameasstring -> coker (Subject: Daniel Coker)\n",
            "Comparing: hasdescriptiveoccupation -> african methodist episcopal bishop with hasfirstnameasstring -> daniel (Subject: Daniel Coker)\n",
            "Comparing: hasdescriptiveoccupation -> african methodist episcopal bishop with hasdescription -> daniel coker was a mixed-race man who was born into slavery, escaped to new york, changed his name, and became a licensed preacher. he later played a significant role in the early settlement of sierra leone and liberia. (Subject: Daniel Coker)\n",
            "Comparing: hasdescriptiveoccupation -> african methodist episcopal bishop with referstoplaceoforigin -> maryland (Subject: Daniel Coker)\n",
            "Comparing: hasdescriptiveoccupation -> african methodist episcopal bishop with has_race -> mixed-race (Subject: Daniel Coker)\n",
            "Comparing: hasdescriptiveoccupation -> african methodist episcopal bishop with hassex -> male (Subject: Daniel Coker)\n",
            "Comparing: hasdescriptiveoccupation -> african methodist episcopal bishop with hasstatusgeneratedevent -> escape to new york, change of name, license to preach, settlement in sierra leone and liberia (Subject: Daniel Coker)\n",
            "Comparing: hasdescriptiveoccupation -> african methodist episcopal bishop with hasvalue -> enslaved person, preacher, settler (Subject: Daniel Coker)\n",
            "Comparing: hasdescriptiveoccupation -> african methodist episcopal bishop with hasvalue -> preacher, settler (Subject: Daniel Coker)\n",
            "Comparing: hasdescriptiveoccupation -> african methodist episcopal bishop with hasinteragentrelationshiptype -> half-brother (Subject: Daniel Coker)\n",
            "Comparing: hasdescriptiveoccupation -> african methodist episcopal bishop with isrelationshipto -> coker half-brothers (Subject: Daniel Coker)\n",
            "Comparing: hasdescriptiveoccupation -> african methodist episcopal bishop with isrelationshipfrom -> daniel coker (Subject: Daniel Coker)\n",
            "Comparing: hasdescriptiveoccupation -> african methodist episcopal bishop with hasparticipantroletype -> leader (Subject: Daniel Coker)\n",
            "Comparing: hasdescriptiveoccupation -> african methodist episcopal bishop with roleprovidedby -> settlement in sierra leone and liberia (Subject: Daniel Coker)\n",
            "No match found for truth row: Daniel T Coker, Predicate: hasdescriptiveoccupation, Object: african methodist episcopal bishop\n",
            "Comparing: hasfirstname -> daniel t. with haspreferrednamevariant -> daniel coker (Subject: Daniel Coker)\n",
            "Comparing: hasfirstname -> daniel t. with fullnameasstring -> daniel coker (Subject: Daniel Coker)\n",
            "Comparing: hasfirstname -> daniel t. with hassurnameasstring -> coker (Subject: Daniel Coker)\n",
            "Comparing: hasfirstname -> daniel t. with hasfirstnameasstring -> daniel (Subject: Daniel Coker)\n",
            "Comparing: hasfirstname -> daniel t. with hasdescription -> daniel coker was a mixed-race man who was born into slavery, escaped to new york, changed his name, and became a licensed preacher. he later played a significant role in the early settlement of sierra leone and liberia. (Subject: Daniel Coker)\n",
            "Comparing: hasfirstname -> daniel t. with referstoplaceoforigin -> maryland (Subject: Daniel Coker)\n",
            "Comparing: hasfirstname -> daniel t. with has_race -> mixed-race (Subject: Daniel Coker)\n",
            "Comparing: hasfirstname -> daniel t. with hassex -> male (Subject: Daniel Coker)\n",
            "Comparing: hasfirstname -> daniel t. with hasstatusgeneratedevent -> escape to new york, change of name, license to preach, settlement in sierra leone and liberia (Subject: Daniel Coker)\n",
            "Comparing: hasfirstname -> daniel t. with hasvalue -> enslaved person, preacher, settler (Subject: Daniel Coker)\n",
            "Comparing: hasfirstname -> daniel t. with hasvalue -> preacher, settler (Subject: Daniel Coker)\n",
            "Comparing: hasfirstname -> daniel t. with hasinteragentrelationshiptype -> half-brother (Subject: Daniel Coker)\n",
            "Comparing: hasfirstname -> daniel t. with isrelationshipto -> coker half-brothers (Subject: Daniel Coker)\n",
            "Comparing: hasfirstname -> daniel t. with isrelationshipfrom -> daniel coker (Subject: Daniel Coker)\n",
            "Comparing: hasfirstname -> daniel t. with hasparticipantroletype -> leader (Subject: Daniel Coker)\n",
            "Comparing: hasfirstname -> daniel t. with roleprovidedby -> settlement in sierra leone and liberia (Subject: Daniel Coker)\n",
            "No match found for truth row: Daniel T Coker, Predicate: hasfirstname, Object: daniel t.\n",
            "Comparing: hassurname -> coker with haspreferrednamevariant -> daniel coker (Subject: Daniel Coker)\n",
            "Comparing: hassurname -> coker with fullnameasstring -> daniel coker (Subject: Daniel Coker)\n",
            "Comparing: hassurname -> coker with hassurnameasstring -> coker (Subject: Daniel Coker)\n",
            "Comparing: hassurname -> coker with hasfirstnameasstring -> daniel (Subject: Daniel Coker)\n",
            "Comparing: hassurname -> coker with hasdescription -> daniel coker was a mixed-race man who was born into slavery, escaped to new york, changed his name, and became a licensed preacher. he later played a significant role in the early settlement of sierra leone and liberia. (Subject: Daniel Coker)\n",
            "Comparing: hassurname -> coker with referstoplaceoforigin -> maryland (Subject: Daniel Coker)\n",
            "Comparing: hassurname -> coker with has_race -> mixed-race (Subject: Daniel Coker)\n",
            "Comparing: hassurname -> coker with hassex -> male (Subject: Daniel Coker)\n",
            "Comparing: hassurname -> coker with hasstatusgeneratedevent -> escape to new york, change of name, license to preach, settlement in sierra leone and liberia (Subject: Daniel Coker)\n",
            "Comparing: hassurname -> coker with hasvalue -> enslaved person, preacher, settler (Subject: Daniel Coker)\n",
            "Comparing: hassurname -> coker with hasvalue -> preacher, settler (Subject: Daniel Coker)\n",
            "Comparing: hassurname -> coker with hasinteragentrelationshiptype -> half-brother (Subject: Daniel Coker)\n",
            "Comparing: hassurname -> coker with isrelationshipto -> coker half-brothers (Subject: Daniel Coker)\n",
            "Comparing: hassurname -> coker with isrelationshipfrom -> daniel coker (Subject: Daniel Coker)\n",
            "Comparing: hassurname -> coker with hasparticipantroletype -> leader (Subject: Daniel Coker)\n",
            "Comparing: hassurname -> coker with roleprovidedby -> settlement in sierra leone and liberia (Subject: Daniel Coker)\n",
            "No match found for truth row: Daniel T Coker, Predicate: hassurname, Object: coker\n",
            "Comparing: hasname -> daniel t. coker with haspreferrednamevariant -> daniel coker (Subject: Daniel Coker)\n",
            "Comparing: hasname -> daniel t. coker with fullnameasstring -> daniel coker (Subject: Daniel Coker)\n",
            "Comparing: hasname -> daniel t. coker with hassurnameasstring -> coker (Subject: Daniel Coker)\n",
            "Comparing: hasname -> daniel t. coker with hasfirstnameasstring -> daniel (Subject: Daniel Coker)\n",
            "Comparing: hasname -> daniel t. coker with hasdescription -> daniel coker was a mixed-race man who was born into slavery, escaped to new york, changed his name, and became a licensed preacher. he later played a significant role in the early settlement of sierra leone and liberia. (Subject: Daniel Coker)\n",
            "Comparing: hasname -> daniel t. coker with referstoplaceoforigin -> maryland (Subject: Daniel Coker)\n",
            "Comparing: hasname -> daniel t. coker with has_race -> mixed-race (Subject: Daniel Coker)\n",
            "Comparing: hasname -> daniel t. coker with hassex -> male (Subject: Daniel Coker)\n",
            "Comparing: hasname -> daniel t. coker with hasstatusgeneratedevent -> escape to new york, change of name, license to preach, settlement in sierra leone and liberia (Subject: Daniel Coker)\n",
            "Comparing: hasname -> daniel t. coker with hasvalue -> enslaved person, preacher, settler (Subject: Daniel Coker)\n",
            "Comparing: hasname -> daniel t. coker with hasvalue -> preacher, settler (Subject: Daniel Coker)\n",
            "Comparing: hasname -> daniel t. coker with hasinteragentrelationshiptype -> half-brother (Subject: Daniel Coker)\n",
            "Comparing: hasname -> daniel t. coker with isrelationshipto -> coker half-brothers (Subject: Daniel Coker)\n",
            "Comparing: hasname -> daniel t. coker with isrelationshipfrom -> daniel coker (Subject: Daniel Coker)\n",
            "Comparing: hasname -> daniel t. coker with hasparticipantroletype -> leader (Subject: Daniel Coker)\n",
            "Comparing: hasname -> daniel t. coker with roleprovidedby -> settlement in sierra leone and liberia (Subject: Daniel Coker)\n",
            "No match found for truth row: Daniel T Coker, Predicate: hasname, Object: daniel t. coker\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. published a 43-page pamphlet containing one of his many antislavery sermons, was a founding member of the african methodist episcopal church, ad traveled to sierra leone as a missionary. with haspreferrednamevariant -> daniel coker (Subject: Daniel Coker)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. published a 43-page pamphlet containing one of his many antislavery sermons, was a founding member of the african methodist episcopal church, ad traveled to sierra leone as a missionary. with fullnameasstring -> daniel coker (Subject: Daniel Coker)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. published a 43-page pamphlet containing one of his many antislavery sermons, was a founding member of the african methodist episcopal church, ad traveled to sierra leone as a missionary. with hassurnameasstring -> coker (Subject: Daniel Coker)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. published a 43-page pamphlet containing one of his many antislavery sermons, was a founding member of the african methodist episcopal church, ad traveled to sierra leone as a missionary. with hasfirstnameasstring -> daniel (Subject: Daniel Coker)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. published a 43-page pamphlet containing one of his many antislavery sermons, was a founding member of the african methodist episcopal church, ad traveled to sierra leone as a missionary. with hasdescription -> daniel coker was a mixed-race man who was born into slavery, escaped to new york, changed his name, and became a licensed preacher. he later played a significant role in the early settlement of sierra leone and liberia. (Subject: Daniel Coker)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. published a 43-page pamphlet containing one of his many antislavery sermons, was a founding member of the african methodist episcopal church, ad traveled to sierra leone as a missionary. with referstoplaceoforigin -> maryland (Subject: Daniel Coker)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. published a 43-page pamphlet containing one of his many antislavery sermons, was a founding member of the african methodist episcopal church, ad traveled to sierra leone as a missionary. with has_race -> mixed-race (Subject: Daniel Coker)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. published a 43-page pamphlet containing one of his many antislavery sermons, was a founding member of the african methodist episcopal church, ad traveled to sierra leone as a missionary. with hassex -> male (Subject: Daniel Coker)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. published a 43-page pamphlet containing one of his many antislavery sermons, was a founding member of the african methodist episcopal church, ad traveled to sierra leone as a missionary. with hasstatusgeneratedevent -> escape to new york, change of name, license to preach, settlement in sierra leone and liberia (Subject: Daniel Coker)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. published a 43-page pamphlet containing one of his many antislavery sermons, was a founding member of the african methodist episcopal church, ad traveled to sierra leone as a missionary. with hasvalue -> enslaved person, preacher, settler (Subject: Daniel Coker)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. published a 43-page pamphlet containing one of his many antislavery sermons, was a founding member of the african methodist episcopal church, ad traveled to sierra leone as a missionary. with hasvalue -> preacher, settler (Subject: Daniel Coker)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. published a 43-page pamphlet containing one of his many antislavery sermons, was a founding member of the african methodist episcopal church, ad traveled to sierra leone as a missionary. with hasinteragentrelationshiptype -> half-brother (Subject: Daniel Coker)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. published a 43-page pamphlet containing one of his many antislavery sermons, was a founding member of the african methodist episcopal church, ad traveled to sierra leone as a missionary. with isrelationshipto -> coker half-brothers (Subject: Daniel Coker)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. published a 43-page pamphlet containing one of his many antislavery sermons, was a founding member of the african methodist episcopal church, ad traveled to sierra leone as a missionary. with isrelationshipfrom -> daniel coker (Subject: Daniel Coker)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. published a 43-page pamphlet containing one of his many antislavery sermons, was a founding member of the african methodist episcopal church, ad traveled to sierra leone as a missionary. with hasparticipantroletype -> leader (Subject: Daniel Coker)\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. published a 43-page pamphlet containing one of his many antislavery sermons, was a founding member of the african methodist episcopal church, ad traveled to sierra leone as a missionary. with roleprovidedby -> settlement in sierra leone and liberia (Subject: Daniel Coker)\n",
            "No match found for truth row: Daniel T Coker, Predicate: hasdescription, Object: enslaved. free before 13th amendment. published a 43-page pamphlet containing one of his many antislavery sermons, was a founding member of the african methodist episcopal church, ad traveled to sierra leone as a missionary.\n",
            "\n",
            "Processing truth file: Cudjoe.tsv\n",
            "Truth DataFrame:\n",
            "        0                             1                2\n",
            "0  Cudjoe                   instance of           Person\n",
            "1  Cudjoe                       hasName           Cudjoe\n",
            "2  Cudjoe                        hasSex             Male\n",
            "3  Cudjoe                        hasAge           Age 29\n",
            "4  Cudjoe  hasEthnolinguisticDescriptor           Yoruba\n",
            "5  Cudjoe            hasParticipantRole  Captured Person\n",
            "6  Cudjoe               hasPersonStatus  Enslaved Person\n",
            "7  Cudjoe              hasAlternateName            Kudjo\n",
            "\n",
            "Processing truth file: Elizabeth_Hobbs_Keckley.tsv\n",
            "Truth DataFrame:\n",
            "                          0                         1  \\\n",
            "0   Elizabeth Hobbs Keckley               instance of   \n",
            "1   Elizabeth Hobbs Keckley                    hasSex   \n",
            "2   Elizabeth Hobbs Keckley        hasParticipantRole   \n",
            "3   Elizabeth Hobbs Keckley        hasParticipantRole   \n",
            "4   Elizabeth Hobbs Keckley  hasDescriptiveOccupation   \n",
            "5   Elizabeth Hobbs Keckley  hasDescriptiveOccupation   \n",
            "6   Elizabeth Hobbs Keckley  hasDescriptiveOccupation   \n",
            "7   Elizabeth Hobbs Keckley  hasDescriptiveOccupation   \n",
            "8   Elizabeth Hobbs Keckley  hasDescriptiveOccupation   \n",
            "9   Elizabeth Hobbs Keckley              hasFirstName   \n",
            "10  Elizabeth Hobbs Keckley                hasSurname   \n",
            "11  Elizabeth Hobbs Keckley                   hasName   \n",
            "12  Elizabeth Hobbs Keckley            hasDescription   \n",
            "\n",
            "                                                    2  \n",
            "0                                              Person  \n",
            "1                                              Female  \n",
            "2                                               Child  \n",
            "3                                     Deceased Person  \n",
            "4                                          Seamstress  \n",
            "5                                               Slave  \n",
            "6                                          Confidante  \n",
            "7                          Autobiographer / Memoirist  \n",
            "8                                   White House Staff  \n",
            "9                                     Elizabeth Hobbs  \n",
            "10                                            Keckley  \n",
            "11                            Elizabeth Hobbs Keckley  \n",
            "12  Enslaved. Free before 13th Amendment. A dressm...  \n",
            "\n",
            "Processing truth file: George_W._Woodbey.tsv\n",
            "Truth DataFrame:\n",
            "                            0                         1  \\\n",
            "0   George Washington Woodbey               instance of   \n",
            "1   George Washington Woodbey                    hasSex   \n",
            "2   George Washington Woodbey        hasParticipantRole   \n",
            "3   George Washington Woodbey        hasParticipantRole   \n",
            "4   George Washington Woodbey  hasDescriptiveOccupation   \n",
            "5   George Washington Woodbey  hasDescriptiveOccupation   \n",
            "6   George Washington Woodbey  hasDescriptiveOccupation   \n",
            "7   George Washington Woodbey              hasFirstName   \n",
            "8   George Washington Woodbey                hasSurname   \n",
            "9   George Washington Woodbey                   hasName   \n",
            "10  George Washington Woodbey            hasDescription   \n",
            "\n",
            "                                                    2  \n",
            "0                                              Person  \n",
            "1                                                Male  \n",
            "2                                               Child  \n",
            "3                                     Deceased Person  \n",
            "4                                               Slave  \n",
            "5                                           Socialist  \n",
            "6                                      Baptist Clergy  \n",
            "7                                   George Washington  \n",
            "8                                             Woodbey  \n",
            "9                           George Washington Woodbey  \n",
            "10  Born Enslaved. In 1902 became the first Africa...  \n",
            "\n",
            "Processing truth file: Peter_Barnabas_Barrow.tsv\n",
            "Truth DataFrame:\n",
            "               0                         1  \\\n",
            "0   Peter Barrow               instance of   \n",
            "1   Peter Barrow                    hasSex   \n",
            "2   Peter Barrow        hasParticipantRole   \n",
            "3   Peter Barrow        hasParticipantRole   \n",
            "4   Peter Barrow  hasDescriptiveOccupation   \n",
            "5   Peter Barrow  hasDescriptiveOccupation   \n",
            "6   Peter Barrow  hasDescriptiveOccupation   \n",
            "7   Peter Barrow  hasDescriptiveOccupation   \n",
            "8   Peter Barrow              hasFirstName   \n",
            "9   Peter Barrow                hasSurname   \n",
            "10  Peter Barrow                   hasName   \n",
            "11  Peter Barrow            hasDescription   \n",
            "\n",
            "                                                    2  \n",
            "0                                              Person  \n",
            "1                                                Male  \n",
            "2                                               Child  \n",
            "3                                     Deceased Person  \n",
            "4                                               Slave  \n",
            "5                                    State Legislator  \n",
            "6                                             Soldier  \n",
            "7                                      Baptist Clergy  \n",
            "8                                               Peter  \n",
            "9                                              Barrow  \n",
            "10                                       Peter Barrow  \n",
            "11  Born Enslaved. A former slave who served in th...  \n",
            "\n",
            "Found matching generated file: Peter_Barnabas_Barrow_triples.tsv\n",
            "Generated DataFrame before transformation:\n",
            "                          Subject                Predicate  \\\n",
            "1   Peter_Barnabas_Barrow_triples  hasPreferredNameVariant   \n",
            "2   Peter_Barnabas_Barrow_triples         fullNameAsString   \n",
            "3   Peter_Barnabas_Barrow_triples       hasSurnameAsString   \n",
            "4   Peter_Barnabas_Barrow_triples     hasFirstnameAsString   \n",
            "5   Peter_Barnabas_Barrow_triples           hasDescription   \n",
            "6   Peter_Barnabas_Barrow_triples    refersToPlaceOfOrigin   \n",
            "7   Peter_Barnabas_Barrow_triples                 has_Race   \n",
            "8   Peter_Barnabas_Barrow_triples             hasBirthDate   \n",
            "9   Peter_Barnabas_Barrow_triples             hasDeathDate   \n",
            "10  Peter_Barnabas_Barrow_triples                   hasSex   \n",
            "11  Peter_Barnabas_Barrow_triples  hasStatusGeneratedEvent   \n",
            "12  Peter_Barnabas_Barrow_triples                 hasValue   \n",
            "13  Peter_Barnabas_Barrow_triples                 hasValue   \n",
            "\n",
            "                                               Object  \n",
            "1                               Peter Barnabas Barrow  \n",
            "2                               Peter Barnabas Barrow  \n",
            "3                                              Barrow  \n",
            "4                                               Peter  \n",
            "5   Peter Barnabas Barrow was a slave, soldier, st...  \n",
            "6                                Petersburg, Virginia  \n",
            "7                                    African-American  \n",
            "8                                                1840  \n",
            "9                                                1906  \n",
            "10                                               Male  \n",
            "11                freedom, state legislator, minister  \n",
            "12         Slave, Soldier, State Legislator, Minister  \n",
            "13                Soldier, State Legislator, Minister  \n",
            "Generated DataFrame after transformation:\n",
            "                  Subject                Predicate  \\\n",
            "1   Peter Barnabas Barrow  haspreferrednamevariant   \n",
            "2   Peter Barnabas Barrow         fullnameasstring   \n",
            "3   Peter Barnabas Barrow       hassurnameasstring   \n",
            "4   Peter Barnabas Barrow     hasfirstnameasstring   \n",
            "5   Peter Barnabas Barrow           hasdescription   \n",
            "6   Peter Barnabas Barrow    referstoplaceoforigin   \n",
            "7   Peter Barnabas Barrow                 has_race   \n",
            "8   Peter Barnabas Barrow             hasbirthdate   \n",
            "9   Peter Barnabas Barrow             hasdeathdate   \n",
            "10  Peter Barnabas Barrow                   hassex   \n",
            "11  Peter Barnabas Barrow  hasstatusgeneratedevent   \n",
            "12  Peter Barnabas Barrow                 hasvalue   \n",
            "13  Peter Barnabas Barrow                 hasvalue   \n",
            "\n",
            "                                               Object  \n",
            "1                               peter barnabas barrow  \n",
            "2                               peter barnabas barrow  \n",
            "3                                              barrow  \n",
            "4                                               peter  \n",
            "5   peter barnabas barrow was a slave, soldier, st...  \n",
            "6                                petersburg, virginia  \n",
            "7                                    african-american  \n",
            "8                                                1840  \n",
            "9                                                1906  \n",
            "10                                               male  \n",
            "11                freedom, state legislator, minister  \n",
            "12         slave, soldier, state legislator, minister  \n",
            "13                soldier, state legislator, minister  \n",
            "Comparing: instance of -> person with haspreferrednamevariant -> peter barnabas barrow (Subject: Peter Barnabas Barrow)\n",
            "Comparing: instance of -> person with fullnameasstring -> peter barnabas barrow (Subject: Peter Barnabas Barrow)\n",
            "Comparing: instance of -> person with hassurnameasstring -> barrow (Subject: Peter Barnabas Barrow)\n",
            "Comparing: instance of -> person with hasfirstnameasstring -> peter (Subject: Peter Barnabas Barrow)\n",
            "Comparing: instance of -> person with hasdescription -> peter barnabas barrow was a slave, soldier, state legislator, and minister in the united states. he served in the mississippi senate, and mississippi house of representatives 1870-1871. later in life he established a baptist church, calvary baptist church, in spokane, washington, and served as its pastor. he owned an apple orchard. (Subject: Peter Barnabas Barrow)\n",
            "Comparing: instance of -> person with referstoplaceoforigin -> petersburg, virginia (Subject: Peter Barnabas Barrow)\n",
            "Comparing: instance of -> person with has_race -> african-american (Subject: Peter Barnabas Barrow)\n",
            "Comparing: instance of -> person with hasbirthdate -> 1840 (Subject: Peter Barnabas Barrow)\n",
            "Comparing: instance of -> person with hasdeathdate -> 1906 (Subject: Peter Barnabas Barrow)\n",
            "Comparing: instance of -> person with hassex -> male (Subject: Peter Barnabas Barrow)\n",
            "Comparing: instance of -> person with hasstatusgeneratedevent -> freedom, state legislator, minister (Subject: Peter Barnabas Barrow)\n",
            "Comparing: instance of -> person with hasvalue -> slave, soldier, state legislator, minister (Subject: Peter Barnabas Barrow)\n",
            "Comparing: instance of -> person with hasvalue -> soldier, state legislator, minister (Subject: Peter Barnabas Barrow)\n",
            "No match found for truth row: Peter Barrow, Predicate: instance of, Object: person\n",
            "Comparing: hassex -> male with haspreferrednamevariant -> peter barnabas barrow (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hassex -> male with fullnameasstring -> peter barnabas barrow (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hassex -> male with hassurnameasstring -> barrow (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hassex -> male with hasfirstnameasstring -> peter (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hassex -> male with hasdescription -> peter barnabas barrow was a slave, soldier, state legislator, and minister in the united states. he served in the mississippi senate, and mississippi house of representatives 1870-1871. later in life he established a baptist church, calvary baptist church, in spokane, washington, and served as its pastor. he owned an apple orchard. (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hassex -> male with referstoplaceoforigin -> petersburg, virginia (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hassex -> male with has_race -> african-american (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hassex -> male with hasbirthdate -> 1840 (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hassex -> male with hasdeathdate -> 1906 (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hassex -> male with hassex -> male (Subject: Peter Barnabas Barrow)\n",
            "Match found for truth row Peter Barrow with generated row Peter Barnabas Barrow\n",
            "Comparing: hassex -> male with hasstatusgeneratedevent -> freedom, state legislator, minister (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hassex -> male with hasvalue -> slave, soldier, state legislator, minister (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hassex -> male with hasvalue -> soldier, state legislator, minister (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasparticipantrole -> child with haspreferrednamevariant -> peter barnabas barrow (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasparticipantrole -> child with fullnameasstring -> peter barnabas barrow (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasparticipantrole -> child with hassurnameasstring -> barrow (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasparticipantrole -> child with hasfirstnameasstring -> peter (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasparticipantrole -> child with hasdescription -> peter barnabas barrow was a slave, soldier, state legislator, and minister in the united states. he served in the mississippi senate, and mississippi house of representatives 1870-1871. later in life he established a baptist church, calvary baptist church, in spokane, washington, and served as its pastor. he owned an apple orchard. (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasparticipantrole -> child with referstoplaceoforigin -> petersburg, virginia (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasparticipantrole -> child with has_race -> african-american (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasparticipantrole -> child with hasbirthdate -> 1840 (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasparticipantrole -> child with hasdeathdate -> 1906 (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasparticipantrole -> child with hassex -> male (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasparticipantrole -> child with hasstatusgeneratedevent -> freedom, state legislator, minister (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasparticipantrole -> child with hasvalue -> slave, soldier, state legislator, minister (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasparticipantrole -> child with hasvalue -> soldier, state legislator, minister (Subject: Peter Barnabas Barrow)\n",
            "No match found for truth row: Peter Barrow, Predicate: hasparticipantrole, Object: child\n",
            "Comparing: hasparticipantrole -> deceased person with haspreferrednamevariant -> peter barnabas barrow (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasparticipantrole -> deceased person with fullnameasstring -> peter barnabas barrow (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasparticipantrole -> deceased person with hassurnameasstring -> barrow (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasparticipantrole -> deceased person with hasfirstnameasstring -> peter (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasparticipantrole -> deceased person with hasdescription -> peter barnabas barrow was a slave, soldier, state legislator, and minister in the united states. he served in the mississippi senate, and mississippi house of representatives 1870-1871. later in life he established a baptist church, calvary baptist church, in spokane, washington, and served as its pastor. he owned an apple orchard. (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasparticipantrole -> deceased person with referstoplaceoforigin -> petersburg, virginia (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasparticipantrole -> deceased person with has_race -> african-american (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasparticipantrole -> deceased person with hasbirthdate -> 1840 (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasparticipantrole -> deceased person with hasdeathdate -> 1906 (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasparticipantrole -> deceased person with hassex -> male (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasparticipantrole -> deceased person with hasstatusgeneratedevent -> freedom, state legislator, minister (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasparticipantrole -> deceased person with hasvalue -> slave, soldier, state legislator, minister (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasparticipantrole -> deceased person with hasvalue -> soldier, state legislator, minister (Subject: Peter Barnabas Barrow)\n",
            "No match found for truth row: Peter Barrow, Predicate: hasparticipantrole, Object: deceased person\n",
            "Comparing: hasdescriptiveoccupation -> slave with haspreferrednamevariant -> peter barnabas barrow (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescriptiveoccupation -> slave with fullnameasstring -> peter barnabas barrow (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hassurnameasstring -> barrow (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasfirstnameasstring -> peter (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasdescription -> peter barnabas barrow was a slave, soldier, state legislator, and minister in the united states. he served in the mississippi senate, and mississippi house of representatives 1870-1871. later in life he established a baptist church, calvary baptist church, in spokane, washington, and served as its pastor. he owned an apple orchard. (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescriptiveoccupation -> slave with referstoplaceoforigin -> petersburg, virginia (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescriptiveoccupation -> slave with has_race -> african-american (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasbirthdate -> 1840 (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasdeathdate -> 1906 (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hassex -> male (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasstatusgeneratedevent -> freedom, state legislator, minister (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasvalue -> slave, soldier, state legislator, minister (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasvalue -> soldier, state legislator, minister (Subject: Peter Barnabas Barrow)\n",
            "No match found for truth row: Peter Barrow, Predicate: hasdescriptiveoccupation, Object: slave\n",
            "Comparing: hasdescriptiveoccupation -> state legislator with haspreferrednamevariant -> peter barnabas barrow (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescriptiveoccupation -> state legislator with fullnameasstring -> peter barnabas barrow (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescriptiveoccupation -> state legislator with hassurnameasstring -> barrow (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescriptiveoccupation -> state legislator with hasfirstnameasstring -> peter (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescriptiveoccupation -> state legislator with hasdescription -> peter barnabas barrow was a slave, soldier, state legislator, and minister in the united states. he served in the mississippi senate, and mississippi house of representatives 1870-1871. later in life he established a baptist church, calvary baptist church, in spokane, washington, and served as its pastor. he owned an apple orchard. (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescriptiveoccupation -> state legislator with referstoplaceoforigin -> petersburg, virginia (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescriptiveoccupation -> state legislator with has_race -> african-american (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescriptiveoccupation -> state legislator with hasbirthdate -> 1840 (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescriptiveoccupation -> state legislator with hasdeathdate -> 1906 (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescriptiveoccupation -> state legislator with hassex -> male (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescriptiveoccupation -> state legislator with hasstatusgeneratedevent -> freedom, state legislator, minister (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescriptiveoccupation -> state legislator with hasvalue -> slave, soldier, state legislator, minister (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescriptiveoccupation -> state legislator with hasvalue -> soldier, state legislator, minister (Subject: Peter Barnabas Barrow)\n",
            "No match found for truth row: Peter Barrow, Predicate: hasdescriptiveoccupation, Object: state legislator\n",
            "Comparing: hasdescriptiveoccupation -> soldier with haspreferrednamevariant -> peter barnabas barrow (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescriptiveoccupation -> soldier with fullnameasstring -> peter barnabas barrow (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescriptiveoccupation -> soldier with hassurnameasstring -> barrow (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescriptiveoccupation -> soldier with hasfirstnameasstring -> peter (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescriptiveoccupation -> soldier with hasdescription -> peter barnabas barrow was a slave, soldier, state legislator, and minister in the united states. he served in the mississippi senate, and mississippi house of representatives 1870-1871. later in life he established a baptist church, calvary baptist church, in spokane, washington, and served as its pastor. he owned an apple orchard. (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescriptiveoccupation -> soldier with referstoplaceoforigin -> petersburg, virginia (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescriptiveoccupation -> soldier with has_race -> african-american (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescriptiveoccupation -> soldier with hasbirthdate -> 1840 (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescriptiveoccupation -> soldier with hasdeathdate -> 1906 (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescriptiveoccupation -> soldier with hassex -> male (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescriptiveoccupation -> soldier with hasstatusgeneratedevent -> freedom, state legislator, minister (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescriptiveoccupation -> soldier with hasvalue -> slave, soldier, state legislator, minister (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescriptiveoccupation -> soldier with hasvalue -> soldier, state legislator, minister (Subject: Peter Barnabas Barrow)\n",
            "No match found for truth row: Peter Barrow, Predicate: hasdescriptiveoccupation, Object: soldier\n",
            "Comparing: hasdescriptiveoccupation -> baptist clergy with haspreferrednamevariant -> peter barnabas barrow (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescriptiveoccupation -> baptist clergy with fullnameasstring -> peter barnabas barrow (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescriptiveoccupation -> baptist clergy with hassurnameasstring -> barrow (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescriptiveoccupation -> baptist clergy with hasfirstnameasstring -> peter (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescriptiveoccupation -> baptist clergy with hasdescription -> peter barnabas barrow was a slave, soldier, state legislator, and minister in the united states. he served in the mississippi senate, and mississippi house of representatives 1870-1871. later in life he established a baptist church, calvary baptist church, in spokane, washington, and served as its pastor. he owned an apple orchard. (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescriptiveoccupation -> baptist clergy with referstoplaceoforigin -> petersburg, virginia (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescriptiveoccupation -> baptist clergy with has_race -> african-american (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescriptiveoccupation -> baptist clergy with hasbirthdate -> 1840 (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescriptiveoccupation -> baptist clergy with hasdeathdate -> 1906 (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescriptiveoccupation -> baptist clergy with hassex -> male (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescriptiveoccupation -> baptist clergy with hasstatusgeneratedevent -> freedom, state legislator, minister (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescriptiveoccupation -> baptist clergy with hasvalue -> slave, soldier, state legislator, minister (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescriptiveoccupation -> baptist clergy with hasvalue -> soldier, state legislator, minister (Subject: Peter Barnabas Barrow)\n",
            "No match found for truth row: Peter Barrow, Predicate: hasdescriptiveoccupation, Object: baptist clergy\n",
            "Comparing: hasfirstname -> peter with haspreferrednamevariant -> peter barnabas barrow (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasfirstname -> peter with fullnameasstring -> peter barnabas barrow (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasfirstname -> peter with hassurnameasstring -> barrow (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasfirstname -> peter with hasfirstnameasstring -> peter (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasfirstname -> peter with hasdescription -> peter barnabas barrow was a slave, soldier, state legislator, and minister in the united states. he served in the mississippi senate, and mississippi house of representatives 1870-1871. later in life he established a baptist church, calvary baptist church, in spokane, washington, and served as its pastor. he owned an apple orchard. (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasfirstname -> peter with referstoplaceoforigin -> petersburg, virginia (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasfirstname -> peter with has_race -> african-american (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasfirstname -> peter with hasbirthdate -> 1840 (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasfirstname -> peter with hasdeathdate -> 1906 (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasfirstname -> peter with hassex -> male (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasfirstname -> peter with hasstatusgeneratedevent -> freedom, state legislator, minister (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasfirstname -> peter with hasvalue -> slave, soldier, state legislator, minister (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasfirstname -> peter with hasvalue -> soldier, state legislator, minister (Subject: Peter Barnabas Barrow)\n",
            "No match found for truth row: Peter Barrow, Predicate: hasfirstname, Object: peter\n",
            "Comparing: hassurname -> barrow with haspreferrednamevariant -> peter barnabas barrow (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hassurname -> barrow with fullnameasstring -> peter barnabas barrow (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hassurname -> barrow with hassurnameasstring -> barrow (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hassurname -> barrow with hasfirstnameasstring -> peter (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hassurname -> barrow with hasdescription -> peter barnabas barrow was a slave, soldier, state legislator, and minister in the united states. he served in the mississippi senate, and mississippi house of representatives 1870-1871. later in life he established a baptist church, calvary baptist church, in spokane, washington, and served as its pastor. he owned an apple orchard. (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hassurname -> barrow with referstoplaceoforigin -> petersburg, virginia (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hassurname -> barrow with has_race -> african-american (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hassurname -> barrow with hasbirthdate -> 1840 (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hassurname -> barrow with hasdeathdate -> 1906 (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hassurname -> barrow with hassex -> male (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hassurname -> barrow with hasstatusgeneratedevent -> freedom, state legislator, minister (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hassurname -> barrow with hasvalue -> slave, soldier, state legislator, minister (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hassurname -> barrow with hasvalue -> soldier, state legislator, minister (Subject: Peter Barnabas Barrow)\n",
            "No match found for truth row: Peter Barrow, Predicate: hassurname, Object: barrow\n",
            "Comparing: hasname -> peter barrow with haspreferrednamevariant -> peter barnabas barrow (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasname -> peter barrow with fullnameasstring -> peter barnabas barrow (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasname -> peter barrow with hassurnameasstring -> barrow (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasname -> peter barrow with hasfirstnameasstring -> peter (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasname -> peter barrow with hasdescription -> peter barnabas barrow was a slave, soldier, state legislator, and minister in the united states. he served in the mississippi senate, and mississippi house of representatives 1870-1871. later in life he established a baptist church, calvary baptist church, in spokane, washington, and served as its pastor. he owned an apple orchard. (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasname -> peter barrow with referstoplaceoforigin -> petersburg, virginia (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasname -> peter barrow with has_race -> african-american (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasname -> peter barrow with hasbirthdate -> 1840 (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasname -> peter barrow with hasdeathdate -> 1906 (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasname -> peter barrow with hassex -> male (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasname -> peter barrow with hasstatusgeneratedevent -> freedom, state legislator, minister (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasname -> peter barrow with hasvalue -> slave, soldier, state legislator, minister (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasname -> peter barrow with hasvalue -> soldier, state legislator, minister (Subject: Peter Barnabas Barrow)\n",
            "No match found for truth row: Peter Barrow, Predicate: hasname, Object: peter barrow\n",
            "Comparing: hasdescription -> born enslaved. a former slave who served in the mississippi house of representatives and of the state senate during the 1870s, and who later founded an african american community in spokane, wa, where he also joined the populist party and became a presidential elector. with haspreferrednamevariant -> peter barnabas barrow (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescription -> born enslaved. a former slave who served in the mississippi house of representatives and of the state senate during the 1870s, and who later founded an african american community in spokane, wa, where he also joined the populist party and became a presidential elector. with fullnameasstring -> peter barnabas barrow (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescription -> born enslaved. a former slave who served in the mississippi house of representatives and of the state senate during the 1870s, and who later founded an african american community in spokane, wa, where he also joined the populist party and became a presidential elector. with hassurnameasstring -> barrow (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescription -> born enslaved. a former slave who served in the mississippi house of representatives and of the state senate during the 1870s, and who later founded an african american community in spokane, wa, where he also joined the populist party and became a presidential elector. with hasfirstnameasstring -> peter (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescription -> born enslaved. a former slave who served in the mississippi house of representatives and of the state senate during the 1870s, and who later founded an african american community in spokane, wa, where he also joined the populist party and became a presidential elector. with hasdescription -> peter barnabas barrow was a slave, soldier, state legislator, and minister in the united states. he served in the mississippi senate, and mississippi house of representatives 1870-1871. later in life he established a baptist church, calvary baptist church, in spokane, washington, and served as its pastor. he owned an apple orchard. (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescription -> born enslaved. a former slave who served in the mississippi house of representatives and of the state senate during the 1870s, and who later founded an african american community in spokane, wa, where he also joined the populist party and became a presidential elector. with referstoplaceoforigin -> petersburg, virginia (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescription -> born enslaved. a former slave who served in the mississippi house of representatives and of the state senate during the 1870s, and who later founded an african american community in spokane, wa, where he also joined the populist party and became a presidential elector. with has_race -> african-american (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescription -> born enslaved. a former slave who served in the mississippi house of representatives and of the state senate during the 1870s, and who later founded an african american community in spokane, wa, where he also joined the populist party and became a presidential elector. with hasbirthdate -> 1840 (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescription -> born enslaved. a former slave who served in the mississippi house of representatives and of the state senate during the 1870s, and who later founded an african american community in spokane, wa, where he also joined the populist party and became a presidential elector. with hasdeathdate -> 1906 (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescription -> born enslaved. a former slave who served in the mississippi house of representatives and of the state senate during the 1870s, and who later founded an african american community in spokane, wa, where he also joined the populist party and became a presidential elector. with hassex -> male (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescription -> born enslaved. a former slave who served in the mississippi house of representatives and of the state senate during the 1870s, and who later founded an african american community in spokane, wa, where he also joined the populist party and became a presidential elector. with hasstatusgeneratedevent -> freedom, state legislator, minister (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescription -> born enslaved. a former slave who served in the mississippi house of representatives and of the state senate during the 1870s, and who later founded an african american community in spokane, wa, where he also joined the populist party and became a presidential elector. with hasvalue -> slave, soldier, state legislator, minister (Subject: Peter Barnabas Barrow)\n",
            "Comparing: hasdescription -> born enslaved. a former slave who served in the mississippi house of representatives and of the state senate during the 1870s, and who later founded an african american community in spokane, wa, where he also joined the populist party and became a presidential elector. with hasvalue -> soldier, state legislator, minister (Subject: Peter Barnabas Barrow)\n",
            "No match found for truth row: Peter Barrow, Predicate: hasdescription, Object: born enslaved. a former slave who served in the mississippi house of representatives and of the state senate during the 1870s, and who later founded an african american community in spokane, wa, where he also joined the populist party and became a presidential elector.\n",
            "\n",
            "Processing truth file: Quamina.tsv\n",
            "Truth DataFrame:\n",
            "         0                                1  \\\n",
            "0  Quamina                      instance of   \n",
            "1  Quamina                          hasName   \n",
            "2  Quamina                           hasSex   \n",
            "3  Quamina                  hasPersonStatus   \n",
            "4  Quamina               hasParticipantRole   \n",
            "5  Quamina               hasParticipantRole   \n",
            "6  Quamina                   hasRaceorColor   \n",
            "7  Quamina                    hasOccupation   \n",
            "8  Quamina                           hasAge   \n",
            "9  Quamina  hasInterAgentRelationshipTypeTo   \n",
            "\n",
            "                                      2  \n",
            "0                                Person  \n",
            "1                               Quamina  \n",
            "2                                  Male  \n",
            "3                       Enslaved Person  \n",
            "4                                 Child  \n",
            "5                     Registered Person  \n",
            "6                                 black  \n",
            "7  Agriculture, Husbandry, and Forestry  \n",
            "8                                Age 30  \n",
            "9                     Enslaver or Owner  \n",
            "\n",
            "Found matching generated file: Quamina_triples.tsv\n",
            "Generated DataFrame before transformation:\n",
            "            Subject                      Predicate  \\\n",
            "1   Quamina_triples        hasPreferredNameVariant   \n",
            "2   Quamina_triples               fullNameAsString   \n",
            "3   Quamina_triples             hasSurnameAsString   \n",
            "4   Quamina_triples           hasFirstnameAsString   \n",
            "5   Quamina_triples                 hasDescription   \n",
            "6   Quamina_triples          refersToPlaceOfOrigin   \n",
            "7   Quamina_triples                       has_Race   \n",
            "8   Quamina_triples                   hasBirthDate   \n",
            "9   Quamina_triples                   hasDeathDate   \n",
            "10  Quamina_triples                         hasSex   \n",
            "11  Quamina_triples        hasStatusGeneratedEvent   \n",
            "12  Quamina_triples                       hasValue   \n",
            "13  Quamina_triples                       hasValue   \n",
            "14  Quamina_triples  hasInterAgentRelationshipType   \n",
            "15  Quamina_triples               isRelationshipTo   \n",
            "16  Quamina_triples             isRelationshipFrom   \n",
            "17  Quamina_triples         hasParticipantRoleType   \n",
            "18  Quamina_triples                 roleProvidedBy   \n",
            "\n",
            "                                               Object  \n",
            "1                                             Quamina  \n",
            "2                                   Quamina Gladstone  \n",
            "3                                           Gladstone  \n",
            "4                                             Quamina  \n",
            "5   Quamina was a Guyanese slave from Africa and f...  \n",
            "6                                              Africa  \n",
            "7                                             African  \n",
            "8                                                1778  \n",
            "9                                   16 September 1823  \n",
            "10                                               Male  \n",
            "11                         Demerara rebellion of 1823  \n",
            "12                                              Slave  \n",
            "13                                          Carpenter  \n",
            "14                                             Father  \n",
            "15                                     Jack Gladstone  \n",
            "16                                  Quamina Gladstone  \n",
            "17                                              Rebel  \n",
            "18                         Demerara rebellion of 1823  \n",
            "Generated DataFrame after transformation:\n",
            "    Subject                      Predicate  \\\n",
            "1   Quamina        haspreferrednamevariant   \n",
            "2   Quamina               fullnameasstring   \n",
            "3   Quamina             hassurnameasstring   \n",
            "4   Quamina           hasfirstnameasstring   \n",
            "5   Quamina                 hasdescription   \n",
            "6   Quamina          referstoplaceoforigin   \n",
            "7   Quamina                       has_race   \n",
            "8   Quamina                   hasbirthdate   \n",
            "9   Quamina                   hasdeathdate   \n",
            "10  Quamina                         hassex   \n",
            "11  Quamina        hasstatusgeneratedevent   \n",
            "12  Quamina                       hasvalue   \n",
            "13  Quamina                       hasvalue   \n",
            "14  Quamina  hasinteragentrelationshiptype   \n",
            "15  Quamina               isrelationshipto   \n",
            "16  Quamina             isrelationshipfrom   \n",
            "17  Quamina         hasparticipantroletype   \n",
            "18  Quamina                 roleprovidedby   \n",
            "\n",
            "                                               Object  \n",
            "1                                             quamina  \n",
            "2                                   quamina gladstone  \n",
            "3                                           gladstone  \n",
            "4                                             quamina  \n",
            "5   quamina was a guyanese slave from africa and f...  \n",
            "6                                              africa  \n",
            "7                                             african  \n",
            "8                                                1778  \n",
            "9                                   16 september 1823  \n",
            "10                                               male  \n",
            "11                         demerara rebellion of 1823  \n",
            "12                                              slave  \n",
            "13                                          carpenter  \n",
            "14                                             father  \n",
            "15                                     jack gladstone  \n",
            "16                                  quamina gladstone  \n",
            "17                                              rebel  \n",
            "18                         demerara rebellion of 1823  \n",
            "Comparing: instance of -> person with haspreferrednamevariant -> quamina (Subject: Quamina)\n",
            "Comparing: instance of -> person with fullnameasstring -> quamina gladstone (Subject: Quamina)\n",
            "Comparing: instance of -> person with hassurnameasstring -> gladstone (Subject: Quamina)\n",
            "Comparing: instance of -> person with hasfirstnameasstring -> quamina (Subject: Quamina)\n",
            "Comparing: instance of -> person with hasdescription -> quamina was a guyanese slave from africa and father of jack gladstone. he and his son were involved in the demerara rebellion of 1823, one of the largest slave revolts in the british colonies before slavery was abolished. (Subject: Quamina)\n",
            "Comparing: instance of -> person with referstoplaceoforigin -> africa (Subject: Quamina)\n",
            "Comparing: instance of -> person with has_race -> african (Subject: Quamina)\n",
            "Comparing: instance of -> person with hasbirthdate -> 1778 (Subject: Quamina)\n",
            "Comparing: instance of -> person with hasdeathdate -> 16 september 1823 (Subject: Quamina)\n",
            "Comparing: instance of -> person with hassex -> male (Subject: Quamina)\n",
            "Comparing: instance of -> person with hasstatusgeneratedevent -> demerara rebellion of 1823 (Subject: Quamina)\n",
            "Comparing: instance of -> person with hasvalue -> slave (Subject: Quamina)\n",
            "Comparing: instance of -> person with hasvalue -> carpenter (Subject: Quamina)\n",
            "Comparing: instance of -> person with hasinteragentrelationshiptype -> father (Subject: Quamina)\n",
            "Comparing: instance of -> person with isrelationshipto -> jack gladstone (Subject: Quamina)\n",
            "Comparing: instance of -> person with isrelationshipfrom -> quamina gladstone (Subject: Quamina)\n",
            "Comparing: instance of -> person with hasparticipantroletype -> rebel (Subject: Quamina)\n",
            "Comparing: instance of -> person with roleprovidedby -> demerara rebellion of 1823 (Subject: Quamina)\n",
            "No match found for truth row: Quamina, Predicate: instance of, Object: person\n",
            "Comparing: hasname -> quamina with haspreferrednamevariant -> quamina (Subject: Quamina)\n",
            "Comparing: hasname -> quamina with fullnameasstring -> quamina gladstone (Subject: Quamina)\n",
            "Comparing: hasname -> quamina with hassurnameasstring -> gladstone (Subject: Quamina)\n",
            "Comparing: hasname -> quamina with hasfirstnameasstring -> quamina (Subject: Quamina)\n",
            "Comparing: hasname -> quamina with hasdescription -> quamina was a guyanese slave from africa and father of jack gladstone. he and his son were involved in the demerara rebellion of 1823, one of the largest slave revolts in the british colonies before slavery was abolished. (Subject: Quamina)\n",
            "Comparing: hasname -> quamina with referstoplaceoforigin -> africa (Subject: Quamina)\n",
            "Comparing: hasname -> quamina with has_race -> african (Subject: Quamina)\n",
            "Comparing: hasname -> quamina with hasbirthdate -> 1778 (Subject: Quamina)\n",
            "Comparing: hasname -> quamina with hasdeathdate -> 16 september 1823 (Subject: Quamina)\n",
            "Comparing: hasname -> quamina with hassex -> male (Subject: Quamina)\n",
            "Comparing: hasname -> quamina with hasstatusgeneratedevent -> demerara rebellion of 1823 (Subject: Quamina)\n",
            "Comparing: hasname -> quamina with hasvalue -> slave (Subject: Quamina)\n",
            "Comparing: hasname -> quamina with hasvalue -> carpenter (Subject: Quamina)\n",
            "Comparing: hasname -> quamina with hasinteragentrelationshiptype -> father (Subject: Quamina)\n",
            "Comparing: hasname -> quamina with isrelationshipto -> jack gladstone (Subject: Quamina)\n",
            "Comparing: hasname -> quamina with isrelationshipfrom -> quamina gladstone (Subject: Quamina)\n",
            "Comparing: hasname -> quamina with hasparticipantroletype -> rebel (Subject: Quamina)\n",
            "Comparing: hasname -> quamina with roleprovidedby -> demerara rebellion of 1823 (Subject: Quamina)\n",
            "No match found for truth row: Quamina, Predicate: hasname, Object: quamina\n",
            "Comparing: hassex -> male with haspreferrednamevariant -> quamina (Subject: Quamina)\n",
            "Comparing: hassex -> male with fullnameasstring -> quamina gladstone (Subject: Quamina)\n",
            "Comparing: hassex -> male with hassurnameasstring -> gladstone (Subject: Quamina)\n",
            "Comparing: hassex -> male with hasfirstnameasstring -> quamina (Subject: Quamina)\n",
            "Comparing: hassex -> male with hasdescription -> quamina was a guyanese slave from africa and father of jack gladstone. he and his son were involved in the demerara rebellion of 1823, one of the largest slave revolts in the british colonies before slavery was abolished. (Subject: Quamina)\n",
            "Comparing: hassex -> male with referstoplaceoforigin -> africa (Subject: Quamina)\n",
            "Comparing: hassex -> male with has_race -> african (Subject: Quamina)\n",
            "Comparing: hassex -> male with hasbirthdate -> 1778 (Subject: Quamina)\n",
            "Comparing: hassex -> male with hasdeathdate -> 16 september 1823 (Subject: Quamina)\n",
            "Comparing: hassex -> male with hassex -> male (Subject: Quamina)\n",
            "Match found for truth row Quamina with generated row Quamina\n",
            "Comparing: hassex -> male with hasstatusgeneratedevent -> demerara rebellion of 1823 (Subject: Quamina)\n",
            "Comparing: hassex -> male with hasvalue -> slave (Subject: Quamina)\n",
            "Comparing: hassex -> male with hasvalue -> carpenter (Subject: Quamina)\n",
            "Comparing: hassex -> male with hasinteragentrelationshiptype -> father (Subject: Quamina)\n",
            "Comparing: hassex -> male with isrelationshipto -> jack gladstone (Subject: Quamina)\n",
            "Comparing: hassex -> male with isrelationshipfrom -> quamina gladstone (Subject: Quamina)\n",
            "Comparing: hassex -> male with hasparticipantroletype -> rebel (Subject: Quamina)\n",
            "Comparing: hassex -> male with roleprovidedby -> demerara rebellion of 1823 (Subject: Quamina)\n",
            "Comparing: haspersonstatus -> enslaved person with haspreferrednamevariant -> quamina (Subject: Quamina)\n",
            "Comparing: haspersonstatus -> enslaved person with fullnameasstring -> quamina gladstone (Subject: Quamina)\n",
            "Comparing: haspersonstatus -> enslaved person with hassurnameasstring -> gladstone (Subject: Quamina)\n",
            "Comparing: haspersonstatus -> enslaved person with hasfirstnameasstring -> quamina (Subject: Quamina)\n",
            "Comparing: haspersonstatus -> enslaved person with hasdescription -> quamina was a guyanese slave from africa and father of jack gladstone. he and his son were involved in the demerara rebellion of 1823, one of the largest slave revolts in the british colonies before slavery was abolished. (Subject: Quamina)\n",
            "Comparing: haspersonstatus -> enslaved person with referstoplaceoforigin -> africa (Subject: Quamina)\n",
            "Comparing: haspersonstatus -> enslaved person with has_race -> african (Subject: Quamina)\n",
            "Comparing: haspersonstatus -> enslaved person with hasbirthdate -> 1778 (Subject: Quamina)\n",
            "Comparing: haspersonstatus -> enslaved person with hasdeathdate -> 16 september 1823 (Subject: Quamina)\n",
            "Comparing: haspersonstatus -> enslaved person with hassex -> male (Subject: Quamina)\n",
            "Comparing: haspersonstatus -> enslaved person with hasstatusgeneratedevent -> demerara rebellion of 1823 (Subject: Quamina)\n",
            "Comparing: haspersonstatus -> enslaved person with hasvalue -> slave (Subject: Quamina)\n",
            "Comparing: haspersonstatus -> enslaved person with hasvalue -> carpenter (Subject: Quamina)\n",
            "Comparing: haspersonstatus -> enslaved person with hasinteragentrelationshiptype -> father (Subject: Quamina)\n",
            "Comparing: haspersonstatus -> enslaved person with isrelationshipto -> jack gladstone (Subject: Quamina)\n",
            "Comparing: haspersonstatus -> enslaved person with isrelationshipfrom -> quamina gladstone (Subject: Quamina)\n",
            "Comparing: haspersonstatus -> enslaved person with hasparticipantroletype -> rebel (Subject: Quamina)\n",
            "Comparing: haspersonstatus -> enslaved person with roleprovidedby -> demerara rebellion of 1823 (Subject: Quamina)\n",
            "No match found for truth row: Quamina, Predicate: haspersonstatus, Object: enslaved person\n",
            "Comparing: hasparticipantrole -> child with haspreferrednamevariant -> quamina (Subject: Quamina)\n",
            "Comparing: hasparticipantrole -> child with fullnameasstring -> quamina gladstone (Subject: Quamina)\n",
            "Comparing: hasparticipantrole -> child with hassurnameasstring -> gladstone (Subject: Quamina)\n",
            "Comparing: hasparticipantrole -> child with hasfirstnameasstring -> quamina (Subject: Quamina)\n",
            "Comparing: hasparticipantrole -> child with hasdescription -> quamina was a guyanese slave from africa and father of jack gladstone. he and his son were involved in the demerara rebellion of 1823, one of the largest slave revolts in the british colonies before slavery was abolished. (Subject: Quamina)\n",
            "Comparing: hasparticipantrole -> child with referstoplaceoforigin -> africa (Subject: Quamina)\n",
            "Comparing: hasparticipantrole -> child with has_race -> african (Subject: Quamina)\n",
            "Comparing: hasparticipantrole -> child with hasbirthdate -> 1778 (Subject: Quamina)\n",
            "Comparing: hasparticipantrole -> child with hasdeathdate -> 16 september 1823 (Subject: Quamina)\n",
            "Comparing: hasparticipantrole -> child with hassex -> male (Subject: Quamina)\n",
            "Comparing: hasparticipantrole -> child with hasstatusgeneratedevent -> demerara rebellion of 1823 (Subject: Quamina)\n",
            "Comparing: hasparticipantrole -> child with hasvalue -> slave (Subject: Quamina)\n",
            "Comparing: hasparticipantrole -> child with hasvalue -> carpenter (Subject: Quamina)\n",
            "Comparing: hasparticipantrole -> child with hasinteragentrelationshiptype -> father (Subject: Quamina)\n",
            "Comparing: hasparticipantrole -> child with isrelationshipto -> jack gladstone (Subject: Quamina)\n",
            "Comparing: hasparticipantrole -> child with isrelationshipfrom -> quamina gladstone (Subject: Quamina)\n",
            "Comparing: hasparticipantrole -> child with hasparticipantroletype -> rebel (Subject: Quamina)\n",
            "Comparing: hasparticipantrole -> child with roleprovidedby -> demerara rebellion of 1823 (Subject: Quamina)\n",
            "No match found for truth row: Quamina, Predicate: hasparticipantrole, Object: child\n",
            "Comparing: hasparticipantrole -> registered person with haspreferrednamevariant -> quamina (Subject: Quamina)\n",
            "Comparing: hasparticipantrole -> registered person with fullnameasstring -> quamina gladstone (Subject: Quamina)\n",
            "Comparing: hasparticipantrole -> registered person with hassurnameasstring -> gladstone (Subject: Quamina)\n",
            "Comparing: hasparticipantrole -> registered person with hasfirstnameasstring -> quamina (Subject: Quamina)\n",
            "Comparing: hasparticipantrole -> registered person with hasdescription -> quamina was a guyanese slave from africa and father of jack gladstone. he and his son were involved in the demerara rebellion of 1823, one of the largest slave revolts in the british colonies before slavery was abolished. (Subject: Quamina)\n",
            "Comparing: hasparticipantrole -> registered person with referstoplaceoforigin -> africa (Subject: Quamina)\n",
            "Comparing: hasparticipantrole -> registered person with has_race -> african (Subject: Quamina)\n",
            "Comparing: hasparticipantrole -> registered person with hasbirthdate -> 1778 (Subject: Quamina)\n",
            "Comparing: hasparticipantrole -> registered person with hasdeathdate -> 16 september 1823 (Subject: Quamina)\n",
            "Comparing: hasparticipantrole -> registered person with hassex -> male (Subject: Quamina)\n",
            "Comparing: hasparticipantrole -> registered person with hasstatusgeneratedevent -> demerara rebellion of 1823 (Subject: Quamina)\n",
            "Comparing: hasparticipantrole -> registered person with hasvalue -> slave (Subject: Quamina)\n",
            "Comparing: hasparticipantrole -> registered person with hasvalue -> carpenter (Subject: Quamina)\n",
            "Comparing: hasparticipantrole -> registered person with hasinteragentrelationshiptype -> father (Subject: Quamina)\n",
            "Comparing: hasparticipantrole -> registered person with isrelationshipto -> jack gladstone (Subject: Quamina)\n",
            "Comparing: hasparticipantrole -> registered person with isrelationshipfrom -> quamina gladstone (Subject: Quamina)\n",
            "Comparing: hasparticipantrole -> registered person with hasparticipantroletype -> rebel (Subject: Quamina)\n",
            "Comparing: hasparticipantrole -> registered person with roleprovidedby -> demerara rebellion of 1823 (Subject: Quamina)\n",
            "No match found for truth row: Quamina, Predicate: hasparticipantrole, Object: registered person\n",
            "Comparing: hasraceorcolor -> black with haspreferrednamevariant -> quamina (Subject: Quamina)\n",
            "Comparing: hasraceorcolor -> black with fullnameasstring -> quamina gladstone (Subject: Quamina)\n",
            "Comparing: hasraceorcolor -> black with hassurnameasstring -> gladstone (Subject: Quamina)\n",
            "Comparing: hasraceorcolor -> black with hasfirstnameasstring -> quamina (Subject: Quamina)\n",
            "Comparing: hasraceorcolor -> black with hasdescription -> quamina was a guyanese slave from africa and father of jack gladstone. he and his son were involved in the demerara rebellion of 1823, one of the largest slave revolts in the british colonies before slavery was abolished. (Subject: Quamina)\n",
            "Comparing: hasraceorcolor -> black with referstoplaceoforigin -> africa (Subject: Quamina)\n",
            "Comparing: hasraceorcolor -> black with has_race -> african (Subject: Quamina)\n",
            "Comparing: hasraceorcolor -> black with hasbirthdate -> 1778 (Subject: Quamina)\n",
            "Comparing: hasraceorcolor -> black with hasdeathdate -> 16 september 1823 (Subject: Quamina)\n",
            "Comparing: hasraceorcolor -> black with hassex -> male (Subject: Quamina)\n",
            "Comparing: hasraceorcolor -> black with hasstatusgeneratedevent -> demerara rebellion of 1823 (Subject: Quamina)\n",
            "Comparing: hasraceorcolor -> black with hasvalue -> slave (Subject: Quamina)\n",
            "Comparing: hasraceorcolor -> black with hasvalue -> carpenter (Subject: Quamina)\n",
            "Comparing: hasraceorcolor -> black with hasinteragentrelationshiptype -> father (Subject: Quamina)\n",
            "Comparing: hasraceorcolor -> black with isrelationshipto -> jack gladstone (Subject: Quamina)\n",
            "Comparing: hasraceorcolor -> black with isrelationshipfrom -> quamina gladstone (Subject: Quamina)\n",
            "Comparing: hasraceorcolor -> black with hasparticipantroletype -> rebel (Subject: Quamina)\n",
            "Comparing: hasraceorcolor -> black with roleprovidedby -> demerara rebellion of 1823 (Subject: Quamina)\n",
            "No match found for truth row: Quamina, Predicate: hasraceorcolor, Object: black\n",
            "Comparing: hasoccupation -> agriculture, husbandry, and forestry with haspreferrednamevariant -> quamina (Subject: Quamina)\n",
            "Comparing: hasoccupation -> agriculture, husbandry, and forestry with fullnameasstring -> quamina gladstone (Subject: Quamina)\n",
            "Comparing: hasoccupation -> agriculture, husbandry, and forestry with hassurnameasstring -> gladstone (Subject: Quamina)\n",
            "Comparing: hasoccupation -> agriculture, husbandry, and forestry with hasfirstnameasstring -> quamina (Subject: Quamina)\n",
            "Comparing: hasoccupation -> agriculture, husbandry, and forestry with hasdescription -> quamina was a guyanese slave from africa and father of jack gladstone. he and his son were involved in the demerara rebellion of 1823, one of the largest slave revolts in the british colonies before slavery was abolished. (Subject: Quamina)\n",
            "Comparing: hasoccupation -> agriculture, husbandry, and forestry with referstoplaceoforigin -> africa (Subject: Quamina)\n",
            "Comparing: hasoccupation -> agriculture, husbandry, and forestry with has_race -> african (Subject: Quamina)\n",
            "Comparing: hasoccupation -> agriculture, husbandry, and forestry with hasbirthdate -> 1778 (Subject: Quamina)\n",
            "Comparing: hasoccupation -> agriculture, husbandry, and forestry with hasdeathdate -> 16 september 1823 (Subject: Quamina)\n",
            "Comparing: hasoccupation -> agriculture, husbandry, and forestry with hassex -> male (Subject: Quamina)\n",
            "Comparing: hasoccupation -> agriculture, husbandry, and forestry with hasstatusgeneratedevent -> demerara rebellion of 1823 (Subject: Quamina)\n",
            "Comparing: hasoccupation -> agriculture, husbandry, and forestry with hasvalue -> slave (Subject: Quamina)\n",
            "Comparing: hasoccupation -> agriculture, husbandry, and forestry with hasvalue -> carpenter (Subject: Quamina)\n",
            "Comparing: hasoccupation -> agriculture, husbandry, and forestry with hasinteragentrelationshiptype -> father (Subject: Quamina)\n",
            "Comparing: hasoccupation -> agriculture, husbandry, and forestry with isrelationshipto -> jack gladstone (Subject: Quamina)\n",
            "Comparing: hasoccupation -> agriculture, husbandry, and forestry with isrelationshipfrom -> quamina gladstone (Subject: Quamina)\n",
            "Comparing: hasoccupation -> agriculture, husbandry, and forestry with hasparticipantroletype -> rebel (Subject: Quamina)\n",
            "Comparing: hasoccupation -> agriculture, husbandry, and forestry with roleprovidedby -> demerara rebellion of 1823 (Subject: Quamina)\n",
            "No match found for truth row: Quamina, Predicate: hasoccupation, Object: agriculture, husbandry, and forestry\n",
            "Comparing: hasage -> age 30 with haspreferrednamevariant -> quamina (Subject: Quamina)\n",
            "Comparing: hasage -> age 30 with fullnameasstring -> quamina gladstone (Subject: Quamina)\n",
            "Comparing: hasage -> age 30 with hassurnameasstring -> gladstone (Subject: Quamina)\n",
            "Comparing: hasage -> age 30 with hasfirstnameasstring -> quamina (Subject: Quamina)\n",
            "Comparing: hasage -> age 30 with hasdescription -> quamina was a guyanese slave from africa and father of jack gladstone. he and his son were involved in the demerara rebellion of 1823, one of the largest slave revolts in the british colonies before slavery was abolished. (Subject: Quamina)\n",
            "Comparing: hasage -> age 30 with referstoplaceoforigin -> africa (Subject: Quamina)\n",
            "Comparing: hasage -> age 30 with has_race -> african (Subject: Quamina)\n",
            "Comparing: hasage -> age 30 with hasbirthdate -> 1778 (Subject: Quamina)\n",
            "Comparing: hasage -> age 30 with hasdeathdate -> 16 september 1823 (Subject: Quamina)\n",
            "Comparing: hasage -> age 30 with hassex -> male (Subject: Quamina)\n",
            "Comparing: hasage -> age 30 with hasstatusgeneratedevent -> demerara rebellion of 1823 (Subject: Quamina)\n",
            "Comparing: hasage -> age 30 with hasvalue -> slave (Subject: Quamina)\n",
            "Comparing: hasage -> age 30 with hasvalue -> carpenter (Subject: Quamina)\n",
            "Comparing: hasage -> age 30 with hasinteragentrelationshiptype -> father (Subject: Quamina)\n",
            "Comparing: hasage -> age 30 with isrelationshipto -> jack gladstone (Subject: Quamina)\n",
            "Comparing: hasage -> age 30 with isrelationshipfrom -> quamina gladstone (Subject: Quamina)\n",
            "Comparing: hasage -> age 30 with hasparticipantroletype -> rebel (Subject: Quamina)\n",
            "Comparing: hasage -> age 30 with roleprovidedby -> demerara rebellion of 1823 (Subject: Quamina)\n",
            "No match found for truth row: Quamina, Predicate: hasage, Object: age 30\n",
            "Comparing: hasinteragentrelationshiptypeto -> enslaver or owner with haspreferrednamevariant -> quamina (Subject: Quamina)\n",
            "Comparing: hasinteragentrelationshiptypeto -> enslaver or owner with fullnameasstring -> quamina gladstone (Subject: Quamina)\n",
            "Comparing: hasinteragentrelationshiptypeto -> enslaver or owner with hassurnameasstring -> gladstone (Subject: Quamina)\n",
            "Comparing: hasinteragentrelationshiptypeto -> enslaver or owner with hasfirstnameasstring -> quamina (Subject: Quamina)\n",
            "Comparing: hasinteragentrelationshiptypeto -> enslaver or owner with hasdescription -> quamina was a guyanese slave from africa and father of jack gladstone. he and his son were involved in the demerara rebellion of 1823, one of the largest slave revolts in the british colonies before slavery was abolished. (Subject: Quamina)\n",
            "Comparing: hasinteragentrelationshiptypeto -> enslaver or owner with referstoplaceoforigin -> africa (Subject: Quamina)\n",
            "Comparing: hasinteragentrelationshiptypeto -> enslaver or owner with has_race -> african (Subject: Quamina)\n",
            "Comparing: hasinteragentrelationshiptypeto -> enslaver or owner with hasbirthdate -> 1778 (Subject: Quamina)\n",
            "Comparing: hasinteragentrelationshiptypeto -> enslaver or owner with hasdeathdate -> 16 september 1823 (Subject: Quamina)\n",
            "Comparing: hasinteragentrelationshiptypeto -> enslaver or owner with hassex -> male (Subject: Quamina)\n",
            "Comparing: hasinteragentrelationshiptypeto -> enslaver or owner with hasstatusgeneratedevent -> demerara rebellion of 1823 (Subject: Quamina)\n",
            "Comparing: hasinteragentrelationshiptypeto -> enslaver or owner with hasvalue -> slave (Subject: Quamina)\n",
            "Comparing: hasinteragentrelationshiptypeto -> enslaver or owner with hasvalue -> carpenter (Subject: Quamina)\n",
            "Comparing: hasinteragentrelationshiptypeto -> enslaver or owner with hasinteragentrelationshiptype -> father (Subject: Quamina)\n",
            "Comparing: hasinteragentrelationshiptypeto -> enslaver or owner with isrelationshipto -> jack gladstone (Subject: Quamina)\n",
            "Comparing: hasinteragentrelationshiptypeto -> enslaver or owner with isrelationshipfrom -> quamina gladstone (Subject: Quamina)\n",
            "Comparing: hasinteragentrelationshiptypeto -> enslaver or owner with hasparticipantroletype -> rebel (Subject: Quamina)\n",
            "Comparing: hasinteragentrelationshiptypeto -> enslaver or owner with roleprovidedby -> demerara rebellion of 1823 (Subject: Quamina)\n",
            "No match found for truth row: Quamina, Predicate: hasinteragentrelationshiptypeto, Object: enslaver or owner\n",
            "\n",
            "Processing truth file: Jane_Johnson.tsv\n",
            "Truth DataFrame:\n",
            "               0                         1  \\\n",
            "0   Jane Johnson               instance of   \n",
            "1   Jane Johnson                    hasSex   \n",
            "2   Jane Johnson        hasParticipantRole   \n",
            "3   Jane Johnson        hasParticipantRole   \n",
            "4   Jane Johnson  hasDescriptiveOccupation   \n",
            "5   Jane Johnson  hasDescriptiveOccupation   \n",
            "6   Jane Johnson  hasDescriptiveOccupation   \n",
            "7   Jane Johnson              hasFirstName   \n",
            "8   Jane Johnson                hasSurname   \n",
            "9   Jane Johnson                   hasName   \n",
            "10  Jane Johnson            hasDescription   \n",
            "\n",
            "                                                    2  \n",
            "0                                              Person  \n",
            "1                                              Female  \n",
            "2                                               Child  \n",
            "3                                     Deceased Person  \n",
            "4                                               Slave  \n",
            "5                                      Fugitive Slave  \n",
            "6                      Underground Railroad Conductor  \n",
            "7                                                Jane  \n",
            "8                                             Johnson  \n",
            "9                                        Jane Johnson  \n",
            "10  Enslaved. Free before 13th Amendment. She may ...  \n",
            "\n",
            "Found matching generated file: Jane_Johnson_(slave)_triples.tsv\n",
            "Generated DataFrame before transformation:\n",
            "                         Subject                      Predicate  \\\n",
            "1   Jane_Johnson_(slave)_triples        hasPreferredNameVariant   \n",
            "2   Jane_Johnson_(slave)_triples               fullNameAsString   \n",
            "3   Jane_Johnson_(slave)_triples             hasSurnameAsString   \n",
            "4   Jane_Johnson_(slave)_triples           hasFirstnameAsString   \n",
            "5   Jane_Johnson_(slave)_triples                 hasDescription   \n",
            "6   Jane_Johnson_(slave)_triples          refersToPlaceOfOrigin   \n",
            "7   Jane_Johnson_(slave)_triples                       has_Race   \n",
            "8   Jane_Johnson_(slave)_triples                   hasDeathDate   \n",
            "9   Jane_Johnson_(slave)_triples                         hasSex   \n",
            "10  Jane_Johnson_(slave)_triples        hasStatusGeneratedEvent   \n",
            "11  Jane_Johnson_(slave)_triples                       hasValue   \n",
            "12  Jane_Johnson_(slave)_triples                       hasValue   \n",
            "13  Jane_Johnson_(slave)_triples  hasInterAgentRelationshipType   \n",
            "14  Jane_Johnson_(slave)_triples               isRelationshipTo   \n",
            "15  Jane_Johnson_(slave)_triples             isRelationshipFrom   \n",
            "16  Jane_Johnson_(slave)_triples         hasParticipantRoleType   \n",
            "17  Jane_Johnson_(slave)_triples                 roleProvidedBy   \n",
            "\n",
            "                                               Object  \n",
            "1                                        Jane Johnson  \n",
            "2                                        Jane Johnson  \n",
            "3                                             Johnson  \n",
            "4                                                Jane  \n",
            "5   Born into slavery, Jane Johnson escaped and te...  \n",
            "6                                    Washington, D.C.  \n",
            "7                                    African-American  \n",
            "8                                                1872  \n",
            "9                                              Female  \n",
            "10  Escape from slavery, Testimony in trial, Settl...  \n",
            "11                             Escaped Slave, Witness  \n",
            "12                                     Domestic Slave  \n",
            "13                                           Enslaver  \n",
            "14                                  John Hill Wheeler  \n",
            "15                                       Jane Johnson  \n",
            "16                                          Defendant  \n",
            "17        Trial of William Still and five dockworkers  \n",
            "Generated DataFrame after transformation:\n",
            "                 Subject                      Predicate  \\\n",
            "1   Jane Johnson (slave)        haspreferrednamevariant   \n",
            "2   Jane Johnson (slave)               fullnameasstring   \n",
            "3   Jane Johnson (slave)             hassurnameasstring   \n",
            "4   Jane Johnson (slave)           hasfirstnameasstring   \n",
            "5   Jane Johnson (slave)                 hasdescription   \n",
            "6   Jane Johnson (slave)          referstoplaceoforigin   \n",
            "7   Jane Johnson (slave)                       has_race   \n",
            "8   Jane Johnson (slave)                   hasdeathdate   \n",
            "9   Jane Johnson (slave)                         hassex   \n",
            "10  Jane Johnson (slave)        hasstatusgeneratedevent   \n",
            "11  Jane Johnson (slave)                       hasvalue   \n",
            "12  Jane Johnson (slave)                       hasvalue   \n",
            "13  Jane Johnson (slave)  hasinteragentrelationshiptype   \n",
            "14  Jane Johnson (slave)               isrelationshipto   \n",
            "15  Jane Johnson (slave)             isrelationshipfrom   \n",
            "16  Jane Johnson (slave)         hasparticipantroletype   \n",
            "17  Jane Johnson (slave)                 roleprovidedby   \n",
            "\n",
            "                                               Object  \n",
            "1                                        jane johnson  \n",
            "2                                        jane johnson  \n",
            "3                                             johnson  \n",
            "4                                                jane  \n",
            "5   born into slavery, jane johnson escaped and te...  \n",
            "6                                    washington, d.c.  \n",
            "7                                    african-american  \n",
            "8                                                1872  \n",
            "9                                              female  \n",
            "10  escape from slavery, testimony in trial, settl...  \n",
            "11                             escaped slave, witness  \n",
            "12                                     domestic slave  \n",
            "13                                           enslaver  \n",
            "14                                  john hill wheeler  \n",
            "15                                       jane johnson  \n",
            "16                                          defendant  \n",
            "17        trial of william still and five dockworkers  \n",
            "Comparing: instance of -> person with haspreferrednamevariant -> jane johnson (Subject: Jane Johnson (slave))\n",
            "Comparing: instance of -> person with fullnameasstring -> jane johnson (Subject: Jane Johnson (slave))\n",
            "Comparing: instance of -> person with hassurnameasstring -> johnson (Subject: Jane Johnson (slave))\n",
            "Comparing: instance of -> person with hasfirstnameasstring -> jane (Subject: Jane Johnson (slave))\n",
            "Comparing: instance of -> person with hasdescription -> born into slavery, jane johnson escaped and testified in a trial that led to the acquittal of those who aided her escape. she later settled in boston and married again. (Subject: Jane Johnson (slave))\n",
            "Comparing: instance of -> person with referstoplaceoforigin -> washington, d.c. (Subject: Jane Johnson (slave))\n",
            "Comparing: instance of -> person with has_race -> african-american (Subject: Jane Johnson (slave))\n",
            "Comparing: instance of -> person with hasdeathdate -> 1872 (Subject: Jane Johnson (slave))\n",
            "Comparing: instance of -> person with hassex -> female (Subject: Jane Johnson (slave))\n",
            "Comparing: instance of -> person with hasstatusgeneratedevent -> escape from slavery, testimony in trial, settlement in boston (Subject: Jane Johnson (slave))\n",
            "Comparing: instance of -> person with hasvalue -> escaped slave, witness (Subject: Jane Johnson (slave))\n",
            "Comparing: instance of -> person with hasvalue -> domestic slave (Subject: Jane Johnson (slave))\n",
            "Comparing: instance of -> person with hasinteragentrelationshiptype -> enslaver (Subject: Jane Johnson (slave))\n",
            "Comparing: instance of -> person with isrelationshipto -> john hill wheeler (Subject: Jane Johnson (slave))\n",
            "Comparing: instance of -> person with isrelationshipfrom -> jane johnson (Subject: Jane Johnson (slave))\n",
            "Comparing: instance of -> person with hasparticipantroletype -> defendant (Subject: Jane Johnson (slave))\n",
            "Comparing: instance of -> person with roleprovidedby -> trial of william still and five dockworkers (Subject: Jane Johnson (slave))\n",
            "No match found for truth row: Jane Johnson, Predicate: instance of, Object: person\n",
            "Comparing: hassex -> female with haspreferrednamevariant -> jane johnson (Subject: Jane Johnson (slave))\n",
            "Comparing: hassex -> female with fullnameasstring -> jane johnson (Subject: Jane Johnson (slave))\n",
            "Comparing: hassex -> female with hassurnameasstring -> johnson (Subject: Jane Johnson (slave))\n",
            "Comparing: hassex -> female with hasfirstnameasstring -> jane (Subject: Jane Johnson (slave))\n",
            "Comparing: hassex -> female with hasdescription -> born into slavery, jane johnson escaped and testified in a trial that led to the acquittal of those who aided her escape. she later settled in boston and married again. (Subject: Jane Johnson (slave))\n",
            "Comparing: hassex -> female with referstoplaceoforigin -> washington, d.c. (Subject: Jane Johnson (slave))\n",
            "Comparing: hassex -> female with has_race -> african-american (Subject: Jane Johnson (slave))\n",
            "Comparing: hassex -> female with hasdeathdate -> 1872 (Subject: Jane Johnson (slave))\n",
            "Comparing: hassex -> female with hassex -> female (Subject: Jane Johnson (slave))\n",
            "Match found for truth row Jane Johnson with generated row Jane Johnson (slave)\n",
            "Comparing: hassex -> female with hasstatusgeneratedevent -> escape from slavery, testimony in trial, settlement in boston (Subject: Jane Johnson (slave))\n",
            "Comparing: hassex -> female with hasvalue -> escaped slave, witness (Subject: Jane Johnson (slave))\n",
            "Comparing: hassex -> female with hasvalue -> domestic slave (Subject: Jane Johnson (slave))\n",
            "Comparing: hassex -> female with hasinteragentrelationshiptype -> enslaver (Subject: Jane Johnson (slave))\n",
            "Comparing: hassex -> female with isrelationshipto -> john hill wheeler (Subject: Jane Johnson (slave))\n",
            "Comparing: hassex -> female with isrelationshipfrom -> jane johnson (Subject: Jane Johnson (slave))\n",
            "Comparing: hassex -> female with hasparticipantroletype -> defendant (Subject: Jane Johnson (slave))\n",
            "Comparing: hassex -> female with roleprovidedby -> trial of william still and five dockworkers (Subject: Jane Johnson (slave))\n",
            "Comparing: hasparticipantrole -> child with haspreferrednamevariant -> jane johnson (Subject: Jane Johnson (slave))\n",
            "Comparing: hasparticipantrole -> child with fullnameasstring -> jane johnson (Subject: Jane Johnson (slave))\n",
            "Comparing: hasparticipantrole -> child with hassurnameasstring -> johnson (Subject: Jane Johnson (slave))\n",
            "Comparing: hasparticipantrole -> child with hasfirstnameasstring -> jane (Subject: Jane Johnson (slave))\n",
            "Comparing: hasparticipantrole -> child with hasdescription -> born into slavery, jane johnson escaped and testified in a trial that led to the acquittal of those who aided her escape. she later settled in boston and married again. (Subject: Jane Johnson (slave))\n",
            "Comparing: hasparticipantrole -> child with referstoplaceoforigin -> washington, d.c. (Subject: Jane Johnson (slave))\n",
            "Comparing: hasparticipantrole -> child with has_race -> african-american (Subject: Jane Johnson (slave))\n",
            "Comparing: hasparticipantrole -> child with hasdeathdate -> 1872 (Subject: Jane Johnson (slave))\n",
            "Comparing: hasparticipantrole -> child with hassex -> female (Subject: Jane Johnson (slave))\n",
            "Comparing: hasparticipantrole -> child with hasstatusgeneratedevent -> escape from slavery, testimony in trial, settlement in boston (Subject: Jane Johnson (slave))\n",
            "Comparing: hasparticipantrole -> child with hasvalue -> escaped slave, witness (Subject: Jane Johnson (slave))\n",
            "Comparing: hasparticipantrole -> child with hasvalue -> domestic slave (Subject: Jane Johnson (slave))\n",
            "Comparing: hasparticipantrole -> child with hasinteragentrelationshiptype -> enslaver (Subject: Jane Johnson (slave))\n",
            "Comparing: hasparticipantrole -> child with isrelationshipto -> john hill wheeler (Subject: Jane Johnson (slave))\n",
            "Comparing: hasparticipantrole -> child with isrelationshipfrom -> jane johnson (Subject: Jane Johnson (slave))\n",
            "Comparing: hasparticipantrole -> child with hasparticipantroletype -> defendant (Subject: Jane Johnson (slave))\n",
            "Comparing: hasparticipantrole -> child with roleprovidedby -> trial of william still and five dockworkers (Subject: Jane Johnson (slave))\n",
            "No match found for truth row: Jane Johnson, Predicate: hasparticipantrole, Object: child\n",
            "Comparing: hasparticipantrole -> deceased person with haspreferrednamevariant -> jane johnson (Subject: Jane Johnson (slave))\n",
            "Comparing: hasparticipantrole -> deceased person with fullnameasstring -> jane johnson (Subject: Jane Johnson (slave))\n",
            "Comparing: hasparticipantrole -> deceased person with hassurnameasstring -> johnson (Subject: Jane Johnson (slave))\n",
            "Comparing: hasparticipantrole -> deceased person with hasfirstnameasstring -> jane (Subject: Jane Johnson (slave))\n",
            "Comparing: hasparticipantrole -> deceased person with hasdescription -> born into slavery, jane johnson escaped and testified in a trial that led to the acquittal of those who aided her escape. she later settled in boston and married again. (Subject: Jane Johnson (slave))\n",
            "Comparing: hasparticipantrole -> deceased person with referstoplaceoforigin -> washington, d.c. (Subject: Jane Johnson (slave))\n",
            "Comparing: hasparticipantrole -> deceased person with has_race -> african-american (Subject: Jane Johnson (slave))\n",
            "Comparing: hasparticipantrole -> deceased person with hasdeathdate -> 1872 (Subject: Jane Johnson (slave))\n",
            "Comparing: hasparticipantrole -> deceased person with hassex -> female (Subject: Jane Johnson (slave))\n",
            "Comparing: hasparticipantrole -> deceased person with hasstatusgeneratedevent -> escape from slavery, testimony in trial, settlement in boston (Subject: Jane Johnson (slave))\n",
            "Comparing: hasparticipantrole -> deceased person with hasvalue -> escaped slave, witness (Subject: Jane Johnson (slave))\n",
            "Comparing: hasparticipantrole -> deceased person with hasvalue -> domestic slave (Subject: Jane Johnson (slave))\n",
            "Comparing: hasparticipantrole -> deceased person with hasinteragentrelationshiptype -> enslaver (Subject: Jane Johnson (slave))\n",
            "Comparing: hasparticipantrole -> deceased person with isrelationshipto -> john hill wheeler (Subject: Jane Johnson (slave))\n",
            "Comparing: hasparticipantrole -> deceased person with isrelationshipfrom -> jane johnson (Subject: Jane Johnson (slave))\n",
            "Comparing: hasparticipantrole -> deceased person with hasparticipantroletype -> defendant (Subject: Jane Johnson (slave))\n",
            "Comparing: hasparticipantrole -> deceased person with roleprovidedby -> trial of william still and five dockworkers (Subject: Jane Johnson (slave))\n",
            "No match found for truth row: Jane Johnson, Predicate: hasparticipantrole, Object: deceased person\n",
            "Comparing: hasdescriptiveoccupation -> slave with haspreferrednamevariant -> jane johnson (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescriptiveoccupation -> slave with fullnameasstring -> jane johnson (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescriptiveoccupation -> slave with hassurnameasstring -> johnson (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasfirstnameasstring -> jane (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasdescription -> born into slavery, jane johnson escaped and testified in a trial that led to the acquittal of those who aided her escape. she later settled in boston and married again. (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescriptiveoccupation -> slave with referstoplaceoforigin -> washington, d.c. (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescriptiveoccupation -> slave with has_race -> african-american (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasdeathdate -> 1872 (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescriptiveoccupation -> slave with hassex -> female (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasstatusgeneratedevent -> escape from slavery, testimony in trial, settlement in boston (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasvalue -> escaped slave, witness (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasvalue -> domestic slave (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasinteragentrelationshiptype -> enslaver (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescriptiveoccupation -> slave with isrelationshipto -> john hill wheeler (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescriptiveoccupation -> slave with isrelationshipfrom -> jane johnson (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasparticipantroletype -> defendant (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescriptiveoccupation -> slave with roleprovidedby -> trial of william still and five dockworkers (Subject: Jane Johnson (slave))\n",
            "No match found for truth row: Jane Johnson, Predicate: hasdescriptiveoccupation, Object: slave\n",
            "Comparing: hasdescriptiveoccupation -> fugitive slave with haspreferrednamevariant -> jane johnson (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescriptiveoccupation -> fugitive slave with fullnameasstring -> jane johnson (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescriptiveoccupation -> fugitive slave with hassurnameasstring -> johnson (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescriptiveoccupation -> fugitive slave with hasfirstnameasstring -> jane (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescriptiveoccupation -> fugitive slave with hasdescription -> born into slavery, jane johnson escaped and testified in a trial that led to the acquittal of those who aided her escape. she later settled in boston and married again. (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescriptiveoccupation -> fugitive slave with referstoplaceoforigin -> washington, d.c. (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescriptiveoccupation -> fugitive slave with has_race -> african-american (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescriptiveoccupation -> fugitive slave with hasdeathdate -> 1872 (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescriptiveoccupation -> fugitive slave with hassex -> female (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescriptiveoccupation -> fugitive slave with hasstatusgeneratedevent -> escape from slavery, testimony in trial, settlement in boston (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescriptiveoccupation -> fugitive slave with hasvalue -> escaped slave, witness (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescriptiveoccupation -> fugitive slave with hasvalue -> domestic slave (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescriptiveoccupation -> fugitive slave with hasinteragentrelationshiptype -> enslaver (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescriptiveoccupation -> fugitive slave with isrelationshipto -> john hill wheeler (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescriptiveoccupation -> fugitive slave with isrelationshipfrom -> jane johnson (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescriptiveoccupation -> fugitive slave with hasparticipantroletype -> defendant (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescriptiveoccupation -> fugitive slave with roleprovidedby -> trial of william still and five dockworkers (Subject: Jane Johnson (slave))\n",
            "No match found for truth row: Jane Johnson, Predicate: hasdescriptiveoccupation, Object: fugitive slave\n",
            "Comparing: hasdescriptiveoccupation -> underground railroad conductor with haspreferrednamevariant -> jane johnson (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescriptiveoccupation -> underground railroad conductor with fullnameasstring -> jane johnson (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescriptiveoccupation -> underground railroad conductor with hassurnameasstring -> johnson (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescriptiveoccupation -> underground railroad conductor with hasfirstnameasstring -> jane (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescriptiveoccupation -> underground railroad conductor with hasdescription -> born into slavery, jane johnson escaped and testified in a trial that led to the acquittal of those who aided her escape. she later settled in boston and married again. (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescriptiveoccupation -> underground railroad conductor with referstoplaceoforigin -> washington, d.c. (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescriptiveoccupation -> underground railroad conductor with has_race -> african-american (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescriptiveoccupation -> underground railroad conductor with hasdeathdate -> 1872 (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescriptiveoccupation -> underground railroad conductor with hassex -> female (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescriptiveoccupation -> underground railroad conductor with hasstatusgeneratedevent -> escape from slavery, testimony in trial, settlement in boston (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescriptiveoccupation -> underground railroad conductor with hasvalue -> escaped slave, witness (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescriptiveoccupation -> underground railroad conductor with hasvalue -> domestic slave (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescriptiveoccupation -> underground railroad conductor with hasinteragentrelationshiptype -> enslaver (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescriptiveoccupation -> underground railroad conductor with isrelationshipto -> john hill wheeler (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescriptiveoccupation -> underground railroad conductor with isrelationshipfrom -> jane johnson (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescriptiveoccupation -> underground railroad conductor with hasparticipantroletype -> defendant (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescriptiveoccupation -> underground railroad conductor with roleprovidedby -> trial of william still and five dockworkers (Subject: Jane Johnson (slave))\n",
            "No match found for truth row: Jane Johnson, Predicate: hasdescriptiveoccupation, Object: underground railroad conductor\n",
            "Comparing: hasfirstname -> jane with haspreferrednamevariant -> jane johnson (Subject: Jane Johnson (slave))\n",
            "Comparing: hasfirstname -> jane with fullnameasstring -> jane johnson (Subject: Jane Johnson (slave))\n",
            "Comparing: hasfirstname -> jane with hassurnameasstring -> johnson (Subject: Jane Johnson (slave))\n",
            "Comparing: hasfirstname -> jane with hasfirstnameasstring -> jane (Subject: Jane Johnson (slave))\n",
            "Comparing: hasfirstname -> jane with hasdescription -> born into slavery, jane johnson escaped and testified in a trial that led to the acquittal of those who aided her escape. she later settled in boston and married again. (Subject: Jane Johnson (slave))\n",
            "Comparing: hasfirstname -> jane with referstoplaceoforigin -> washington, d.c. (Subject: Jane Johnson (slave))\n",
            "Comparing: hasfirstname -> jane with has_race -> african-american (Subject: Jane Johnson (slave))\n",
            "Comparing: hasfirstname -> jane with hasdeathdate -> 1872 (Subject: Jane Johnson (slave))\n",
            "Comparing: hasfirstname -> jane with hassex -> female (Subject: Jane Johnson (slave))\n",
            "Comparing: hasfirstname -> jane with hasstatusgeneratedevent -> escape from slavery, testimony in trial, settlement in boston (Subject: Jane Johnson (slave))\n",
            "Comparing: hasfirstname -> jane with hasvalue -> escaped slave, witness (Subject: Jane Johnson (slave))\n",
            "Comparing: hasfirstname -> jane with hasvalue -> domestic slave (Subject: Jane Johnson (slave))\n",
            "Comparing: hasfirstname -> jane with hasinteragentrelationshiptype -> enslaver (Subject: Jane Johnson (slave))\n",
            "Comparing: hasfirstname -> jane with isrelationshipto -> john hill wheeler (Subject: Jane Johnson (slave))\n",
            "Comparing: hasfirstname -> jane with isrelationshipfrom -> jane johnson (Subject: Jane Johnson (slave))\n",
            "Comparing: hasfirstname -> jane with hasparticipantroletype -> defendant (Subject: Jane Johnson (slave))\n",
            "Comparing: hasfirstname -> jane with roleprovidedby -> trial of william still and five dockworkers (Subject: Jane Johnson (slave))\n",
            "No match found for truth row: Jane Johnson, Predicate: hasfirstname, Object: jane\n",
            "Comparing: hassurname -> johnson with haspreferrednamevariant -> jane johnson (Subject: Jane Johnson (slave))\n",
            "Comparing: hassurname -> johnson with fullnameasstring -> jane johnson (Subject: Jane Johnson (slave))\n",
            "Comparing: hassurname -> johnson with hassurnameasstring -> johnson (Subject: Jane Johnson (slave))\n",
            "Comparing: hassurname -> johnson with hasfirstnameasstring -> jane (Subject: Jane Johnson (slave))\n",
            "Comparing: hassurname -> johnson with hasdescription -> born into slavery, jane johnson escaped and testified in a trial that led to the acquittal of those who aided her escape. she later settled in boston and married again. (Subject: Jane Johnson (slave))\n",
            "Comparing: hassurname -> johnson with referstoplaceoforigin -> washington, d.c. (Subject: Jane Johnson (slave))\n",
            "Comparing: hassurname -> johnson with has_race -> african-american (Subject: Jane Johnson (slave))\n",
            "Comparing: hassurname -> johnson with hasdeathdate -> 1872 (Subject: Jane Johnson (slave))\n",
            "Comparing: hassurname -> johnson with hassex -> female (Subject: Jane Johnson (slave))\n",
            "Comparing: hassurname -> johnson with hasstatusgeneratedevent -> escape from slavery, testimony in trial, settlement in boston (Subject: Jane Johnson (slave))\n",
            "Comparing: hassurname -> johnson with hasvalue -> escaped slave, witness (Subject: Jane Johnson (slave))\n",
            "Comparing: hassurname -> johnson with hasvalue -> domestic slave (Subject: Jane Johnson (slave))\n",
            "Comparing: hassurname -> johnson with hasinteragentrelationshiptype -> enslaver (Subject: Jane Johnson (slave))\n",
            "Comparing: hassurname -> johnson with isrelationshipto -> john hill wheeler (Subject: Jane Johnson (slave))\n",
            "Comparing: hassurname -> johnson with isrelationshipfrom -> jane johnson (Subject: Jane Johnson (slave))\n",
            "Comparing: hassurname -> johnson with hasparticipantroletype -> defendant (Subject: Jane Johnson (slave))\n",
            "Comparing: hassurname -> johnson with roleprovidedby -> trial of william still and five dockworkers (Subject: Jane Johnson (slave))\n",
            "No match found for truth row: Jane Johnson, Predicate: hassurname, Object: johnson\n",
            "Comparing: hasname -> jane johnson with haspreferrednamevariant -> jane johnson (Subject: Jane Johnson (slave))\n",
            "Comparing: hasname -> jane johnson with fullnameasstring -> jane johnson (Subject: Jane Johnson (slave))\n",
            "Comparing: hasname -> jane johnson with hassurnameasstring -> johnson (Subject: Jane Johnson (slave))\n",
            "Comparing: hasname -> jane johnson with hasfirstnameasstring -> jane (Subject: Jane Johnson (slave))\n",
            "Comparing: hasname -> jane johnson with hasdescription -> born into slavery, jane johnson escaped and testified in a trial that led to the acquittal of those who aided her escape. she later settled in boston and married again. (Subject: Jane Johnson (slave))\n",
            "Comparing: hasname -> jane johnson with referstoplaceoforigin -> washington, d.c. (Subject: Jane Johnson (slave))\n",
            "Comparing: hasname -> jane johnson with has_race -> african-american (Subject: Jane Johnson (slave))\n",
            "Comparing: hasname -> jane johnson with hasdeathdate -> 1872 (Subject: Jane Johnson (slave))\n",
            "Comparing: hasname -> jane johnson with hassex -> female (Subject: Jane Johnson (slave))\n",
            "Comparing: hasname -> jane johnson with hasstatusgeneratedevent -> escape from slavery, testimony in trial, settlement in boston (Subject: Jane Johnson (slave))\n",
            "Comparing: hasname -> jane johnson with hasvalue -> escaped slave, witness (Subject: Jane Johnson (slave))\n",
            "Comparing: hasname -> jane johnson with hasvalue -> domestic slave (Subject: Jane Johnson (slave))\n",
            "Comparing: hasname -> jane johnson with hasinteragentrelationshiptype -> enslaver (Subject: Jane Johnson (slave))\n",
            "Comparing: hasname -> jane johnson with isrelationshipto -> john hill wheeler (Subject: Jane Johnson (slave))\n",
            "Comparing: hasname -> jane johnson with isrelationshipfrom -> jane johnson (Subject: Jane Johnson (slave))\n",
            "Comparing: hasname -> jane johnson with hasparticipantroletype -> defendant (Subject: Jane Johnson (slave))\n",
            "Comparing: hasname -> jane johnson with roleprovidedby -> trial of william still and five dockworkers (Subject: Jane Johnson (slave))\n",
            "No match found for truth row: Jane Johnson, Predicate: hasname, Object: jane johnson\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. she may have been the escaped woman, also named jane, referenced in hannah crafts' the bondwoman's narrative. courageously testified in court in 1855 that she had escaped of her own will. with haspreferrednamevariant -> jane johnson (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. she may have been the escaped woman, also named jane, referenced in hannah crafts' the bondwoman's narrative. courageously testified in court in 1855 that she had escaped of her own will. with fullnameasstring -> jane johnson (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. she may have been the escaped woman, also named jane, referenced in hannah crafts' the bondwoman's narrative. courageously testified in court in 1855 that she had escaped of her own will. with hassurnameasstring -> johnson (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. she may have been the escaped woman, also named jane, referenced in hannah crafts' the bondwoman's narrative. courageously testified in court in 1855 that she had escaped of her own will. with hasfirstnameasstring -> jane (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. she may have been the escaped woman, also named jane, referenced in hannah crafts' the bondwoman's narrative. courageously testified in court in 1855 that she had escaped of her own will. with hasdescription -> born into slavery, jane johnson escaped and testified in a trial that led to the acquittal of those who aided her escape. she later settled in boston and married again. (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. she may have been the escaped woman, also named jane, referenced in hannah crafts' the bondwoman's narrative. courageously testified in court in 1855 that she had escaped of her own will. with referstoplaceoforigin -> washington, d.c. (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. she may have been the escaped woman, also named jane, referenced in hannah crafts' the bondwoman's narrative. courageously testified in court in 1855 that she had escaped of her own will. with has_race -> african-american (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. she may have been the escaped woman, also named jane, referenced in hannah crafts' the bondwoman's narrative. courageously testified in court in 1855 that she had escaped of her own will. with hasdeathdate -> 1872 (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. she may have been the escaped woman, also named jane, referenced in hannah crafts' the bondwoman's narrative. courageously testified in court in 1855 that she had escaped of her own will. with hassex -> female (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. she may have been the escaped woman, also named jane, referenced in hannah crafts' the bondwoman's narrative. courageously testified in court in 1855 that she had escaped of her own will. with hasstatusgeneratedevent -> escape from slavery, testimony in trial, settlement in boston (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. she may have been the escaped woman, also named jane, referenced in hannah crafts' the bondwoman's narrative. courageously testified in court in 1855 that she had escaped of her own will. with hasvalue -> escaped slave, witness (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. she may have been the escaped woman, also named jane, referenced in hannah crafts' the bondwoman's narrative. courageously testified in court in 1855 that she had escaped of her own will. with hasvalue -> domestic slave (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. she may have been the escaped woman, also named jane, referenced in hannah crafts' the bondwoman's narrative. courageously testified in court in 1855 that she had escaped of her own will. with hasinteragentrelationshiptype -> enslaver (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. she may have been the escaped woman, also named jane, referenced in hannah crafts' the bondwoman's narrative. courageously testified in court in 1855 that she had escaped of her own will. with isrelationshipto -> john hill wheeler (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. she may have been the escaped woman, also named jane, referenced in hannah crafts' the bondwoman's narrative. courageously testified in court in 1855 that she had escaped of her own will. with isrelationshipfrom -> jane johnson (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. she may have been the escaped woman, also named jane, referenced in hannah crafts' the bondwoman's narrative. courageously testified in court in 1855 that she had escaped of her own will. with hasparticipantroletype -> defendant (Subject: Jane Johnson (slave))\n",
            "Comparing: hasdescription -> enslaved. free before 13th amendment. she may have been the escaped woman, also named jane, referenced in hannah crafts' the bondwoman's narrative. courageously testified in court in 1855 that she had escaped of her own will. with roleprovidedby -> trial of william still and five dockworkers (Subject: Jane Johnson (slave))\n",
            "No match found for truth row: Jane Johnson, Predicate: hasdescription, Object: enslaved. free before 13th amendment. she may have been the escaped woman, also named jane, referenced in hannah crafts' the bondwoman's narrative. courageously testified in court in 1855 that she had escaped of her own will.\n",
            "\n",
            "Processing truth file: Powhatan_Beaty.tsv\n",
            "Truth DataFrame:\n",
            "                 0                         1  \\\n",
            "0   Powhatan Beaty               instance of   \n",
            "1   Powhatan Beaty                    hasSex   \n",
            "2   Powhatan Beaty        hasParticipantRole   \n",
            "3   Powhatan Beaty        hasParticipantRole   \n",
            "4   Powhatan Beaty  hasDescriptiveOccupation   \n",
            "5   Powhatan Beaty  hasDescriptiveOccupation   \n",
            "6   Powhatan Beaty  hasDescriptiveOccupation   \n",
            "7   Powhatan Beaty  hasDescriptiveOccupation   \n",
            "8   Powhatan Beaty  hasDescriptiveOccupation   \n",
            "9   Powhatan Beaty              hasFirstName   \n",
            "10  Powhatan Beaty                hasSurname   \n",
            "11  Powhatan Beaty                   hasName   \n",
            "12  Powhatan Beaty            hasDescription   \n",
            "\n",
            "                                                    2  \n",
            "0                                              Person  \n",
            "1                                                Male  \n",
            "2                                               Child  \n",
            "3                                     Deceased Person  \n",
            "4                                               Slave  \n",
            "5                            Medal of Honor Recipient  \n",
            "6                                             Soldier  \n",
            "7                                Stage / Screen Actor  \n",
            "8                                           Dramatist  \n",
            "9                                            Powhatan  \n",
            "10                                              Beaty  \n",
            "11                                     Powhatan Beaty  \n",
            "12  Born Enslaved. Free before 13th Amendment. Aft...  \n",
            "\n",
            "Found matching generated file: Powhatan_Beaty_triples.tsv\n",
            "Generated DataFrame before transformation:\n",
            "                   Subject                      Predicate  \\\n",
            "1   Powhatan_Beaty_triples        hasPreferredNameVariant   \n",
            "2   Powhatan_Beaty_triples               fullNameAsString   \n",
            "3   Powhatan_Beaty_triples             hasSurnameAsString   \n",
            "4   Powhatan_Beaty_triples           hasFirstnameAsString   \n",
            "5   Powhatan_Beaty_triples                 hasDescription   \n",
            "6   Powhatan_Beaty_triples          refersToPlaceOfOrigin   \n",
            "7   Powhatan_Beaty_triples                       has_Race   \n",
            "8   Powhatan_Beaty_triples                    hasAgeValue   \n",
            "9   Powhatan_Beaty_triples                   hasBirthDate   \n",
            "10  Powhatan_Beaty_triples                   hasDeathDate   \n",
            "11  Powhatan_Beaty_triples                         hasSex   \n",
            "12  Powhatan_Beaty_triples        hasStatusGeneratedEvent   \n",
            "13  Powhatan_Beaty_triples                       hasValue   \n",
            "14  Powhatan_Beaty_triples                       hasValue   \n",
            "15  Powhatan_Beaty_triples  hasInterAgentRelationshipType   \n",
            "16  Powhatan_Beaty_triples               isRelationshipTo   \n",
            "17  Powhatan_Beaty_triples             isRelationshipFrom   \n",
            "18  Powhatan_Beaty_triples         hasParticipantRoleType   \n",
            "19  Powhatan_Beaty_triples                 roleProvidedBy   \n",
            "\n",
            "                                               Object  \n",
            "1                                  Sgt Powhatan Beaty  \n",
            "2                                  Sgt Powhatan Beaty  \n",
            "3                                               Beaty  \n",
            "4                                            Powhatan  \n",
            "5   Sgt Powhatan Beaty was a soldier in the Union ...  \n",
            "6                                          Cincinnati  \n",
            "7                                    African-American  \n",
            "8                                                 NaN  \n",
            "9                           Not mentioned in the text  \n",
            "10                                   December 6, 1916  \n",
            "11                                               Male  \n",
            "12  Enlistment in Union Army, Medal of Honor citation  \n",
            "13                  Soldier, Medal of Honor recipient  \n",
            "14                            Soldier, Drama Director  \n",
            "15                          Not mentioned in the text  \n",
            "16                          Not mentioned in the text  \n",
            "17                          Not mentioned in the text  \n",
            "18                            Soldier, Drama Director  \n",
            "19  American Civil War, Formation of Literary and ...  \n",
            "Generated DataFrame after transformation:\n",
            "           Subject                      Predicate  \\\n",
            "1   Powhatan Beaty        haspreferrednamevariant   \n",
            "2   Powhatan Beaty               fullnameasstring   \n",
            "3   Powhatan Beaty             hassurnameasstring   \n",
            "4   Powhatan Beaty           hasfirstnameasstring   \n",
            "5   Powhatan Beaty                 hasdescription   \n",
            "6   Powhatan Beaty          referstoplaceoforigin   \n",
            "7   Powhatan Beaty                       has_race   \n",
            "8   Powhatan Beaty                    hasagevalue   \n",
            "9   Powhatan Beaty                   hasbirthdate   \n",
            "10  Powhatan Beaty                   hasdeathdate   \n",
            "11  Powhatan Beaty                         hassex   \n",
            "12  Powhatan Beaty        hasstatusgeneratedevent   \n",
            "13  Powhatan Beaty                       hasvalue   \n",
            "14  Powhatan Beaty                       hasvalue   \n",
            "15  Powhatan Beaty  hasinteragentrelationshiptype   \n",
            "16  Powhatan Beaty               isrelationshipto   \n",
            "17  Powhatan Beaty             isrelationshipfrom   \n",
            "18  Powhatan Beaty         hasparticipantroletype   \n",
            "19  Powhatan Beaty                 roleprovidedby   \n",
            "\n",
            "                                               Object  \n",
            "1                                  sgt powhatan beaty  \n",
            "2                                  sgt powhatan beaty  \n",
            "3                                               beaty  \n",
            "4                                            powhatan  \n",
            "5   sgt powhatan beaty was a soldier in the union ...  \n",
            "6                                          cincinnati  \n",
            "7                                    african-american  \n",
            "8                                                 NaN  \n",
            "9                           not mentioned in the text  \n",
            "10                                   december 6, 1916  \n",
            "11                                               male  \n",
            "12  enlistment in union army, medal of honor citation  \n",
            "13                  soldier, medal of honor recipient  \n",
            "14                            soldier, drama director  \n",
            "15                          not mentioned in the text  \n",
            "16                          not mentioned in the text  \n",
            "17                          not mentioned in the text  \n",
            "18                            soldier, drama director  \n",
            "19  american civil war, formation of literary and ...  \n",
            "Comparing: instance of -> person with haspreferrednamevariant -> sgt powhatan beaty (Subject: Powhatan Beaty)\n",
            "Comparing: instance of -> person with fullnameasstring -> sgt powhatan beaty (Subject: Powhatan Beaty)\n",
            "Comparing: instance of -> person with hassurnameasstring -> beaty (Subject: Powhatan Beaty)\n",
            "Comparing: instance of -> person with hasfirstnameasstring -> powhatan (Subject: Powhatan Beaty)\n",
            "Comparing: instance of -> person with hasdescription -> sgt powhatan beaty was a soldier in the union army during the american civil war. he received the medal of honor for gallantry during the battle of chaffin's farm. (Subject: Powhatan Beaty)\n",
            "Comparing: instance of -> person with referstoplaceoforigin -> cincinnati (Subject: Powhatan Beaty)\n",
            "Comparing: instance of -> person with has_race -> african-american (Subject: Powhatan Beaty)\n",
            "Comparing: instance of -> person with hasagevalue -> nan (Subject: Powhatan Beaty)\n",
            "Comparing: instance of -> person with hasbirthdate -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: instance of -> person with hasdeathdate -> december 6, 1916 (Subject: Powhatan Beaty)\n",
            "Comparing: instance of -> person with hassex -> male (Subject: Powhatan Beaty)\n",
            "Comparing: instance of -> person with hasstatusgeneratedevent -> enlistment in union army, medal of honor citation (Subject: Powhatan Beaty)\n",
            "Comparing: instance of -> person with hasvalue -> soldier, medal of honor recipient (Subject: Powhatan Beaty)\n",
            "Comparing: instance of -> person with hasvalue -> soldier, drama director (Subject: Powhatan Beaty)\n",
            "Comparing: instance of -> person with hasinteragentrelationshiptype -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: instance of -> person with isrelationshipto -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: instance of -> person with isrelationshipfrom -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: instance of -> person with hasparticipantroletype -> soldier, drama director (Subject: Powhatan Beaty)\n",
            "Comparing: instance of -> person with roleprovidedby -> american civil war, formation of literary and dramatic club (Subject: Powhatan Beaty)\n",
            "No match found for truth row: Powhatan Beaty, Predicate: instance of, Object: person\n",
            "Comparing: hassex -> male with haspreferrednamevariant -> sgt powhatan beaty (Subject: Powhatan Beaty)\n",
            "Comparing: hassex -> male with fullnameasstring -> sgt powhatan beaty (Subject: Powhatan Beaty)\n",
            "Comparing: hassex -> male with hassurnameasstring -> beaty (Subject: Powhatan Beaty)\n",
            "Comparing: hassex -> male with hasfirstnameasstring -> powhatan (Subject: Powhatan Beaty)\n",
            "Comparing: hassex -> male with hasdescription -> sgt powhatan beaty was a soldier in the union army during the american civil war. he received the medal of honor for gallantry during the battle of chaffin's farm. (Subject: Powhatan Beaty)\n",
            "Comparing: hassex -> male with referstoplaceoforigin -> cincinnati (Subject: Powhatan Beaty)\n",
            "Comparing: hassex -> male with has_race -> african-american (Subject: Powhatan Beaty)\n",
            "Comparing: hassex -> male with hasagevalue -> nan (Subject: Powhatan Beaty)\n",
            "Comparing: hassex -> male with hasbirthdate -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: hassex -> male with hasdeathdate -> december 6, 1916 (Subject: Powhatan Beaty)\n",
            "Comparing: hassex -> male with hassex -> male (Subject: Powhatan Beaty)\n",
            "Match found for truth row Powhatan Beaty with generated row Powhatan Beaty\n",
            "Comparing: hassex -> male with hasstatusgeneratedevent -> enlistment in union army, medal of honor citation (Subject: Powhatan Beaty)\n",
            "Comparing: hassex -> male with hasvalue -> soldier, medal of honor recipient (Subject: Powhatan Beaty)\n",
            "Comparing: hassex -> male with hasvalue -> soldier, drama director (Subject: Powhatan Beaty)\n",
            "Comparing: hassex -> male with hasinteragentrelationshiptype -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: hassex -> male with isrelationshipto -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: hassex -> male with isrelationshipfrom -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: hassex -> male with hasparticipantroletype -> soldier, drama director (Subject: Powhatan Beaty)\n",
            "Comparing: hassex -> male with roleprovidedby -> american civil war, formation of literary and dramatic club (Subject: Powhatan Beaty)\n",
            "Comparing: hasparticipantrole -> child with haspreferrednamevariant -> sgt powhatan beaty (Subject: Powhatan Beaty)\n",
            "Comparing: hasparticipantrole -> child with fullnameasstring -> sgt powhatan beaty (Subject: Powhatan Beaty)\n",
            "Comparing: hasparticipantrole -> child with hassurnameasstring -> beaty (Subject: Powhatan Beaty)\n",
            "Comparing: hasparticipantrole -> child with hasfirstnameasstring -> powhatan (Subject: Powhatan Beaty)\n",
            "Comparing: hasparticipantrole -> child with hasdescription -> sgt powhatan beaty was a soldier in the union army during the american civil war. he received the medal of honor for gallantry during the battle of chaffin's farm. (Subject: Powhatan Beaty)\n",
            "Comparing: hasparticipantrole -> child with referstoplaceoforigin -> cincinnati (Subject: Powhatan Beaty)\n",
            "Comparing: hasparticipantrole -> child with has_race -> african-american (Subject: Powhatan Beaty)\n",
            "Comparing: hasparticipantrole -> child with hasagevalue -> nan (Subject: Powhatan Beaty)\n",
            "Comparing: hasparticipantrole -> child with hasbirthdate -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: hasparticipantrole -> child with hasdeathdate -> december 6, 1916 (Subject: Powhatan Beaty)\n",
            "Comparing: hasparticipantrole -> child with hassex -> male (Subject: Powhatan Beaty)\n",
            "Comparing: hasparticipantrole -> child with hasstatusgeneratedevent -> enlistment in union army, medal of honor citation (Subject: Powhatan Beaty)\n",
            "Comparing: hasparticipantrole -> child with hasvalue -> soldier, medal of honor recipient (Subject: Powhatan Beaty)\n",
            "Comparing: hasparticipantrole -> child with hasvalue -> soldier, drama director (Subject: Powhatan Beaty)\n",
            "Comparing: hasparticipantrole -> child with hasinteragentrelationshiptype -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: hasparticipantrole -> child with isrelationshipto -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: hasparticipantrole -> child with isrelationshipfrom -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: hasparticipantrole -> child with hasparticipantroletype -> soldier, drama director (Subject: Powhatan Beaty)\n",
            "Comparing: hasparticipantrole -> child with roleprovidedby -> american civil war, formation of literary and dramatic club (Subject: Powhatan Beaty)\n",
            "No match found for truth row: Powhatan Beaty, Predicate: hasparticipantrole, Object: child\n",
            "Comparing: hasparticipantrole -> deceased person with haspreferrednamevariant -> sgt powhatan beaty (Subject: Powhatan Beaty)\n",
            "Comparing: hasparticipantrole -> deceased person with fullnameasstring -> sgt powhatan beaty (Subject: Powhatan Beaty)\n",
            "Comparing: hasparticipantrole -> deceased person with hassurnameasstring -> beaty (Subject: Powhatan Beaty)\n",
            "Comparing: hasparticipantrole -> deceased person with hasfirstnameasstring -> powhatan (Subject: Powhatan Beaty)\n",
            "Comparing: hasparticipantrole -> deceased person with hasdescription -> sgt powhatan beaty was a soldier in the union army during the american civil war. he received the medal of honor for gallantry during the battle of chaffin's farm. (Subject: Powhatan Beaty)\n",
            "Comparing: hasparticipantrole -> deceased person with referstoplaceoforigin -> cincinnati (Subject: Powhatan Beaty)\n",
            "Comparing: hasparticipantrole -> deceased person with has_race -> african-american (Subject: Powhatan Beaty)\n",
            "Comparing: hasparticipantrole -> deceased person with hasagevalue -> nan (Subject: Powhatan Beaty)\n",
            "Comparing: hasparticipantrole -> deceased person with hasbirthdate -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: hasparticipantrole -> deceased person with hasdeathdate -> december 6, 1916 (Subject: Powhatan Beaty)\n",
            "Comparing: hasparticipantrole -> deceased person with hassex -> male (Subject: Powhatan Beaty)\n",
            "Comparing: hasparticipantrole -> deceased person with hasstatusgeneratedevent -> enlistment in union army, medal of honor citation (Subject: Powhatan Beaty)\n",
            "Comparing: hasparticipantrole -> deceased person with hasvalue -> soldier, medal of honor recipient (Subject: Powhatan Beaty)\n",
            "Comparing: hasparticipantrole -> deceased person with hasvalue -> soldier, drama director (Subject: Powhatan Beaty)\n",
            "Comparing: hasparticipantrole -> deceased person with hasinteragentrelationshiptype -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: hasparticipantrole -> deceased person with isrelationshipto -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: hasparticipantrole -> deceased person with isrelationshipfrom -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: hasparticipantrole -> deceased person with hasparticipantroletype -> soldier, drama director (Subject: Powhatan Beaty)\n",
            "Comparing: hasparticipantrole -> deceased person with roleprovidedby -> american civil war, formation of literary and dramatic club (Subject: Powhatan Beaty)\n",
            "No match found for truth row: Powhatan Beaty, Predicate: hasparticipantrole, Object: deceased person\n",
            "Comparing: hasdescriptiveoccupation -> slave with haspreferrednamevariant -> sgt powhatan beaty (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> slave with fullnameasstring -> sgt powhatan beaty (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hassurnameasstring -> beaty (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasfirstnameasstring -> powhatan (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasdescription -> sgt powhatan beaty was a soldier in the union army during the american civil war. he received the medal of honor for gallantry during the battle of chaffin's farm. (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> slave with referstoplaceoforigin -> cincinnati (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> slave with has_race -> african-american (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasagevalue -> nan (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasbirthdate -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasdeathdate -> december 6, 1916 (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hassex -> male (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasstatusgeneratedevent -> enlistment in union army, medal of honor citation (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasvalue -> soldier, medal of honor recipient (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasvalue -> soldier, drama director (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasinteragentrelationshiptype -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> slave with isrelationshipto -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> slave with isrelationshipfrom -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> slave with hasparticipantroletype -> soldier, drama director (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> slave with roleprovidedby -> american civil war, formation of literary and dramatic club (Subject: Powhatan Beaty)\n",
            "No match found for truth row: Powhatan Beaty, Predicate: hasdescriptiveoccupation, Object: slave\n",
            "Comparing: hasdescriptiveoccupation -> medal of honor recipient with haspreferrednamevariant -> sgt powhatan beaty (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> medal of honor recipient with fullnameasstring -> sgt powhatan beaty (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> medal of honor recipient with hassurnameasstring -> beaty (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> medal of honor recipient with hasfirstnameasstring -> powhatan (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> medal of honor recipient with hasdescription -> sgt powhatan beaty was a soldier in the union army during the american civil war. he received the medal of honor for gallantry during the battle of chaffin's farm. (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> medal of honor recipient with referstoplaceoforigin -> cincinnati (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> medal of honor recipient with has_race -> african-american (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> medal of honor recipient with hasagevalue -> nan (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> medal of honor recipient with hasbirthdate -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> medal of honor recipient with hasdeathdate -> december 6, 1916 (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> medal of honor recipient with hassex -> male (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> medal of honor recipient with hasstatusgeneratedevent -> enlistment in union army, medal of honor citation (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> medal of honor recipient with hasvalue -> soldier, medal of honor recipient (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> medal of honor recipient with hasvalue -> soldier, drama director (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> medal of honor recipient with hasinteragentrelationshiptype -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> medal of honor recipient with isrelationshipto -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> medal of honor recipient with isrelationshipfrom -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> medal of honor recipient with hasparticipantroletype -> soldier, drama director (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> medal of honor recipient with roleprovidedby -> american civil war, formation of literary and dramatic club (Subject: Powhatan Beaty)\n",
            "No match found for truth row: Powhatan Beaty, Predicate: hasdescriptiveoccupation, Object: medal of honor recipient\n",
            "Comparing: hasdescriptiveoccupation -> soldier with haspreferrednamevariant -> sgt powhatan beaty (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> soldier with fullnameasstring -> sgt powhatan beaty (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> soldier with hassurnameasstring -> beaty (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> soldier with hasfirstnameasstring -> powhatan (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> soldier with hasdescription -> sgt powhatan beaty was a soldier in the union army during the american civil war. he received the medal of honor for gallantry during the battle of chaffin's farm. (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> soldier with referstoplaceoforigin -> cincinnati (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> soldier with has_race -> african-american (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> soldier with hasagevalue -> nan (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> soldier with hasbirthdate -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> soldier with hasdeathdate -> december 6, 1916 (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> soldier with hassex -> male (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> soldier with hasstatusgeneratedevent -> enlistment in union army, medal of honor citation (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> soldier with hasvalue -> soldier, medal of honor recipient (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> soldier with hasvalue -> soldier, drama director (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> soldier with hasinteragentrelationshiptype -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> soldier with isrelationshipto -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> soldier with isrelationshipfrom -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> soldier with hasparticipantroletype -> soldier, drama director (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> soldier with roleprovidedby -> american civil war, formation of literary and dramatic club (Subject: Powhatan Beaty)\n",
            "No match found for truth row: Powhatan Beaty, Predicate: hasdescriptiveoccupation, Object: soldier\n",
            "Comparing: hasdescriptiveoccupation -> stage / screen actor with haspreferrednamevariant -> sgt powhatan beaty (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> stage / screen actor with fullnameasstring -> sgt powhatan beaty (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> stage / screen actor with hassurnameasstring -> beaty (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> stage / screen actor with hasfirstnameasstring -> powhatan (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> stage / screen actor with hasdescription -> sgt powhatan beaty was a soldier in the union army during the american civil war. he received the medal of honor for gallantry during the battle of chaffin's farm. (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> stage / screen actor with referstoplaceoforigin -> cincinnati (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> stage / screen actor with has_race -> african-american (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> stage / screen actor with hasagevalue -> nan (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> stage / screen actor with hasbirthdate -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> stage / screen actor with hasdeathdate -> december 6, 1916 (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> stage / screen actor with hassex -> male (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> stage / screen actor with hasstatusgeneratedevent -> enlistment in union army, medal of honor citation (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> stage / screen actor with hasvalue -> soldier, medal of honor recipient (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> stage / screen actor with hasvalue -> soldier, drama director (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> stage / screen actor with hasinteragentrelationshiptype -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> stage / screen actor with isrelationshipto -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> stage / screen actor with isrelationshipfrom -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> stage / screen actor with hasparticipantroletype -> soldier, drama director (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> stage / screen actor with roleprovidedby -> american civil war, formation of literary and dramatic club (Subject: Powhatan Beaty)\n",
            "No match found for truth row: Powhatan Beaty, Predicate: hasdescriptiveoccupation, Object: stage / screen actor\n",
            "Comparing: hasdescriptiveoccupation -> dramatist with haspreferrednamevariant -> sgt powhatan beaty (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> dramatist with fullnameasstring -> sgt powhatan beaty (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> dramatist with hassurnameasstring -> beaty (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> dramatist with hasfirstnameasstring -> powhatan (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> dramatist with hasdescription -> sgt powhatan beaty was a soldier in the union army during the american civil war. he received the medal of honor for gallantry during the battle of chaffin's farm. (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> dramatist with referstoplaceoforigin -> cincinnati (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> dramatist with has_race -> african-american (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> dramatist with hasagevalue -> nan (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> dramatist with hasbirthdate -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> dramatist with hasdeathdate -> december 6, 1916 (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> dramatist with hassex -> male (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> dramatist with hasstatusgeneratedevent -> enlistment in union army, medal of honor citation (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> dramatist with hasvalue -> soldier, medal of honor recipient (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> dramatist with hasvalue -> soldier, drama director (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> dramatist with hasinteragentrelationshiptype -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> dramatist with isrelationshipto -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> dramatist with isrelationshipfrom -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> dramatist with hasparticipantroletype -> soldier, drama director (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescriptiveoccupation -> dramatist with roleprovidedby -> american civil war, formation of literary and dramatic club (Subject: Powhatan Beaty)\n",
            "No match found for truth row: Powhatan Beaty, Predicate: hasdescriptiveoccupation, Object: dramatist\n",
            "Comparing: hasfirstname -> powhatan with haspreferrednamevariant -> sgt powhatan beaty (Subject: Powhatan Beaty)\n",
            "Comparing: hasfirstname -> powhatan with fullnameasstring -> sgt powhatan beaty (Subject: Powhatan Beaty)\n",
            "Comparing: hasfirstname -> powhatan with hassurnameasstring -> beaty (Subject: Powhatan Beaty)\n",
            "Comparing: hasfirstname -> powhatan with hasfirstnameasstring -> powhatan (Subject: Powhatan Beaty)\n",
            "Comparing: hasfirstname -> powhatan with hasdescription -> sgt powhatan beaty was a soldier in the union army during the american civil war. he received the medal of honor for gallantry during the battle of chaffin's farm. (Subject: Powhatan Beaty)\n",
            "Comparing: hasfirstname -> powhatan with referstoplaceoforigin -> cincinnati (Subject: Powhatan Beaty)\n",
            "Comparing: hasfirstname -> powhatan with has_race -> african-american (Subject: Powhatan Beaty)\n",
            "Comparing: hasfirstname -> powhatan with hasagevalue -> nan (Subject: Powhatan Beaty)\n",
            "Comparing: hasfirstname -> powhatan with hasbirthdate -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: hasfirstname -> powhatan with hasdeathdate -> december 6, 1916 (Subject: Powhatan Beaty)\n",
            "Comparing: hasfirstname -> powhatan with hassex -> male (Subject: Powhatan Beaty)\n",
            "Comparing: hasfirstname -> powhatan with hasstatusgeneratedevent -> enlistment in union army, medal of honor citation (Subject: Powhatan Beaty)\n",
            "Comparing: hasfirstname -> powhatan with hasvalue -> soldier, medal of honor recipient (Subject: Powhatan Beaty)\n",
            "Comparing: hasfirstname -> powhatan with hasvalue -> soldier, drama director (Subject: Powhatan Beaty)\n",
            "Comparing: hasfirstname -> powhatan with hasinteragentrelationshiptype -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: hasfirstname -> powhatan with isrelationshipto -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: hasfirstname -> powhatan with isrelationshipfrom -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: hasfirstname -> powhatan with hasparticipantroletype -> soldier, drama director (Subject: Powhatan Beaty)\n",
            "Comparing: hasfirstname -> powhatan with roleprovidedby -> american civil war, formation of literary and dramatic club (Subject: Powhatan Beaty)\n",
            "No match found for truth row: Powhatan Beaty, Predicate: hasfirstname, Object: powhatan\n",
            "Comparing: hassurname -> beaty with haspreferrednamevariant -> sgt powhatan beaty (Subject: Powhatan Beaty)\n",
            "Comparing: hassurname -> beaty with fullnameasstring -> sgt powhatan beaty (Subject: Powhatan Beaty)\n",
            "Comparing: hassurname -> beaty with hassurnameasstring -> beaty (Subject: Powhatan Beaty)\n",
            "Comparing: hassurname -> beaty with hasfirstnameasstring -> powhatan (Subject: Powhatan Beaty)\n",
            "Comparing: hassurname -> beaty with hasdescription -> sgt powhatan beaty was a soldier in the union army during the american civil war. he received the medal of honor for gallantry during the battle of chaffin's farm. (Subject: Powhatan Beaty)\n",
            "Comparing: hassurname -> beaty with referstoplaceoforigin -> cincinnati (Subject: Powhatan Beaty)\n",
            "Comparing: hassurname -> beaty with has_race -> african-american (Subject: Powhatan Beaty)\n",
            "Comparing: hassurname -> beaty with hasagevalue -> nan (Subject: Powhatan Beaty)\n",
            "Comparing: hassurname -> beaty with hasbirthdate -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: hassurname -> beaty with hasdeathdate -> december 6, 1916 (Subject: Powhatan Beaty)\n",
            "Comparing: hassurname -> beaty with hassex -> male (Subject: Powhatan Beaty)\n",
            "Comparing: hassurname -> beaty with hasstatusgeneratedevent -> enlistment in union army, medal of honor citation (Subject: Powhatan Beaty)\n",
            "Comparing: hassurname -> beaty with hasvalue -> soldier, medal of honor recipient (Subject: Powhatan Beaty)\n",
            "Comparing: hassurname -> beaty with hasvalue -> soldier, drama director (Subject: Powhatan Beaty)\n",
            "Comparing: hassurname -> beaty with hasinteragentrelationshiptype -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: hassurname -> beaty with isrelationshipto -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: hassurname -> beaty with isrelationshipfrom -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: hassurname -> beaty with hasparticipantroletype -> soldier, drama director (Subject: Powhatan Beaty)\n",
            "Comparing: hassurname -> beaty with roleprovidedby -> american civil war, formation of literary and dramatic club (Subject: Powhatan Beaty)\n",
            "No match found for truth row: Powhatan Beaty, Predicate: hassurname, Object: beaty\n",
            "Comparing: hasname -> powhatan beaty with haspreferrednamevariant -> sgt powhatan beaty (Subject: Powhatan Beaty)\n",
            "Comparing: hasname -> powhatan beaty with fullnameasstring -> sgt powhatan beaty (Subject: Powhatan Beaty)\n",
            "Comparing: hasname -> powhatan beaty with hassurnameasstring -> beaty (Subject: Powhatan Beaty)\n",
            "Comparing: hasname -> powhatan beaty with hasfirstnameasstring -> powhatan (Subject: Powhatan Beaty)\n",
            "Comparing: hasname -> powhatan beaty with hasdescription -> sgt powhatan beaty was a soldier in the union army during the american civil war. he received the medal of honor for gallantry during the battle of chaffin's farm. (Subject: Powhatan Beaty)\n",
            "Comparing: hasname -> powhatan beaty with referstoplaceoforigin -> cincinnati (Subject: Powhatan Beaty)\n",
            "Comparing: hasname -> powhatan beaty with has_race -> african-american (Subject: Powhatan Beaty)\n",
            "Comparing: hasname -> powhatan beaty with hasagevalue -> nan (Subject: Powhatan Beaty)\n",
            "Comparing: hasname -> powhatan beaty with hasbirthdate -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: hasname -> powhatan beaty with hasdeathdate -> december 6, 1916 (Subject: Powhatan Beaty)\n",
            "Comparing: hasname -> powhatan beaty with hassex -> male (Subject: Powhatan Beaty)\n",
            "Comparing: hasname -> powhatan beaty with hasstatusgeneratedevent -> enlistment in union army, medal of honor citation (Subject: Powhatan Beaty)\n",
            "Comparing: hasname -> powhatan beaty with hasvalue -> soldier, medal of honor recipient (Subject: Powhatan Beaty)\n",
            "Comparing: hasname -> powhatan beaty with hasvalue -> soldier, drama director (Subject: Powhatan Beaty)\n",
            "Comparing: hasname -> powhatan beaty with hasinteragentrelationshiptype -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: hasname -> powhatan beaty with isrelationshipto -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: hasname -> powhatan beaty with isrelationshipfrom -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: hasname -> powhatan beaty with hasparticipantroletype -> soldier, drama director (Subject: Powhatan Beaty)\n",
            "Comparing: hasname -> powhatan beaty with roleprovidedby -> american civil war, formation of literary and dramatic club (Subject: Powhatan Beaty)\n",
            "No match found for truth row: Powhatan Beaty, Predicate: hasname, Object: powhatan beaty\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. after being forced into a black brigade charged with building fortifications, beaty enlisted in 1863, earning a medal of honor. later became an actor. with haspreferrednamevariant -> sgt powhatan beaty (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. after being forced into a black brigade charged with building fortifications, beaty enlisted in 1863, earning a medal of honor. later became an actor. with fullnameasstring -> sgt powhatan beaty (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. after being forced into a black brigade charged with building fortifications, beaty enlisted in 1863, earning a medal of honor. later became an actor. with hassurnameasstring -> beaty (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. after being forced into a black brigade charged with building fortifications, beaty enlisted in 1863, earning a medal of honor. later became an actor. with hasfirstnameasstring -> powhatan (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. after being forced into a black brigade charged with building fortifications, beaty enlisted in 1863, earning a medal of honor. later became an actor. with hasdescription -> sgt powhatan beaty was a soldier in the union army during the american civil war. he received the medal of honor for gallantry during the battle of chaffin's farm. (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. after being forced into a black brigade charged with building fortifications, beaty enlisted in 1863, earning a medal of honor. later became an actor. with referstoplaceoforigin -> cincinnati (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. after being forced into a black brigade charged with building fortifications, beaty enlisted in 1863, earning a medal of honor. later became an actor. with has_race -> african-american (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. after being forced into a black brigade charged with building fortifications, beaty enlisted in 1863, earning a medal of honor. later became an actor. with hasagevalue -> nan (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. after being forced into a black brigade charged with building fortifications, beaty enlisted in 1863, earning a medal of honor. later became an actor. with hasbirthdate -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. after being forced into a black brigade charged with building fortifications, beaty enlisted in 1863, earning a medal of honor. later became an actor. with hasdeathdate -> december 6, 1916 (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. after being forced into a black brigade charged with building fortifications, beaty enlisted in 1863, earning a medal of honor. later became an actor. with hassex -> male (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. after being forced into a black brigade charged with building fortifications, beaty enlisted in 1863, earning a medal of honor. later became an actor. with hasstatusgeneratedevent -> enlistment in union army, medal of honor citation (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. after being forced into a black brigade charged with building fortifications, beaty enlisted in 1863, earning a medal of honor. later became an actor. with hasvalue -> soldier, medal of honor recipient (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. after being forced into a black brigade charged with building fortifications, beaty enlisted in 1863, earning a medal of honor. later became an actor. with hasvalue -> soldier, drama director (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. after being forced into a black brigade charged with building fortifications, beaty enlisted in 1863, earning a medal of honor. later became an actor. with hasinteragentrelationshiptype -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. after being forced into a black brigade charged with building fortifications, beaty enlisted in 1863, earning a medal of honor. later became an actor. with isrelationshipto -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. after being forced into a black brigade charged with building fortifications, beaty enlisted in 1863, earning a medal of honor. later became an actor. with isrelationshipfrom -> not mentioned in the text (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. after being forced into a black brigade charged with building fortifications, beaty enlisted in 1863, earning a medal of honor. later became an actor. with hasparticipantroletype -> soldier, drama director (Subject: Powhatan Beaty)\n",
            "Comparing: hasdescription -> born enslaved. free before 13th amendment. after being forced into a black brigade charged with building fortifications, beaty enlisted in 1863, earning a medal of honor. later became an actor. with roleprovidedby -> american civil war, formation of literary and dramatic club (Subject: Powhatan Beaty)\n",
            "No match found for truth row: Powhatan Beaty, Predicate: hasdescription, Object: born enslaved. free before 13th amendment. after being forced into a black brigade charged with building fortifications, beaty enlisted in 1863, earning a medal of honor. later became an actor.\n",
            "\n",
            "Processing truth file: October.tsv\n",
            "Truth DataFrame:\n",
            "                              0                   1  \\\n",
            "0  October alias Robert Barclay             hasName   \n",
            "1  October alias Robert Barclay        hasFirstName   \n",
            "2  October alias Robert Barclay          hasSurname   \n",
            "3  October alias Robert Barclay              hasSex   \n",
            "4  October alias Robert Barclay      hasRaceorColor   \n",
            "5  October alias Robert Barclay      hasDescription   \n",
            "6  October alias Robert Barclay         instance of   \n",
            "7  October alias Robert Barclay  hasParticipantRole   \n",
            "8  October alias Robert Barclay  hasParticipantRole   \n",
            "9  October alias Robert Barclay  hasParticipantRole   \n",
            "\n",
            "                                                   2  \n",
            "0                       October alias Robert Barclay  \n",
            "1                               October alias Robert  \n",
            "2                                            Barclay  \n",
            "3                                               Male  \n",
            "4                                             Creole  \n",
            "5  Enslaved person living and working on Unity Va...  \n",
            "6                                             Person  \n",
            "7                                              Child  \n",
            "8                                    Deceased Person  \n",
            "9                                        Participant  \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-240ee1b14af5>\u001b[0m in \u001b[0;36m<cell line: 75>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;31m# Process each file in the truth directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtruth_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtruth_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mprocess_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruth_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-30-240ee1b14af5>\u001b[0m in \u001b[0;36mprocess_files\u001b[0;34m(truth_file)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mgenerated_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Check if the generated_file is a file and matches the base name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerated_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgenerated_base_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerated_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mgenerated_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerated_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nFound matching generated file: {generated_file}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/genericpath.py\u001b[0m in \u001b[0;36misfile\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;34m\"\"\"Test whether a path is a regular file\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fuzzywuzzy\n",
        "!pip install python-Levenshtein\n",
        "!pip install jellyfish\n",
        "!pip install scikit-learn\n",
        "!pip install bert-score\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIJq73zgfCG_",
        "outputId": "17b14cbd-511c-40be-907f-c1bb9d995e08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.10/dist-packages (0.18.0)\n",
            "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.10/dist-packages (0.26.0)\n",
            "Requirement already satisfied: Levenshtein==0.26.0 in /usr/local/lib/python3.10/dist-packages (from python-Levenshtein) (0.26.0)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /usr/local/lib/python3.10/dist-packages (from Levenshtein==0.26.0->python-Levenshtein) (3.10.0)\n",
            "Requirement already satisfied: jellyfish in /usr/local/lib/python3.10/dist-packages (1.1.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: bert-score in /usr/local/lib/python3.10/dist-packages (0.3.13)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert-score) (2.4.1+cu121)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert-score) (2.2.2)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from bert-score) (4.44.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bert-score) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert-score) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.10/dist-packages (from bert-score) (4.66.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert-score) (3.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert-score) (24.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert-score) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert-score) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert-score) (2024.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (2024.6.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (0.24.7)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (0.19.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (3.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score) (2024.8.30)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->bert-score) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# code to make sure it finds the matching files, adjust directory when using a different generated one\n",
        "\n",
        "# Define the directory paths\n",
        "truth_dir = '/content/drive/My Drive/LLM-Data/truth'\n",
        "generated_dir = '/content/drive/My Drive/LLM-Data/llm-generated/GPT4_Enslaved_MainAgent'\n",
        "\n",
        "# Get the list of files in the truth directory\n",
        "truth_files = os.listdir(truth_dir)\n",
        "\n",
        "# Function to transform name from \"First Last\" format to \"Last_First\"\n",
        "def transform_name(name):\n",
        "    parts = name.split(\" \")\n",
        "    if len(parts) == 2:\n",
        "        return f\"{parts[1]}_{parts[0]}\"  # Reverse the order and join with underscore\n",
        "    return name\n",
        "\n",
        "# List to hold the pairs of matching files\n",
        "matching_pairs = []\n",
        "\n",
        "# Iterate over truth files and find corresponding generated files\n",
        "for truth_file in truth_files:\n",
        "    if truth_file.endswith('.tsv'):  # Check if the file is a TSV\n",
        "        # Prepare the generated file base name by transforming the truth file name\n",
        "        generated_base_name = transform_name(truth_file.strip('.tsv'))\n",
        "\n",
        "        # Check for generated files in the generated directory\n",
        "        for generated_file in os.listdir(generated_dir):\n",
        "            if generated_base_name in generated_file and generated_file.endswith('.tsv'):\n",
        "                matching_pairs.append((truth_file, generated_file))\n",
        "\n",
        "# Print the matching pairs\n",
        "for truth_file, generated_file in matching_pairs:\n",
        "    print(f\"Truth File: {truth_file} <--> Generated File: {generated_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FovrS0n8fFZf",
        "outputId": "804ea27f-3ecb-43f4-e4ef-7faf9324e2b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Truth File: William_Lewis.tsv <--> Generated File: William_Lewis_triples.tsv\n",
            "Truth File: David_Abner.tsv <--> Generated File: David_Abner_triples.tsv\n",
            "Truth File: William_Harvey_Carney.tsv <--> Generated File: William_Harvey_Carney_triples.tsv\n",
            "Truth File: Thomas_Sims.tsv <--> Generated File: Thomas_Sims_triples.tsv\n",
            "Truth File: John_Edward_Bruce.tsv <--> Generated File: John_Edward_Bruce_triples.tsv\n",
            "Truth File: James_Collins_Johnson.tsv <--> Generated File: James_Collins_Johnson_triples.tsv\n",
            "Truth File: Stephen_Bishop.tsv <--> Generated File: Stephen_Bishop_(cave_explorer)_triples.tsv\n",
            "Truth File: John_Horse.tsv <--> Generated File: John_Horse_triples.tsv\n",
            "Truth File: Josiah_Henson.tsv <--> Generated File: Josiah_Henson_triples.tsv\n",
            "Truth File: Asa_Coleman.tsv <--> Generated File: Asa_Coleman_triples.tsv\n",
            "Truth File: Fortune.tsv <--> Generated File: Amos_Fortune_triples.tsv\n",
            "Truth File: Fortune.tsv <--> Generated File: Fortune_(American_slave)_triples.tsv\n",
            "Truth File: Fortune.tsv <--> Generated File: Timothy_Thomas_Fortune_triples.tsv\n",
            "Truth File: Marie_Couvent.tsv <--> Generated File: Marie_Couvent_triples.tsv\n",
            "Truth File: Jordan_Winston_Early.tsv <--> Generated File: Jordan_Winston_Early_triples.tsv\n",
            "Truth File: Jim_Williams.tsv <--> Generated File: Jim_Williams_(militia_leader)_triples.tsv\n",
            "Truth File: Kate_Drumgoold.tsv <--> Generated File: Kate_Drumgoold_triples.tsv\n",
            "Truth File: Lewis_Temple.tsv <--> Generated File: Lewis_Temple_triples.tsv\n",
            "Truth File: George_Moses_Horton.tsv <--> Generated File: George_Moses_Horton_triples.tsv\n",
            "Truth File: Lewis_Clarke.tsv <--> Generated File: Lewis_Clarke_triples.tsv\n",
            "Truth File: Richard_Preston.tsv <--> Generated File: Richard_Preston_triples.tsv\n",
            "Truth File: Charlotte_Dupuy.tsv <--> Generated File: Charlotte_Dupuy_triples.tsv\n",
            "Truth File: Solomon_Northup.tsv <--> Generated File: Solomon_Northup_triples.tsv\n",
            "Truth File: Mary_Hemings.tsv <--> Generated File: Mary_Hemings_triples.tsv\n",
            "Truth File: Patrick_Francis_Healy.tsv <--> Generated File: Patrick_Francis_Healy_triples.tsv\n",
            "Truth File: Benjamin_Singleton.tsv <--> Generated File: Benjamin_Singleton_triples.tsv\n",
            "Truth File: Venture_Smith.tsv <--> Generated File: Venture_Smith_triples.tsv\n",
            "Truth File: Onesimus.tsv <--> Generated File: Onesimus_(Bostonian)_triples.tsv\n",
            "Truth File: Jupiter_Hammon.tsv <--> Generated File: Jupiter_Hammon_triples.tsv\n",
            "Truth File: Preston_Taylor.tsv <--> Generated File: Preston_Taylor_triples.tsv\n",
            "Truth File: Harriet_Powers.tsv <--> Generated File: Harriet_Powers_triples.tsv\n",
            "Truth File: Shields_Green.tsv <--> Generated File: Shields_Green_triples.tsv\n",
            "Truth File: James_Somerset.tsv <--> Generated File: James_Somerset_triples.tsv\n",
            "Truth File: Jeremiah_Haralson.tsv <--> Generated File: Jeremiah_Haralson_triples.tsv\n",
            "Truth File: Prince_Estabrook.tsv <--> Generated File: Prince_Estabrook_triples.tsv\n",
            "Truth File: Boston_King.tsv <--> Generated File: Boston_King_triples.tsv\n",
            "Truth File: Mary_Fields.tsv <--> Generated File: Mary_Fields_triples.tsv\n",
            "Truth File: Prince_Whipple.tsv <--> Generated File: Prince_Whipple_triples.tsv\n",
            "Truth File: Timothy_Thomas_Fortune.tsv <--> Generated File: Timothy_Thomas_Fortune_triples.tsv\n",
            "Truth File: Paul_Jennings.tsv <--> Generated File: Paul_Jennings_(abolitionist)_triples.tsv\n",
            "Truth File: Thomas_Lewis_Johnson.tsv <--> Generated File: Thomas_Lewis_Johnson_triples.tsv\n",
            "Truth File: Pierre_Caliste_Landry.tsv <--> Generated File: Pierre_Caliste_Landry_triples.tsv\n",
            "Truth File: Peter_Salem.tsv <--> Generated File: Peter_Salem_triples.tsv\n",
            "Truth File: Elizabeth_Keckley.tsv <--> Generated File: Elizabeth_Keckley_triples.tsv\n",
            "Truth File: Daniel_Coker.tsv <--> Generated File: Daniel_Coker_triples.tsv\n",
            "Truth File: Peter_Barnabas_Barrow.tsv <--> Generated File: Peter_Barnabas_Barrow_triples.tsv\n",
            "Truth File: Quamina.tsv <--> Generated File: Quamina_triples.tsv\n",
            "Truth File: Jane_Johnson.tsv <--> Generated File: Jane_Johnson_(slave)_triples.tsv\n",
            "Truth File: Powhatan_Beaty.tsv <--> Generated File: Powhatan_Beaty_triples.tsv\n",
            "Truth File: Charles_Caldwell.tsv <--> Generated File: Charles_Caldwell_(politician)_triples.tsv\n",
            "Truth File: Ellen_and_William_Craft.tsv <--> Generated File: Ellen_and_William_Craft_triples.tsv\n",
            "Truth File: Cyrus_Bustill.tsv <--> Generated File: Cyrus_Bustill_triples.tsv\n",
            "Truth File: Alonzo_Herndon.tsv <--> Generated File: Alonzo_Herndon_triples.tsv\n",
            "Truth File: Olaudah_Equiano.tsv <--> Generated File: Olaudah_Equiano_triples.tsv\n",
            "Truth File: Joshua_Johnson.tsv <--> Generated File: Joshua_Johnson_(painter)_triples.tsv\n",
            "Truth File: Bill_Traylor.tsv <--> Generated File: Bill_Traylor_triples.tsv\n",
            "Truth File: Alethia_Tanner.tsv <--> Generated File: Alethia_Tanner_triples.tsv\n",
            "Truth File: Elizabeth_Key.tsv <--> Generated File: Elizabeth_Key_Grinstead_triples.tsv\n",
            "Truth File: Ukawsaw_Gronniosaw.tsv <--> Generated File: Ukawsaw_Gronniosaw_triples.tsv\n",
            "Truth File: Henry_Ossian_Flipper.tsv <--> Generated File: Henry_Ossian_Flipper_triples.tsv\n",
            "Truth File: Harriet_Robinson_Scott.tsv <--> Generated File: Harriet_Robinson_Scott_triples.tsv\n",
            "Truth File: Scipio_Africanus.tsv <--> Generated File: Scipio_Africanus_triples.tsv\n",
            "Truth File: Hannah_Archer_Till.tsv <--> Generated File: Hannah_Archer_Till_triples.tsv\n",
            "Truth File: Abraham_Johnstone.tsv <--> Generated File: Abraham_Johnstone_triples.tsv\n",
            "Truth File: Crispus_Attucks.tsv <--> Generated File: Crispus_Attucks_triples.tsv\n",
            "Truth File: Jermain_Wesley_Loguen.tsv <--> Generated File: Jermain_Wesley_Loguen_triples.tsv\n",
            "Truth File: Margaret_Garner.tsv <--> Generated File: Margaret_Garner_triples.tsv\n",
            "Truth File: Burwell_Colbert.tsv <--> Generated File: Burwell_Colbert_triples.tsv\n",
            "Truth File: William_Green.tsv <--> Generated File: William_Green_(former_slave)_triples.tsv\n",
            "Truth File: Hannah_Crafts.tsv <--> Generated File: Hannah_Crafts_triples.tsv\n",
            "Truth File: Samuel_Green.tsv <--> Generated File: Samuel_Green_(freedman)_triples.tsv\n",
            "Truth File: London_Ferrill.tsv <--> Generated File: London_Ferrill_triples.tsv\n",
            "Truth File: Albery_Allson_Whitman.tsv <--> Generated File: Albery_Allson_Whitman_triples.tsv\n",
            "Truth File: A_E_Coleman.tsv <--> Generated File: A_E_Coleman_triples.tsv\n",
            "Truth File: Allen_Allensworth.tsv <--> Generated File: Allen_Allensworth_triples.tsv\n",
            "Truth File: Octavia_Victoria_Rogers_Albert.tsv <--> Generated File: Octavia_Victoria_Rogers_Albert_triples.tsv\n",
            "Truth File: Mary_Prince.tsv <--> Generated File: Mary_Prince_triples.tsv\n",
            "Truth File: Madison_Washington.tsv <--> Generated File: Madison_Washington_triples.tsv\n",
            "Truth File: William_Saunders_Crowdy.tsv <--> Generated File: William_Saunders_Crowdy_triples.tsv\n",
            "Truth File: Solomon_Bayley.tsv <--> Generated File: Solomon_Bayley_triples.tsv\n",
            "Truth File: Elisha_Winfield_Green.tsv <--> Generated File: Elisha_Winfield_Green_triples.tsv\n",
            "Truth File: Anthony_Burns.tsv <--> Generated File: Anthony_Burns_triples.tsv\n",
            "Truth File: Henry_Bibb.tsv <--> Generated File: Henry_Bibb_triples.tsv\n",
            "Truth File: Abraham_Galloway.tsv <--> Generated File: Abraham_Galloway_triples.tsv\n",
            "Truth File: Isaiah_Mays.tsv <--> Generated File: Isaiah_Mays_triples.tsv\n",
            "Truth File: George_Washington_Carver.tsv <--> Generated File: George_Washington_Carver_triples.tsv\n",
            "Truth File: William_Parker.tsv <--> Generated File: William_Parker_(abolitionist)_triples.tsv\n",
            "Truth File: Joshua_Houston.tsv <--> Generated File: Joshua_Houston_triples.tsv\n",
            "Truth File: Free_Frank_McWorter.tsv <--> Generated File: Free_Frank_McWorter_triples.tsv\n",
            "Truth File: Henry_Box_Brown.tsv <--> Generated File: Henry_Box_Brown_triples.tsv\n",
            "Truth File: George_Freeman_Bragg.tsv <--> Generated File: George_Freeman_Bragg_triples.tsv\n",
            "Truth File: Tom_Bass.tsv <--> Generated File: Tom_Bass_(horse_trainer)_triples.tsv\n",
            "Truth File: Primus_Hall.tsv <--> Generated File: Primus_Hall_triples.tsv\n",
            "Truth File: Biddy_Mason.tsv <--> Generated File: Biddy_Mason_triples.tsv\n",
            "Truth File: Madison_Hemings.tsv <--> Generated File: Madison_Hemings_triples.tsv\n",
            "Truth File: Louisa_Picquet.tsv <--> Generated File: Louisa_Picquet_triples.tsv\n",
            "Truth File: Jane_Minor.tsv <--> Generated File: Jane_Minor_triples.tsv\n",
            "Truth File: Mary_Bowser.tsv <--> Generated File: Mary_Bowser_triples.tsv\n",
            "Truth File: Lunsford_Lane.tsv <--> Generated File: Lunsford_Lane_triples.tsv\n",
            "Truth File: Austin_Steward.tsv <--> Generated File: Austin_Steward_triples.tsv\n",
            "Truth File: Amos_Fortune.tsv <--> Generated File: Amos_Fortune_triples.tsv\n",
            "Truth File: Eliza_Ann_Grier.tsv <--> Generated File: Eliza_Ann_Grier_triples.tsv\n",
            "Truth File: Austin_Dabney.tsv <--> Generated File: Austin_Dabney_triples.tsv\n",
            "Truth File: Briton_Hammon.tsv <--> Generated File: Briton_Hammon_triples.tsv\n",
            "Truth File: Jeffrey_Brace.tsv <--> Generated File: Jeffrey_Brace_triples.tsv\n",
            "Truth File: Moses_Williams.tsv <--> Generated File: Moses_Williams_(artist)_triples.tsv\n",
            "Truth File: Cathay_Williams.tsv <--> Generated File: Cathay_Williams_triples.tsv\n",
            "Truth File: Colonel_Tye.tsv <--> Generated File: Colonel_Tye_triples.tsv\n",
            "Truth File: Isaac_Jefferson.tsv <--> Generated File: Isaac_Jefferson_triples.tsv\n",
            "Truth File: Sojourner_Truth.tsv <--> Generated File: Sojourner_Truth_triples.tsv\n",
            "Truth File: Mahommah_Gardo_Baquaqua.tsv <--> Generated File: Mahommah_Gardo_Baquaqua_triples.tsv\n",
            "Truth File: William_Wells_Brown.tsv <--> Generated File: William_Wells_Brown_triples.tsv\n",
            "Truth File: Levi_Miller.tsv <--> Generated File: Levi_Miller_triples.tsv\n",
            "Truth File: Mary_Black.tsv <--> Generated File: Mary_Black_triples.tsv\n",
            "Truth File: William_Drew_Robeson_I.tsv <--> Generated File: William_Drew_Robeson_I_triples.tsv\n",
            "Truth File: Dorothy_Thomas.tsv <--> Generated File: Dorothy_Thomas_(entrepreneur)_triples.tsv\n",
            "Truth File: Adam_Elliot.tsv <--> Generated File: Adam_Elliot_triples.tsv\n",
            "Truth File: Thomas_Fuller.tsv <--> Generated File: Thomas_Fuller_triples.tsv\n",
            "Truth File: Isaiah_Dorman.tsv <--> Generated File: Isaiah_Dorman_triples.tsv\n",
            "Truth File: Polly_Berry.tsv <--> Generated File: Polly_Berry_triples.tsv\n",
            "Truth File: Callie_House.tsv <--> Generated File: Callie_House_triples.tsv\n",
            "Truth File: Samuel_Ringgold_Ward.tsv <--> Generated File: Samuel_Ringgold_Ward_triples.tsv\n",
            "Truth File: Bethany_Veney.tsv <--> Generated File: Bethany_Veney_triples.tsv\n",
            "Truth File: William_Hooper_Councill.tsv <--> Generated File: William_Hooper_Councill_triples.tsv\n",
            "Truth File: David_Drake.tsv <--> Generated File: David_Drake_triples.tsv\n",
            "Truth File: Johnson_Chesnut_Whittaker.tsv <--> Generated File: Johnson_Chesnut_Whittaker_triples.tsv\n",
            "Truth File: Harriet_Jacobs.tsv <--> Generated File: Harriet_Jacobs_triples.tsv\n",
            "Truth File: Yarrow_Mamout.tsv <--> Generated File: Yarrow_Mamout_triples.tsv\n",
            "Truth File: Peter_Bruner.tsv <--> Generated File: Peter_Bruner_triples.tsv\n",
            "Truth File: Moses_Grandy.tsv <--> Generated File: Moses_Grandy_triples.tsv\n",
            "Truth File: Tom_Molineaux.tsv <--> Generated File: Tom_Molineaux_triples.tsv\n",
            "Truth File: Robert_Voorhis.tsv <--> Generated File: Robert_Voorhis_triples.tsv\n",
            "Truth File: Elijah_Abel.tsv <--> Generated File: Elijah_Abel_triples.tsv\n",
            "Truth File: Scott_Winfield_Bond.tsv <--> Generated File: Scott_Winfield_Bond_triples.tsv\n",
            "Truth File: Moses_Roper.tsv <--> Generated File: Moses_Roper_triples.tsv\n",
            "Truth File: Lucy_Delaney.tsv <--> Generated File: Lucy_Delaney_triples.tsv\n",
            "Truth File: Harriet_Tubman.tsv <--> Generated File: Harriet_Tubman_triples.tsv\n",
            "Truth File: Seymour_Burr.tsv <--> Generated File: Seymour_Burr_triples.tsv\n",
            "Truth File: Harriet_Evans_Paine.tsv <--> Generated File: Harriet_Evans_Paine_triples.tsv\n",
            "Truth File: John_Brown.tsv <--> Generated File: John_Brown_(fugitive_slave)_triples.tsv\n",
            "Truth File: Brent_Woods.tsv <--> Generated File: Brent_Woods_triples.tsv\n",
            "Truth File: Elizabeth_Key_Grinstead.tsv <--> Generated File: Elizabeth_Key_Grinstead_triples.tsv\n",
            "Truth File: Matthew_Gaines.tsv <--> Generated File: Matthew_Gaines_triples.tsv\n",
            "Truth File: Archer_Alexander.tsv <--> Generated File: Archer_Alexander_triples.tsv\n",
            "Truth File: Thomas_Peters.tsv <--> Generated File: Thomas_Peters_triples.tsv\n",
            "Truth File: Lucinda_Davis.tsv <--> Generated File: Lucinda_Davis_triples.tsv\n",
            "Truth File: Jared_Maurice_Arter.tsv <--> Generated File: Jared_Maurice_Arter_triples.tsv\n",
            "Truth File: Lucy_Terry.tsv <--> Generated File: Lucy_Terry_triples.tsv\n",
            "Truth File: Dangerfield_Newby.tsv <--> Generated File: Dangerfield_Newby_triples.tsv\n",
            "Truth File: William_Lee.tsv <--> Generated File: William_Lee_(valet)_triples.tsv\n",
            "Truth File: London_Bourne.tsv <--> Generated File: London_Bourne_triples.tsv\n",
            "Truth File: Jean-François_Papillon.tsv <--> Generated File: Jean-François_Papillon_triples.tsv\n",
            "Truth File: Clara_Brown.tsv <--> Generated File: Clara_Brown_triples.tsv\n",
            "Truth File: Jack_Sisson.tsv <--> Generated File: Jack_Sisson_triples.tsv\n",
            "Truth File: Joice_Heth.tsv <--> Generated File: Joice_Heth_triples.tsv\n",
            "Truth File: William_Washington_Browne.tsv <--> Generated File: William_Washington_Browne_triples.tsv\n",
            "Truth File: Salem_Poor.tsv <--> Generated File: Salem_Poor_triples.tsv\n",
            "Truth File: Joseph_Rainey.tsv <--> Generated File: Joseph_Rainey_triples.tsv\n",
            "Truth File: Scipio_Moorhead.tsv <--> Generated File: Scipio_Moorhead_triples.tsv\n",
            "Truth File: Azeline_Hearne.tsv <--> Generated File: Azeline_Hearne_triples.tsv\n",
            "Truth File: Shadrach_Minkins.tsv <--> Generated File: Shadrach_Minkins_triples.tsv\n",
            "Truth File: Amanda_Smith.tsv <--> Generated File: Amanda_Smith_triples.tsv\n",
            "Truth File: Oscar_Marion.tsv <--> Generated File: Oscar_Marion_triples.tsv\n",
            "Truth File: Molly_Williams.tsv <--> Generated File: Molly_Williams_triples.tsv\n",
            "Truth File: David_George.tsv <--> Generated File: David_George_(Baptist)_triples.tsv\n",
            "Truth File: Oscar_Dunn.tsv <--> Generated File: Oscar_Dunn_triples.tsv\n",
            "Truth File: John_Jea.tsv <--> Generated File: John_Jea_triples.tsv\n",
            "Truth File: William_Grimes.tsv <--> Generated File: William_Grimes_(ex-slave)_triples.tsv\n",
            "Truth File: Decatur_Dorsey.tsv <--> Generated File: Decatur_Dorsey_triples.tsv\n",
            "Truth File: Bose_Ikard.tsv <--> Generated File: Bose_Ikard_triples.tsv\n",
            "Truth File: Absalom_Jones.tsv <--> Generated File: Absalom_Jones_triples.tsv\n",
            "Truth File: John_Parker.tsv <--> Generated File: John_Parker_(abolitionist)_triples.tsv\n",
            "Truth File: Pierre_Toussaint.tsv <--> Generated File: Pierre_Toussaint_triples.tsv\n",
            "Truth File: Frederick_Douglass.tsv <--> Generated File: Frederick_Douglass_triples.tsv\n",
            "Truth File: Jenny_Slew.tsv <--> Generated File: Jenny_Slew_triples.tsv\n",
            "Truth File: Lewis_Hayden.tsv <--> Generated File: Lewis_Hayden_triples.tsv\n",
            "Truth File: Ada_Copeland_King.tsv <--> Generated File: Ada_Copeland_King_triples.tsv\n",
            "Truth File: Andrew_Bryan.tsv <--> Generated File: Andrew_Bryan_(Baptist)_triples.tsv\n",
            "Truth File: Benjamin_Bradley.tsv <--> Generated File: Benjamin_Bradley_(inventor)_triples.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Define the generated directory path\n",
        "generated_dir = '/content/drive/My Drive/LLM-Data/llm-generated/GPT4_Enslaved_notrestrictedToMAgent'\n",
        "\n",
        "# List to hold processed DataFrames for verification\n",
        "processed_dataframes = []\n",
        "\n",
        "# Process each generated file in the directory\n",
        "for generated_file in os.listdir(generated_dir):\n",
        "    if generated_file.endswith('.tsv'):\n",
        "        # Load the generated file\n",
        "        generated_path = os.path.join(generated_dir, generated_file)\n",
        "        generated_df = pd.read_csv(generated_path, sep='\\t', header=0)\n",
        "\n",
        "        # Drop the first row\n",
        "        generated_df = generated_df.drop(index=0)\n",
        "\n",
        "        # Modify the 'Subject' column by removing underscores and inverting words\n",
        "        generated_df['Subject'] = generated_df['Subject'].apply(\n",
        "            lambda x: ' '.join(x.replace('_', ' ').replace(\"triples\", \"\").split()[::-1])\n",
        "        )\n",
        "\n",
        "        # Add the processed DataFrame to the list for later inspection\n",
        "        processed_dataframes.append((generated_file, generated_df))\n",
        "\n",
        "        # Print one example processed file (the first one)\n",
        "        if len(processed_dataframes) == 1:\n",
        "            print(f\"Processed DataFrame from {generated_file}:\")\n",
        "            print(generated_df.head())  # Display the first few rows of the DataFrame\n",
        "\n",
        "# Optionally, you can save the modified DataFrames if needed\n",
        "# for file_name, df in processed_dataframes:\n",
        "#     output_path = os.path.join(generated_dir, f\"processed_{file_name}\")\n",
        "#     df.to_csv(output_path, sep='\\t', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhO98K3vkmh7",
        "outputId": "43444839-1b52-41ee-aad9-5f85a33ef508"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed DataFrame from Yarrow_Mamout_triples.tsv:\n",
            "         Subject                Predicate  \\\n",
            "1  Mamout Yarrow  hasPreferredNameVariant   \n",
            "2  Mamout Yarrow         fullNameAsString   \n",
            "3  Mamout Yarrow       hasSurnameAsString   \n",
            "4  Mamout Yarrow     hasFirstnameAsString   \n",
            "5  Mamout Yarrow           hasDescription   \n",
            "\n",
            "                                              Object  \n",
            "1                                      Yarrow Mamout  \n",
            "2                                      Yarrow Mamout  \n",
            "3                                             Mamout  \n",
            "4                                             Yarrow  \n",
            "5  Formerly enslaved African entrepreneur and pro...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mx1VjOAolA9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from fuzzywuzzy import fuzz\n",
        "import jellyfish\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from bert_score import score as bert_score\n",
        "\n",
        "# Define the directory paths and adjust when dealing with different generated files\n",
        "truth_dir = '/content/drive/My Drive/LLM-Data/truth'\n",
        "generated_dir = '/content/drive/My Drive/LLM-Data/llm-generated/GPT4_Enslaved_MainAgent'\n",
        "output_dir = '/content/drive/My Drive/LLM-Data/evaluate-test/GPT4_Enslaved_MainAgent'\n",
        "\n",
        "# Ensure the output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Transform name from \"First Last\" format to \"Last_First\"\n",
        "def transform_name(name):\n",
        "    parts = name.split(\" \")\n",
        "    if len(parts) == 2:\n",
        "        return f\"{parts[1]}_{parts[0]}\"  # Reverse the order and join with underscore\n",
        "    return name\n",
        "\n",
        "# Get the list of files in the truth directory and create matching pairs\n",
        "truth_files = os.listdir(truth_dir)\n",
        "matching_pairs = []\n",
        "\n",
        "for truth_file in truth_files:\n",
        "    if truth_file.endswith('.tsv'):\n",
        "        generated_base_name = transform_name(truth_file.strip('.tsv'))\n",
        "\n",
        "        # Look for generated files matching the base name\n",
        "        for generated_file in os.listdir(generated_dir):\n",
        "            if generated_base_name in generated_file and generated_file.endswith('.tsv'):\n",
        "                matching_pairs.append((truth_file, generated_file))\n",
        "\n",
        "# Print the matching pairs\n",
        "for truth_file, generated_file in matching_pairs:\n",
        "    print(f\"Truth File: {truth_file} <--> Generated File: {generated_file}\")\n",
        "\n",
        "# Process the matched pairs and calculate similarity metrics\n",
        "for truth_file, generated_file in matching_pairs:\n",
        "    print(f\"\\nProcessing truth file: {truth_file} with generated file: {generated_file}\")\n",
        "\n",
        "    # Load the truth file\n",
        "    truth_path = os.path.join(truth_dir, truth_file)\n",
        "    truth_df = pd.read_csv(truth_path, sep='\\t', header=None)\n",
        "    truth_df[1] = truth_df[1].str.lower()  # Normalize predicates\n",
        "    truth_df[2] = truth_df[2].str.lower()  # Normalize objects\n",
        "\n",
        "    # Load the generated file\n",
        "    generated_path = os.path.join(generated_dir, generated_file)\n",
        "    generated_df = pd.read_csv(generated_path, sep='\\t', header=0).drop(index=0)\n",
        "\n",
        "    # Modify the Subject column\n",
        "    generated_df['Subject'] = generated_df['Subject'].apply(\n",
        "        lambda x: ' '.join(x.replace('_', ' ').replace(\"triples\", \"\").split()[::-1])\n",
        "    )\n",
        "\n",
        "    # Normalize generated predicates and objects\n",
        "    generated_df['Predicate'] = generated_df['Predicate'].str.lower()\n",
        "    generated_df['Object'] = generated_df['Object'].str.lower()\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Use a TfidfVectorizer instance outside the loop for efficiency\n",
        "    vectorizer = TfidfVectorizer()\n",
        "\n",
        "    # Debugging: Print the number of rows in truth and generated DataFrames\n",
        "    print(f\"Number of truth triples: {len(truth_df)}\")\n",
        "    print(f\"Number of generated triples: {len(generated_df)}\")\n",
        "\n",
        "    # Iterate through truth and generated triples for similarity metrics\n",
        "    for truth_index, truth_row in truth_df.iterrows():\n",
        "        truth_pred, truth_obj = truth_row[1], truth_row[2]\n",
        "\n",
        "        for generated_index, generated_row in generated_df.iterrows():\n",
        "            generated_pred, generated_obj = generated_row['Predicate'], generated_row['Object']\n",
        "\n",
        "            # Debugging: Print the currently compared triples\n",
        "            print(f\"Comparing Truth Predicate: {truth_pred} with Generated Predicate: {generated_pred}\")\n",
        "            print(f\"Comparing Truth Object: {truth_obj} with Generated Object: {generated_obj}\")\n",
        "\n",
        "            # Calculate similarity metrics\n",
        "            metrics = {\n",
        "                \"Predicate Fuzzy Ratio\": fuzz.ratio(truth_pred, generated_pred),\n",
        "                \"Predicate Jaro-Winkler\": jellyfish.jaro_winkler_similarity(truth_pred, generated_pred),\n",
        "                \"Object Fuzzy Ratio\": fuzz.ratio(truth_obj, generated_obj),\n",
        "                \"Object Jaro-Winkler\": jellyfish.jaro_winkler_similarity(truth_obj, generated_obj),\n",
        "            }\n",
        "\n",
        "            # Calculate Cosine Similarity using a single vectorizer call\n",
        "            vectors = vectorizer.fit_transform([truth_pred, generated_pred]).toarray()\n",
        "            metrics[\"Predicate Cosine Similarity\"] = cosine_similarity(vectors)[0, 1]\n",
        "\n",
        "            vectors = vectorizer.fit_transform([truth_obj, generated_obj]).toarray()\n",
        "            metrics[\"Object Cosine Similarity\"] = cosine_similarity(vectors)[0, 1]\n",
        "\n",
        "            # BERTScore for predicates and objects\n",
        "            P, R, F1 = bert_score([truth_pred], [generated_pred], lang='en', verbose=False)\n",
        "            metrics[\"Predicate BERTScore\"] = F1.mean().item()\n",
        "\n",
        "            P, R, F1 = bert_score([truth_obj], [generated_obj], lang='en', verbose=False)\n",
        "            metrics[\"Object BERTScore\"] = F1.mean().item()\n",
        "\n",
        "            # Append results\n",
        "            results.append({\n",
        "                \"Truth Predicate\": truth_pred,\n",
        "                \"Truth Object\": truth_obj,\n",
        "                \"Generated Predicate\": generated_pred,\n",
        "                \"Generated Object\": generated_obj,\n",
        "                **metrics\n",
        "            })\n",
        "\n",
        "    # Save results to a CSV file for this pair\n",
        "    results_df = pd.DataFrame(results)\n",
        "    output_file_name = f\"{os.path.splitext(generated_file)[0]}_evaluation.csv\"\n",
        "    output_file_path = os.path.join(output_dir, output_file_name)\n",
        "    results_df.to_csv(output_file_path, index=False)\n",
        "    print(f\"Results saved to: {output_file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Mf5-JfCqldKO",
        "outputId": "0e4cb3ed-44e2-4a44-903f-611d01241967"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Truth File: William_Lewis.tsv <--> Generated File: William_Lewis_triples.tsv\n",
            "Truth File: David_Abner.tsv <--> Generated File: David_Abner_triples.tsv\n",
            "Truth File: William_Harvey_Carney.tsv <--> Generated File: William_Harvey_Carney_triples.tsv\n",
            "Truth File: Thomas_Sims.tsv <--> Generated File: Thomas_Sims_triples.tsv\n",
            "Truth File: John_Edward_Bruce.tsv <--> Generated File: John_Edward_Bruce_triples.tsv\n",
            "Truth File: James_Collins_Johnson.tsv <--> Generated File: James_Collins_Johnson_triples.tsv\n",
            "Truth File: Stephen_Bishop.tsv <--> Generated File: Stephen_Bishop_(cave_explorer)_triples.tsv\n",
            "Truth File: John_Horse.tsv <--> Generated File: John_Horse_triples.tsv\n",
            "Truth File: Josiah_Henson.tsv <--> Generated File: Josiah_Henson_triples.tsv\n",
            "Truth File: Asa_Coleman.tsv <--> Generated File: Asa_Coleman_triples.tsv\n",
            "Truth File: Fortune.tsv <--> Generated File: Amos_Fortune_triples.tsv\n",
            "Truth File: Fortune.tsv <--> Generated File: Fortune_(American_slave)_triples.tsv\n",
            "Truth File: Fortune.tsv <--> Generated File: Timothy_Thomas_Fortune_triples.tsv\n",
            "Truth File: Marie_Couvent.tsv <--> Generated File: Marie_Couvent_triples.tsv\n",
            "Truth File: Jordan_Winston_Early.tsv <--> Generated File: Jordan_Winston_Early_triples.tsv\n",
            "Truth File: Jim_Williams.tsv <--> Generated File: Jim_Williams_(militia_leader)_triples.tsv\n",
            "Truth File: Kate_Drumgoold.tsv <--> Generated File: Kate_Drumgoold_triples.tsv\n",
            "Truth File: Lewis_Temple.tsv <--> Generated File: Lewis_Temple_triples.tsv\n",
            "Truth File: George_Moses_Horton.tsv <--> Generated File: George_Moses_Horton_triples.tsv\n",
            "Truth File: Lewis_Clarke.tsv <--> Generated File: Lewis_Clarke_triples.tsv\n",
            "Truth File: Richard_Preston.tsv <--> Generated File: Richard_Preston_triples.tsv\n",
            "Truth File: Charlotte_Dupuy.tsv <--> Generated File: Charlotte_Dupuy_triples.tsv\n",
            "Truth File: Solomon_Northup.tsv <--> Generated File: Solomon_Northup_triples.tsv\n",
            "Truth File: Mary_Hemings.tsv <--> Generated File: Mary_Hemings_triples.tsv\n",
            "Truth File: Patrick_Francis_Healy.tsv <--> Generated File: Patrick_Francis_Healy_triples.tsv\n",
            "Truth File: Benjamin_Singleton.tsv <--> Generated File: Benjamin_Singleton_triples.tsv\n",
            "Truth File: Venture_Smith.tsv <--> Generated File: Venture_Smith_triples.tsv\n",
            "Truth File: Onesimus.tsv <--> Generated File: Onesimus_(Bostonian)_triples.tsv\n",
            "Truth File: Jupiter_Hammon.tsv <--> Generated File: Jupiter_Hammon_triples.tsv\n",
            "Truth File: Preston_Taylor.tsv <--> Generated File: Preston_Taylor_triples.tsv\n",
            "Truth File: Harriet_Powers.tsv <--> Generated File: Harriet_Powers_triples.tsv\n",
            "Truth File: Shields_Green.tsv <--> Generated File: Shields_Green_triples.tsv\n",
            "Truth File: James_Somerset.tsv <--> Generated File: James_Somerset_triples.tsv\n",
            "Truth File: Jeremiah_Haralson.tsv <--> Generated File: Jeremiah_Haralson_triples.tsv\n",
            "Truth File: Prince_Estabrook.tsv <--> Generated File: Prince_Estabrook_triples.tsv\n",
            "Truth File: Boston_King.tsv <--> Generated File: Boston_King_triples.tsv\n",
            "Truth File: Mary_Fields.tsv <--> Generated File: Mary_Fields_triples.tsv\n",
            "Truth File: Prince_Whipple.tsv <--> Generated File: Prince_Whipple_triples.tsv\n",
            "Truth File: Timothy_Thomas_Fortune.tsv <--> Generated File: Timothy_Thomas_Fortune_triples.tsv\n",
            "Truth File: Paul_Jennings.tsv <--> Generated File: Paul_Jennings_(abolitionist)_triples.tsv\n",
            "Truth File: Thomas_Lewis_Johnson.tsv <--> Generated File: Thomas_Lewis_Johnson_triples.tsv\n",
            "Truth File: Pierre_Caliste_Landry.tsv <--> Generated File: Pierre_Caliste_Landry_triples.tsv\n",
            "Truth File: Peter_Salem.tsv <--> Generated File: Peter_Salem_triples.tsv\n",
            "Truth File: Elizabeth_Keckley.tsv <--> Generated File: Elizabeth_Keckley_triples.tsv\n",
            "Truth File: Daniel_Coker.tsv <--> Generated File: Daniel_Coker_triples.tsv\n",
            "Truth File: Peter_Barnabas_Barrow.tsv <--> Generated File: Peter_Barnabas_Barrow_triples.tsv\n",
            "Truth File: Quamina.tsv <--> Generated File: Quamina_triples.tsv\n",
            "Truth File: Jane_Johnson.tsv <--> Generated File: Jane_Johnson_(slave)_triples.tsv\n",
            "Truth File: Powhatan_Beaty.tsv <--> Generated File: Powhatan_Beaty_triples.tsv\n",
            "Truth File: Charles_Caldwell.tsv <--> Generated File: Charles_Caldwell_(politician)_triples.tsv\n",
            "Truth File: Ellen_and_William_Craft.tsv <--> Generated File: Ellen_and_William_Craft_triples.tsv\n",
            "Truth File: Cyrus_Bustill.tsv <--> Generated File: Cyrus_Bustill_triples.tsv\n",
            "Truth File: Alonzo_Herndon.tsv <--> Generated File: Alonzo_Herndon_triples.tsv\n",
            "Truth File: Olaudah_Equiano.tsv <--> Generated File: Olaudah_Equiano_triples.tsv\n",
            "Truth File: Joshua_Johnson.tsv <--> Generated File: Joshua_Johnson_(painter)_triples.tsv\n",
            "Truth File: Bill_Traylor.tsv <--> Generated File: Bill_Traylor_triples.tsv\n",
            "Truth File: Alethia_Tanner.tsv <--> Generated File: Alethia_Tanner_triples.tsv\n",
            "Truth File: Elizabeth_Key.tsv <--> Generated File: Elizabeth_Key_Grinstead_triples.tsv\n",
            "Truth File: Ukawsaw_Gronniosaw.tsv <--> Generated File: Ukawsaw_Gronniosaw_triples.tsv\n",
            "Truth File: Henry_Ossian_Flipper.tsv <--> Generated File: Henry_Ossian_Flipper_triples.tsv\n",
            "Truth File: Harriet_Robinson_Scott.tsv <--> Generated File: Harriet_Robinson_Scott_triples.tsv\n",
            "Truth File: Scipio_Africanus.tsv <--> Generated File: Scipio_Africanus_triples.tsv\n",
            "Truth File: Hannah_Archer_Till.tsv <--> Generated File: Hannah_Archer_Till_triples.tsv\n",
            "Truth File: Abraham_Johnstone.tsv <--> Generated File: Abraham_Johnstone_triples.tsv\n",
            "Truth File: Crispus_Attucks.tsv <--> Generated File: Crispus_Attucks_triples.tsv\n",
            "Truth File: Jermain_Wesley_Loguen.tsv <--> Generated File: Jermain_Wesley_Loguen_triples.tsv\n",
            "Truth File: Margaret_Garner.tsv <--> Generated File: Margaret_Garner_triples.tsv\n",
            "Truth File: Burwell_Colbert.tsv <--> Generated File: Burwell_Colbert_triples.tsv\n",
            "Truth File: William_Green.tsv <--> Generated File: William_Green_(former_slave)_triples.tsv\n",
            "Truth File: Hannah_Crafts.tsv <--> Generated File: Hannah_Crafts_triples.tsv\n",
            "Truth File: Samuel_Green.tsv <--> Generated File: Samuel_Green_(freedman)_triples.tsv\n",
            "Truth File: London_Ferrill.tsv <--> Generated File: London_Ferrill_triples.tsv\n",
            "Truth File: Albery_Allson_Whitman.tsv <--> Generated File: Albery_Allson_Whitman_triples.tsv\n",
            "Truth File: A_E_Coleman.tsv <--> Generated File: A_E_Coleman_triples.tsv\n",
            "Truth File: Allen_Allensworth.tsv <--> Generated File: Allen_Allensworth_triples.tsv\n",
            "Truth File: Octavia_Victoria_Rogers_Albert.tsv <--> Generated File: Octavia_Victoria_Rogers_Albert_triples.tsv\n",
            "Truth File: Mary_Prince.tsv <--> Generated File: Mary_Prince_triples.tsv\n",
            "Truth File: Madison_Washington.tsv <--> Generated File: Madison_Washington_triples.tsv\n",
            "Truth File: William_Saunders_Crowdy.tsv <--> Generated File: William_Saunders_Crowdy_triples.tsv\n",
            "Truth File: Solomon_Bayley.tsv <--> Generated File: Solomon_Bayley_triples.tsv\n",
            "Truth File: Elisha_Winfield_Green.tsv <--> Generated File: Elisha_Winfield_Green_triples.tsv\n",
            "Truth File: Anthony_Burns.tsv <--> Generated File: Anthony_Burns_triples.tsv\n",
            "Truth File: Henry_Bibb.tsv <--> Generated File: Henry_Bibb_triples.tsv\n",
            "Truth File: Abraham_Galloway.tsv <--> Generated File: Abraham_Galloway_triples.tsv\n",
            "Truth File: Isaiah_Mays.tsv <--> Generated File: Isaiah_Mays_triples.tsv\n",
            "Truth File: George_Washington_Carver.tsv <--> Generated File: George_Washington_Carver_triples.tsv\n",
            "Truth File: William_Parker.tsv <--> Generated File: William_Parker_(abolitionist)_triples.tsv\n",
            "Truth File: Joshua_Houston.tsv <--> Generated File: Joshua_Houston_triples.tsv\n",
            "Truth File: Free_Frank_McWorter.tsv <--> Generated File: Free_Frank_McWorter_triples.tsv\n",
            "Truth File: Henry_Box_Brown.tsv <--> Generated File: Henry_Box_Brown_triples.tsv\n",
            "Truth File: George_Freeman_Bragg.tsv <--> Generated File: George_Freeman_Bragg_triples.tsv\n",
            "Truth File: Tom_Bass.tsv <--> Generated File: Tom_Bass_(horse_trainer)_triples.tsv\n",
            "Truth File: Primus_Hall.tsv <--> Generated File: Primus_Hall_triples.tsv\n",
            "Truth File: Biddy_Mason.tsv <--> Generated File: Biddy_Mason_triples.tsv\n",
            "Truth File: Madison_Hemings.tsv <--> Generated File: Madison_Hemings_triples.tsv\n",
            "Truth File: Louisa_Picquet.tsv <--> Generated File: Louisa_Picquet_triples.tsv\n",
            "Truth File: Jane_Minor.tsv <--> Generated File: Jane_Minor_triples.tsv\n",
            "Truth File: Mary_Bowser.tsv <--> Generated File: Mary_Bowser_triples.tsv\n",
            "Truth File: Lunsford_Lane.tsv <--> Generated File: Lunsford_Lane_triples.tsv\n",
            "Truth File: Austin_Steward.tsv <--> Generated File: Austin_Steward_triples.tsv\n",
            "Truth File: Amos_Fortune.tsv <--> Generated File: Amos_Fortune_triples.tsv\n",
            "Truth File: Eliza_Ann_Grier.tsv <--> Generated File: Eliza_Ann_Grier_triples.tsv\n",
            "Truth File: Austin_Dabney.tsv <--> Generated File: Austin_Dabney_triples.tsv\n",
            "Truth File: Briton_Hammon.tsv <--> Generated File: Briton_Hammon_triples.tsv\n",
            "Truth File: Jeffrey_Brace.tsv <--> Generated File: Jeffrey_Brace_triples.tsv\n",
            "Truth File: Moses_Williams.tsv <--> Generated File: Moses_Williams_(artist)_triples.tsv\n",
            "Truth File: Cathay_Williams.tsv <--> Generated File: Cathay_Williams_triples.tsv\n",
            "Truth File: Colonel_Tye.tsv <--> Generated File: Colonel_Tye_triples.tsv\n",
            "Truth File: Isaac_Jefferson.tsv <--> Generated File: Isaac_Jefferson_triples.tsv\n",
            "Truth File: Sojourner_Truth.tsv <--> Generated File: Sojourner_Truth_triples.tsv\n",
            "Truth File: Mahommah_Gardo_Baquaqua.tsv <--> Generated File: Mahommah_Gardo_Baquaqua_triples.tsv\n",
            "Truth File: William_Wells_Brown.tsv <--> Generated File: William_Wells_Brown_triples.tsv\n",
            "Truth File: Levi_Miller.tsv <--> Generated File: Levi_Miller_triples.tsv\n",
            "Truth File: Mary_Black.tsv <--> Generated File: Mary_Black_triples.tsv\n",
            "Truth File: William_Drew_Robeson_I.tsv <--> Generated File: William_Drew_Robeson_I_triples.tsv\n",
            "Truth File: Dorothy_Thomas.tsv <--> Generated File: Dorothy_Thomas_(entrepreneur)_triples.tsv\n",
            "Truth File: Adam_Elliot.tsv <--> Generated File: Adam_Elliot_triples.tsv\n",
            "Truth File: Thomas_Fuller.tsv <--> Generated File: Thomas_Fuller_triples.tsv\n",
            "Truth File: Isaiah_Dorman.tsv <--> Generated File: Isaiah_Dorman_triples.tsv\n",
            "Truth File: Polly_Berry.tsv <--> Generated File: Polly_Berry_triples.tsv\n",
            "Truth File: Callie_House.tsv <--> Generated File: Callie_House_triples.tsv\n",
            "Truth File: Samuel_Ringgold_Ward.tsv <--> Generated File: Samuel_Ringgold_Ward_triples.tsv\n",
            "Truth File: Bethany_Veney.tsv <--> Generated File: Bethany_Veney_triples.tsv\n",
            "Truth File: William_Hooper_Councill.tsv <--> Generated File: William_Hooper_Councill_triples.tsv\n",
            "Truth File: David_Drake.tsv <--> Generated File: David_Drake_triples.tsv\n",
            "Truth File: Johnson_Chesnut_Whittaker.tsv <--> Generated File: Johnson_Chesnut_Whittaker_triples.tsv\n",
            "Truth File: Harriet_Jacobs.tsv <--> Generated File: Harriet_Jacobs_triples.tsv\n",
            "Truth File: Yarrow_Mamout.tsv <--> Generated File: Yarrow_Mamout_triples.tsv\n",
            "Truth File: Peter_Bruner.tsv <--> Generated File: Peter_Bruner_triples.tsv\n",
            "Truth File: Moses_Grandy.tsv <--> Generated File: Moses_Grandy_triples.tsv\n",
            "Truth File: Tom_Molineaux.tsv <--> Generated File: Tom_Molineaux_triples.tsv\n",
            "Truth File: Robert_Voorhis.tsv <--> Generated File: Robert_Voorhis_triples.tsv\n",
            "Truth File: Elijah_Abel.tsv <--> Generated File: Elijah_Abel_triples.tsv\n",
            "Truth File: Scott_Winfield_Bond.tsv <--> Generated File: Scott_Winfield_Bond_triples.tsv\n",
            "Truth File: Moses_Roper.tsv <--> Generated File: Moses_Roper_triples.tsv\n",
            "Truth File: Lucy_Delaney.tsv <--> Generated File: Lucy_Delaney_triples.tsv\n",
            "Truth File: Harriet_Tubman.tsv <--> Generated File: Harriet_Tubman_triples.tsv\n",
            "Truth File: Seymour_Burr.tsv <--> Generated File: Seymour_Burr_triples.tsv\n",
            "Truth File: Harriet_Evans_Paine.tsv <--> Generated File: Harriet_Evans_Paine_triples.tsv\n",
            "Truth File: John_Brown.tsv <--> Generated File: John_Brown_(fugitive_slave)_triples.tsv\n",
            "Truth File: Brent_Woods.tsv <--> Generated File: Brent_Woods_triples.tsv\n",
            "Truth File: Elizabeth_Key_Grinstead.tsv <--> Generated File: Elizabeth_Key_Grinstead_triples.tsv\n",
            "Truth File: Matthew_Gaines.tsv <--> Generated File: Matthew_Gaines_triples.tsv\n",
            "Truth File: Archer_Alexander.tsv <--> Generated File: Archer_Alexander_triples.tsv\n",
            "Truth File: Thomas_Peters.tsv <--> Generated File: Thomas_Peters_triples.tsv\n",
            "Truth File: Lucinda_Davis.tsv <--> Generated File: Lucinda_Davis_triples.tsv\n",
            "Truth File: Jared_Maurice_Arter.tsv <--> Generated File: Jared_Maurice_Arter_triples.tsv\n",
            "Truth File: Lucy_Terry.tsv <--> Generated File: Lucy_Terry_triples.tsv\n",
            "Truth File: Dangerfield_Newby.tsv <--> Generated File: Dangerfield_Newby_triples.tsv\n",
            "Truth File: William_Lee.tsv <--> Generated File: William_Lee_(valet)_triples.tsv\n",
            "Truth File: London_Bourne.tsv <--> Generated File: London_Bourne_triples.tsv\n",
            "Truth File: Jean-François_Papillon.tsv <--> Generated File: Jean-François_Papillon_triples.tsv\n",
            "Truth File: Clara_Brown.tsv <--> Generated File: Clara_Brown_triples.tsv\n",
            "Truth File: Jack_Sisson.tsv <--> Generated File: Jack_Sisson_triples.tsv\n",
            "Truth File: Joice_Heth.tsv <--> Generated File: Joice_Heth_triples.tsv\n",
            "Truth File: William_Washington_Browne.tsv <--> Generated File: William_Washington_Browne_triples.tsv\n",
            "Truth File: Salem_Poor.tsv <--> Generated File: Salem_Poor_triples.tsv\n",
            "Truth File: Joseph_Rainey.tsv <--> Generated File: Joseph_Rainey_triples.tsv\n",
            "Truth File: Scipio_Moorhead.tsv <--> Generated File: Scipio_Moorhead_triples.tsv\n",
            "Truth File: Azeline_Hearne.tsv <--> Generated File: Azeline_Hearne_triples.tsv\n",
            "Truth File: Shadrach_Minkins.tsv <--> Generated File: Shadrach_Minkins_triples.tsv\n",
            "Truth File: Amanda_Smith.tsv <--> Generated File: Amanda_Smith_triples.tsv\n",
            "Truth File: Oscar_Marion.tsv <--> Generated File: Oscar_Marion_triples.tsv\n",
            "Truth File: Molly_Williams.tsv <--> Generated File: Molly_Williams_triples.tsv\n",
            "Truth File: David_George.tsv <--> Generated File: David_George_(Baptist)_triples.tsv\n",
            "Truth File: Oscar_Dunn.tsv <--> Generated File: Oscar_Dunn_triples.tsv\n",
            "Truth File: John_Jea.tsv <--> Generated File: John_Jea_triples.tsv\n",
            "Truth File: William_Grimes.tsv <--> Generated File: William_Grimes_(ex-slave)_triples.tsv\n",
            "Truth File: Decatur_Dorsey.tsv <--> Generated File: Decatur_Dorsey_triples.tsv\n",
            "Truth File: Bose_Ikard.tsv <--> Generated File: Bose_Ikard_triples.tsv\n",
            "Truth File: Absalom_Jones.tsv <--> Generated File: Absalom_Jones_triples.tsv\n",
            "Truth File: John_Parker.tsv <--> Generated File: John_Parker_(abolitionist)_triples.tsv\n",
            "Truth File: Pierre_Toussaint.tsv <--> Generated File: Pierre_Toussaint_triples.tsv\n",
            "Truth File: Frederick_Douglass.tsv <--> Generated File: Frederick_Douglass_triples.tsv\n",
            "Truth File: Jenny_Slew.tsv <--> Generated File: Jenny_Slew_triples.tsv\n",
            "Truth File: Lewis_Hayden.tsv <--> Generated File: Lewis_Hayden_triples.tsv\n",
            "Truth File: Ada_Copeland_King.tsv <--> Generated File: Ada_Copeland_King_triples.tsv\n",
            "Truth File: Andrew_Bryan.tsv <--> Generated File: Andrew_Bryan_(Baptist)_triples.tsv\n",
            "Truth File: Benjamin_Bradley.tsv <--> Generated File: Benjamin_Bradley_(inventor)_triples.tsv\n",
            "\n",
            "Processing truth file: William_Lewis.tsv with generated file: William_Lewis_triples.tsv\n",
            "Number of truth triples: 12\n",
            "Number of generated triples: 16\n",
            "Comparing Truth Predicate: hasname with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: william lewis with Generated Object: william lewis reece\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: william lewis with Generated Object: william lewis reece\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: william lewis with Generated Object: reece\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: william lewis with Generated Object: william lewis\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: william lewis with Generated Object: american serial killer, rapist and kidnapper. in 2015, he was linked via dna to the 1997 cold case murder of a woman in oklahoma, for which he was subsequently convicted and sentenced to death. not long after, he confessed to three murders associated with the texas killing fields, for which he was convicted and sentenced to life imprisonment in 2022.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: william lewis with Generated Object: oklahoma\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: william lewis with Generated Object: july 1, 1959\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hassex\n",
            "Comparing Truth Object: william lewis with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: william lewis with Generated Object: conviction for murder, kidnapping and rape\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: william lewis with Generated Object: convicted criminal\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: william lewis with Generated Object: farm laborer\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: william lewis with Generated Object: spouse\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: william lewis with Generated Object: judy flaming\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: william lewis with Generated Object: william lewis reece\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: william lewis with Generated Object: perpetrator\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: william lewis with Generated Object: murders, kidnapping and rape\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: male with Generated Object: william lewis reece\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: male with Generated Object: william lewis reece\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: male with Generated Object: reece\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: male with Generated Object: william lewis\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: male with Generated Object: american serial killer, rapist and kidnapper. in 2015, he was linked via dna to the 1997 cold case murder of a woman in oklahoma, for which he was subsequently convicted and sentenced to death. not long after, he confessed to three murders associated with the texas killing fields, for which he was convicted and sentenced to life imprisonment in 2022.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: male with Generated Object: oklahoma\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: male with Generated Object: july 1, 1959\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hassex\n",
            "Comparing Truth Object: male with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: male with Generated Object: conviction for murder, kidnapping and rape\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: male with Generated Object: convicted criminal\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: male with Generated Object: farm laborer\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: male with Generated Object: spouse\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: male with Generated Object: judy flaming\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: male with Generated Object: william lewis reece\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: male with Generated Object: perpetrator\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: male with Generated Object: murders, kidnapping and rape\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasethnolinguisticdescriptor with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: anglo-african with Generated Object: william lewis reece\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasethnolinguisticdescriptor with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: anglo-african with Generated Object: william lewis reece\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasethnolinguisticdescriptor with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: anglo-african with Generated Object: reece\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasethnolinguisticdescriptor with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: anglo-african with Generated Object: william lewis\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasethnolinguisticdescriptor with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: anglo-african with Generated Object: american serial killer, rapist and kidnapper. in 2015, he was linked via dna to the 1997 cold case murder of a woman in oklahoma, for which he was subsequently convicted and sentenced to death. not long after, he confessed to three murders associated with the texas killing fields, for which he was convicted and sentenced to life imprisonment in 2022.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasethnolinguisticdescriptor with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: anglo-african with Generated Object: oklahoma\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasethnolinguisticdescriptor with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: anglo-african with Generated Object: july 1, 1959\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasethnolinguisticdescriptor with Generated Predicate: hassex\n",
            "Comparing Truth Object: anglo-african with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasethnolinguisticdescriptor with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: anglo-african with Generated Object: conviction for murder, kidnapping and rape\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasethnolinguisticdescriptor with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: anglo-african with Generated Object: convicted criminal\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasethnolinguisticdescriptor with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: anglo-african with Generated Object: farm laborer\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasethnolinguisticdescriptor with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: anglo-african with Generated Object: spouse\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasethnolinguisticdescriptor with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: anglo-african with Generated Object: judy flaming\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasethnolinguisticdescriptor with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: anglo-african with Generated Object: william lewis reece\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasethnolinguisticdescriptor with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: anglo-african with Generated Object: perpetrator\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasethnolinguisticdescriptor with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: anglo-african with Generated Object: murders, kidnapping and rape\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasraceorcolor with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: mulatto with Generated Object: william lewis reece\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasraceorcolor with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: mulatto with Generated Object: william lewis reece\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasraceorcolor with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: mulatto with Generated Object: reece\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasraceorcolor with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: mulatto with Generated Object: william lewis\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasraceorcolor with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: mulatto with Generated Object: american serial killer, rapist and kidnapper. in 2015, he was linked via dna to the 1997 cold case murder of a woman in oklahoma, for which he was subsequently convicted and sentenced to death. not long after, he confessed to three murders associated with the texas killing fields, for which he was convicted and sentenced to life imprisonment in 2022.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasraceorcolor with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: mulatto with Generated Object: oklahoma\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasraceorcolor with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: mulatto with Generated Object: july 1, 1959\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasraceorcolor with Generated Predicate: hassex\n",
            "Comparing Truth Object: mulatto with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasraceorcolor with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: mulatto with Generated Object: conviction for murder, kidnapping and rape\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasraceorcolor with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: mulatto with Generated Object: convicted criminal\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasraceorcolor with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: mulatto with Generated Object: farm laborer\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasraceorcolor with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: mulatto with Generated Object: spouse\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasraceorcolor with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: mulatto with Generated Object: judy flaming\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasraceorcolor with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: mulatto with Generated Object: william lewis reece\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasraceorcolor with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: mulatto with Generated Object: perpetrator\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasraceorcolor with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: mulatto with Generated Object: murders, kidnapping and rape\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: william with Generated Object: william lewis reece\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: william with Generated Object: william lewis reece\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: william with Generated Object: reece\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: william with Generated Object: william lewis\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: william with Generated Object: american serial killer, rapist and kidnapper. in 2015, he was linked via dna to the 1997 cold case murder of a woman in oklahoma, for which he was subsequently convicted and sentenced to death. not long after, he confessed to three murders associated with the texas killing fields, for which he was convicted and sentenced to life imprisonment in 2022.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: william with Generated Object: oklahoma\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: william with Generated Object: july 1, 1959\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hassex\n",
            "Comparing Truth Object: william with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: william with Generated Object: conviction for murder, kidnapping and rape\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: william with Generated Object: convicted criminal\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: william with Generated Object: farm laborer\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: william with Generated Object: spouse\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: william with Generated Object: judy flaming\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: william with Generated Object: william lewis reece\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: william with Generated Object: perpetrator\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: william with Generated Object: murders, kidnapping and rape\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: lewis with Generated Object: william lewis reece\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: lewis with Generated Object: william lewis reece\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: lewis with Generated Object: reece\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: lewis with Generated Object: william lewis\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: lewis with Generated Object: american serial killer, rapist and kidnapper. in 2015, he was linked via dna to the 1997 cold case murder of a woman in oklahoma, for which he was subsequently convicted and sentenced to death. not long after, he confessed to three murders associated with the texas killing fields, for which he was convicted and sentenced to life imprisonment in 2022.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: lewis with Generated Object: oklahoma\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: lewis with Generated Object: july 1, 1959\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hassex\n",
            "Comparing Truth Object: lewis with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: lewis with Generated Object: conviction for murder, kidnapping and rape\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: lewis with Generated Object: convicted criminal\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: lewis with Generated Object: farm laborer\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: lewis with Generated Object: spouse\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: lewis with Generated Object: judy flaming\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: lewis with Generated Object: william lewis reece\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: lewis with Generated Object: perpetrator\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: lewis with Generated Object: murders, kidnapping and rape\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasoccupation with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: maritime with Generated Object: william lewis reece\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasoccupation with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: maritime with Generated Object: william lewis reece\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasoccupation with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: maritime with Generated Object: reece\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasoccupation with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: maritime with Generated Object: william lewis\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasoccupation with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: maritime with Generated Object: american serial killer, rapist and kidnapper. in 2015, he was linked via dna to the 1997 cold case murder of a woman in oklahoma, for which he was subsequently convicted and sentenced to death. not long after, he confessed to three murders associated with the texas killing fields, for which he was convicted and sentenced to life imprisonment in 2022.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasoccupation with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: maritime with Generated Object: oklahoma\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasoccupation with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: maritime with Generated Object: july 1, 1959\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasoccupation with Generated Predicate: hassex\n",
            "Comparing Truth Object: maritime with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasoccupation with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: maritime with Generated Object: conviction for murder, kidnapping and rape\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasoccupation with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: maritime with Generated Object: convicted criminal\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasoccupation with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: maritime with Generated Object: farm laborer\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasoccupation with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: maritime with Generated Object: spouse\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasoccupation with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: maritime with Generated Object: judy flaming\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasoccupation with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: maritime with Generated Object: william lewis reece\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasoccupation with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: maritime with Generated Object: perpetrator\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasoccupation with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: maritime with Generated Object: murders, kidnapping and rape\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasage with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: age 38 with Generated Object: william lewis reece\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasage with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: age 38 with Generated Object: william lewis reece\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasage with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: age 38 with Generated Object: reece\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasage with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: age 38 with Generated Object: william lewis\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasage with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: age 38 with Generated Object: american serial killer, rapist and kidnapper. in 2015, he was linked via dna to the 1997 cold case murder of a woman in oklahoma, for which he was subsequently convicted and sentenced to death. not long after, he confessed to three murders associated with the texas killing fields, for which he was convicted and sentenced to life imprisonment in 2022.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasage with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: age 38 with Generated Object: oklahoma\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasage with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: age 38 with Generated Object: july 1, 1959\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasage with Generated Predicate: hassex\n",
            "Comparing Truth Object: age 38 with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasage with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: age 38 with Generated Object: conviction for murder, kidnapping and rape\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasage with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: age 38 with Generated Object: convicted criminal\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasage with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: age 38 with Generated Object: farm laborer\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasage with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: age 38 with Generated Object: spouse\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasage with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: age 38 with Generated Object: judy flaming\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasage with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: age 38 with Generated Object: william lewis reece\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasage with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: age 38 with Generated Object: perpetrator\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasage with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: age 38 with Generated Object: murders, kidnapping and rape\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: child with Generated Object: william lewis reece\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: child with Generated Object: william lewis reece\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: child with Generated Object: reece\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: child with Generated Object: william lewis\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: child with Generated Object: american serial killer, rapist and kidnapper. in 2015, he was linked via dna to the 1997 cold case murder of a woman in oklahoma, for which he was subsequently convicted and sentenced to death. not long after, he confessed to three murders associated with the texas killing fields, for which he was convicted and sentenced to life imprisonment in 2022.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: child with Generated Object: oklahoma\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: child with Generated Object: july 1, 1959\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hassex\n",
            "Comparing Truth Object: child with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: child with Generated Object: conviction for murder, kidnapping and rape\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: child with Generated Object: convicted criminal\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: child with Generated Object: farm laborer\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: child with Generated Object: spouse\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: child with Generated Object: judy flaming\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: child with Generated Object: william lewis reece\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: child with Generated Object: perpetrator\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: child with Generated Object: murders, kidnapping and rape\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: relocated person with Generated Object: william lewis reece\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: relocated person with Generated Object: william lewis reece\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: relocated person with Generated Object: reece\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: relocated person with Generated Object: william lewis\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: relocated person with Generated Object: american serial killer, rapist and kidnapper. in 2015, he was linked via dna to the 1997 cold case murder of a woman in oklahoma, for which he was subsequently convicted and sentenced to death. not long after, he confessed to three murders associated with the texas killing fields, for which he was convicted and sentenced to life imprisonment in 2022.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: relocated person with Generated Object: oklahoma\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: relocated person with Generated Object: july 1, 1959\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hassex\n",
            "Comparing Truth Object: relocated person with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: relocated person with Generated Object: conviction for murder, kidnapping and rape\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: relocated person with Generated Object: convicted criminal\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: relocated person with Generated Object: farm laborer\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: relocated person with Generated Object: spouse\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: relocated person with Generated Object: judy flaming\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: relocated person with Generated Object: william lewis reece\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: relocated person with Generated Object: perpetrator\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: relocated person with Generated Object: murders, kidnapping and rape\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: registered person with Generated Object: william lewis reece\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: registered person with Generated Object: william lewis reece\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: registered person with Generated Object: reece\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: registered person with Generated Object: william lewis\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: registered person with Generated Object: american serial killer, rapist and kidnapper. in 2015, he was linked via dna to the 1997 cold case murder of a woman in oklahoma, for which he was subsequently convicted and sentenced to death. not long after, he confessed to three murders associated with the texas killing fields, for which he was convicted and sentenced to life imprisonment in 2022.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: registered person with Generated Object: oklahoma\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: registered person with Generated Object: july 1, 1959\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hassex\n",
            "Comparing Truth Object: registered person with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: registered person with Generated Object: conviction for murder, kidnapping and rape\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: registered person with Generated Object: convicted criminal\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: registered person with Generated Object: farm laborer\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: registered person with Generated Object: spouse\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: registered person with Generated Object: judy flaming\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: registered person with Generated Object: william lewis reece\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: registered person with Generated Object: perpetrator\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: registered person with Generated Object: murders, kidnapping and rape\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: person with Generated Object: william lewis reece\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: person with Generated Object: william lewis reece\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: person with Generated Object: reece\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: person with Generated Object: william lewis\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: person with Generated Object: american serial killer, rapist and kidnapper. in 2015, he was linked via dna to the 1997 cold case murder of a woman in oklahoma, for which he was subsequently convicted and sentenced to death. not long after, he confessed to three murders associated with the texas killing fields, for which he was convicted and sentenced to life imprisonment in 2022.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: person with Generated Object: oklahoma\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: person with Generated Object: july 1, 1959\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hassex\n",
            "Comparing Truth Object: person with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: person with Generated Object: conviction for murder, kidnapping and rape\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: person with Generated Object: convicted criminal\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: person with Generated Object: farm laborer\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: person with Generated Object: spouse\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: person with Generated Object: judy flaming\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: person with Generated Object: william lewis reece\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: person with Generated Object: perpetrator\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: person with Generated Object: murders, kidnapping and rape\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results saved to: /content/drive/My Drive/LLM-Data/evaluate-test/GPT4_Enslaved_MainAgent/William_Lewis_triples_evaluation.csv\n",
            "\n",
            "Processing truth file: David_Abner.tsv with generated file: David_Abner_triples.tsv\n",
            "Number of truth triples: 14\n",
            "Number of generated triples: 10\n",
            "Comparing Truth Predicate: instance of with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: person with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: person with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: person with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: person with Generated Object: abner was the cousin of king saul and the commander-in-chief of his army.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: person with Generated Object: commander-in-chief of saul's army\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: person with Generated Object: cousin\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: person with Generated Object: saul\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: person with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: person with Generated Object: commander\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: person with Generated Object: battle of gibeon\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: male with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: male with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: male with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: male with Generated Object: abner was the cousin of king saul and the commander-in-chief of his army.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: male with Generated Object: commander-in-chief of saul's army\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: male with Generated Object: cousin\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: male with Generated Object: saul\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: male with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: male with Generated Object: commander\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: male with Generated Object: battle of gibeon\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: child with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: child with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: child with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: child with Generated Object: abner was the cousin of king saul and the commander-in-chief of his army.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: child with Generated Object: commander-in-chief of saul's army\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: child with Generated Object: cousin\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: child with Generated Object: saul\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: child with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: child with Generated Object: commander\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: child with Generated Object: battle of gibeon\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: deceased person with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: deceased person with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: deceased person with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: deceased person with Generated Object: abner was the cousin of king saul and the commander-in-chief of his army.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: deceased person with Generated Object: commander-in-chief of saul's army\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: deceased person with Generated Object: cousin\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: deceased person with Generated Object: saul\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: deceased person with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: deceased person with Generated Object: commander\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: deceased person with Generated Object: battle of gibeon\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: farmer with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: farmer with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: farmer with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: farmer with Generated Object: abner was the cousin of king saul and the commander-in-chief of his army.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: farmer with Generated Object: commander-in-chief of saul's army\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: farmer with Generated Object: cousin\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: farmer with Generated Object: saul\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: farmer with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: farmer with Generated Object: commander\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: farmer with Generated Object: battle of gibeon\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: entrepreneur with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: entrepreneur with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: entrepreneur with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: entrepreneur with Generated Object: abner was the cousin of king saul and the commander-in-chief of his army.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: entrepreneur with Generated Object: commander-in-chief of saul's army\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: entrepreneur with Generated Object: cousin\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: entrepreneur with Generated Object: saul\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: entrepreneur with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: entrepreneur with Generated Object: commander\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: entrepreneur with Generated Object: battle of gibeon\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: slave with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: slave with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: slave with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: slave with Generated Object: abner was the cousin of king saul and the commander-in-chief of his army.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: slave with Generated Object: commander-in-chief of saul's army\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: slave with Generated Object: cousin\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: slave with Generated Object: saul\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: slave with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: slave with Generated Object: commander\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: slave with Generated Object: battle of gibeon\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: state legislator with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: state legislator with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: state legislator with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: state legislator with Generated Object: abner was the cousin of king saul and the commander-in-chief of his army.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: state legislator with Generated Object: commander-in-chief of saul's army\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: state legislator with Generated Object: cousin\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: state legislator with Generated Object: saul\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: state legislator with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: state legislator with Generated Object: commander\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: state legislator with Generated Object: battle of gibeon\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: building materials industry leader with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: building materials industry leader with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: building materials industry leader with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: building materials industry leader with Generated Object: abner was the cousin of king saul and the commander-in-chief of his army.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: building materials industry leader with Generated Object: commander-in-chief of saul's army\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: building materials industry leader with Generated Object: cousin\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: building materials industry leader with Generated Object: saul\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: building materials industry leader with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: building materials industry leader with Generated Object: commander\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: building materials industry leader with Generated Object: battle of gibeon\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: baptist clergy with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: baptist clergy with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: baptist clergy with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: baptist clergy with Generated Object: abner was the cousin of king saul and the commander-in-chief of his army.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: baptist clergy with Generated Object: commander-in-chief of saul's army\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: baptist clergy with Generated Object: cousin\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: baptist clergy with Generated Object: saul\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: baptist clergy with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: baptist clergy with Generated Object: commander\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: baptist clergy with Generated Object: battle of gibeon\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: david with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: david with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: david with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: david with Generated Object: abner was the cousin of king saul and the commander-in-chief of his army.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: david with Generated Object: commander-in-chief of saul's army\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: david with Generated Object: cousin\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: david with Generated Object: saul\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: david with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: david with Generated Object: commander\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: david with Generated Object: battle of gibeon\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: abner with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: abner with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: abner with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: abner with Generated Object: abner was the cousin of king saul and the commander-in-chief of his army.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: abner with Generated Object: commander-in-chief of saul's army\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: abner with Generated Object: cousin\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: abner with Generated Object: saul\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: abner with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: abner with Generated Object: commander\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: abner with Generated Object: battle of gibeon\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: david abner with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: david abner with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: david abner with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: david abner with Generated Object: abner was the cousin of king saul and the commander-in-chief of his army.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: david abner with Generated Object: commander-in-chief of saul's army\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: david abner with Generated Object: cousin\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: david abner with Generated Object: saul\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: david abner with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: david abner with Generated Object: commander\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: david abner with Generated Object: battle of gibeon\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: born enslaved. formerly enslaved in alabama and texas, abner served a term in the texas state legislature in 1874 and was a delegate to the state constitutional convention the following year. with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: born enslaved. formerly enslaved in alabama and texas, abner served a term in the texas state legislature in 1874 and was a delegate to the state constitutional convention the following year. with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: born enslaved. formerly enslaved in alabama and texas, abner served a term in the texas state legislature in 1874 and was a delegate to the state constitutional convention the following year. with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: born enslaved. formerly enslaved in alabama and texas, abner served a term in the texas state legislature in 1874 and was a delegate to the state constitutional convention the following year. with Generated Object: abner was the cousin of king saul and the commander-in-chief of his army.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: born enslaved. formerly enslaved in alabama and texas, abner served a term in the texas state legislature in 1874 and was a delegate to the state constitutional convention the following year. with Generated Object: commander-in-chief of saul's army\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: born enslaved. formerly enslaved in alabama and texas, abner served a term in the texas state legislature in 1874 and was a delegate to the state constitutional convention the following year. with Generated Object: cousin\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: born enslaved. formerly enslaved in alabama and texas, abner served a term in the texas state legislature in 1874 and was a delegate to the state constitutional convention the following year. with Generated Object: saul\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: born enslaved. formerly enslaved in alabama and texas, abner served a term in the texas state legislature in 1874 and was a delegate to the state constitutional convention the following year. with Generated Object: abner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: born enslaved. formerly enslaved in alabama and texas, abner served a term in the texas state legislature in 1874 and was a delegate to the state constitutional convention the following year. with Generated Object: commander\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: born enslaved. formerly enslaved in alabama and texas, abner served a term in the texas state legislature in 1874 and was a delegate to the state constitutional convention the following year. with Generated Object: battle of gibeon\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results saved to: /content/drive/My Drive/LLM-Data/evaluate-test/GPT4_Enslaved_MainAgent/David_Abner_triples_evaluation.csv\n",
            "\n",
            "Processing truth file: William_Harvey_Carney.tsv with generated file: William_Harvey_Carney_triples.tsv\n",
            "Number of truth triples: 11\n",
            "Number of generated triples: 17\n",
            "Comparing Truth Predicate: instance of with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: person with Generated Object: william harvey carney\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: person with Generated Object: william harvey carney\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: person with Generated Object: carney\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: person with Generated Object: william harvey\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: person with Generated Object: american soldier during the american civil war. born enslaved, he was awarded the medal of honor in 1900 for his gallantry in saving the regimental colors during the battle of fort wagner in 1863.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: person with Generated Object: norfolk, virginia\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: has_race\n",
            "Comparing Truth Object: person with Generated Object: african-american\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: person with Generated Object: february 29, 1840\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasdeathdate\n",
            "Comparing Truth Object: person with Generated Object: december 9, 1908\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hassex\n",
            "Comparing Truth Object: person with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: person with Generated Object: battle of fort wagner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: person with Generated Object: soldier, medal of honor recipient\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: person with Generated Object: soldier, mailman\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: person with Generated Object: susannah williams\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: person with Generated Object: william harvey carney\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: person with Generated Object: soldier\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: person with Generated Object: battle of fort wagner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: male with Generated Object: william harvey carney\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: male with Generated Object: william harvey carney\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: male with Generated Object: carney\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: male with Generated Object: william harvey\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: male with Generated Object: american soldier during the american civil war. born enslaved, he was awarded the medal of honor in 1900 for his gallantry in saving the regimental colors during the battle of fort wagner in 1863.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: male with Generated Object: norfolk, virginia\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: has_race\n",
            "Comparing Truth Object: male with Generated Object: african-american\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: male with Generated Object: february 29, 1840\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasdeathdate\n",
            "Comparing Truth Object: male with Generated Object: december 9, 1908\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hassex\n",
            "Comparing Truth Object: male with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: male with Generated Object: battle of fort wagner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: male with Generated Object: soldier, medal of honor recipient\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: male with Generated Object: soldier, mailman\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: male with Generated Object: susannah williams\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: male with Generated Object: william harvey carney\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: male with Generated Object: soldier\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: male with Generated Object: battle of fort wagner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: child with Generated Object: william harvey carney\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: child with Generated Object: william harvey carney\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: child with Generated Object: carney\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: child with Generated Object: william harvey\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: child with Generated Object: american soldier during the american civil war. born enslaved, he was awarded the medal of honor in 1900 for his gallantry in saving the regimental colors during the battle of fort wagner in 1863.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: child with Generated Object: norfolk, virginia\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: has_race\n",
            "Comparing Truth Object: child with Generated Object: african-american\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: child with Generated Object: february 29, 1840\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasdeathdate\n",
            "Comparing Truth Object: child with Generated Object: december 9, 1908\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hassex\n",
            "Comparing Truth Object: child with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: child with Generated Object: battle of fort wagner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: child with Generated Object: soldier, medal of honor recipient\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: child with Generated Object: soldier, mailman\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: child with Generated Object: susannah williams\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: child with Generated Object: william harvey carney\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: child with Generated Object: soldier\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: child with Generated Object: battle of fort wagner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: deceased person with Generated Object: william harvey carney\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: deceased person with Generated Object: william harvey carney\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: deceased person with Generated Object: carney\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: deceased person with Generated Object: william harvey\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: deceased person with Generated Object: american soldier during the american civil war. born enslaved, he was awarded the medal of honor in 1900 for his gallantry in saving the regimental colors during the battle of fort wagner in 1863.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: deceased person with Generated Object: norfolk, virginia\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: has_race\n",
            "Comparing Truth Object: deceased person with Generated Object: african-american\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: deceased person with Generated Object: february 29, 1840\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasdeathdate\n",
            "Comparing Truth Object: deceased person with Generated Object: december 9, 1908\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hassex\n",
            "Comparing Truth Object: deceased person with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: deceased person with Generated Object: battle of fort wagner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: deceased person with Generated Object: soldier, medal of honor recipient\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: deceased person with Generated Object: soldier, mailman\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: deceased person with Generated Object: susannah williams\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: deceased person with Generated Object: william harvey carney\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: deceased person with Generated Object: soldier\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: deceased person with Generated Object: battle of fort wagner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: union army officer with Generated Object: william harvey carney\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: union army officer with Generated Object: william harvey carney\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: union army officer with Generated Object: carney\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: union army officer with Generated Object: william harvey\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: union army officer with Generated Object: american soldier during the american civil war. born enslaved, he was awarded the medal of honor in 1900 for his gallantry in saving the regimental colors during the battle of fort wagner in 1863.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: union army officer with Generated Object: norfolk, virginia\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: has_race\n",
            "Comparing Truth Object: union army officer with Generated Object: african-american\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: union army officer with Generated Object: february 29, 1840\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasdeathdate\n",
            "Comparing Truth Object: union army officer with Generated Object: december 9, 1908\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hassex\n",
            "Comparing Truth Object: union army officer with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: union army officer with Generated Object: battle of fort wagner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: union army officer with Generated Object: soldier, medal of honor recipient\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: union army officer with Generated Object: soldier, mailman\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: union army officer with Generated Object: susannah williams\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: union army officer with Generated Object: william harvey carney\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: union army officer with Generated Object: soldier\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: union army officer with Generated Object: battle of fort wagner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: medal of honor recipient with Generated Object: william harvey carney\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: medal of honor recipient with Generated Object: william harvey carney\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: medal of honor recipient with Generated Object: carney\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: medal of honor recipient with Generated Object: william harvey\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: medal of honor recipient with Generated Object: american soldier during the american civil war. born enslaved, he was awarded the medal of honor in 1900 for his gallantry in saving the regimental colors during the battle of fort wagner in 1863.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: medal of honor recipient with Generated Object: norfolk, virginia\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: has_race\n",
            "Comparing Truth Object: medal of honor recipient with Generated Object: african-american\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: medal of honor recipient with Generated Object: february 29, 1840\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasdeathdate\n",
            "Comparing Truth Object: medal of honor recipient with Generated Object: december 9, 1908\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hassex\n",
            "Comparing Truth Object: medal of honor recipient with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: medal of honor recipient with Generated Object: battle of fort wagner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: medal of honor recipient with Generated Object: soldier, medal of honor recipient\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: medal of honor recipient with Generated Object: soldier, mailman\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: medal of honor recipient with Generated Object: susannah williams\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: medal of honor recipient with Generated Object: william harvey carney\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: medal of honor recipient with Generated Object: soldier\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: medal of honor recipient with Generated Object: battle of fort wagner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: war hero with Generated Object: william harvey carney\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: war hero with Generated Object: william harvey carney\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: war hero with Generated Object: carney\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: war hero with Generated Object: william harvey\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: war hero with Generated Object: american soldier during the american civil war. born enslaved, he was awarded the medal of honor in 1900 for his gallantry in saving the regimental colors during the battle of fort wagner in 1863.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: war hero with Generated Object: norfolk, virginia\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: has_race\n",
            "Comparing Truth Object: war hero with Generated Object: african-american\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: war hero with Generated Object: february 29, 1840\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasdeathdate\n",
            "Comparing Truth Object: war hero with Generated Object: december 9, 1908\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hassex\n",
            "Comparing Truth Object: war hero with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: war hero with Generated Object: battle of fort wagner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: war hero with Generated Object: soldier, medal of honor recipient\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: war hero with Generated Object: soldier, mailman\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: war hero with Generated Object: susannah williams\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: war hero with Generated Object: william harvey carney\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: war hero with Generated Object: soldier\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: war hero with Generated Object: battle of fort wagner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: william harvey with Generated Object: william harvey carney\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: william harvey with Generated Object: william harvey carney\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: william harvey with Generated Object: carney\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: william harvey with Generated Object: william harvey\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: william harvey with Generated Object: american soldier during the american civil war. born enslaved, he was awarded the medal of honor in 1900 for his gallantry in saving the regimental colors during the battle of fort wagner in 1863.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: william harvey with Generated Object: norfolk, virginia\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: has_race\n",
            "Comparing Truth Object: william harvey with Generated Object: african-american\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: william harvey with Generated Object: february 29, 1840\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasdeathdate\n",
            "Comparing Truth Object: william harvey with Generated Object: december 9, 1908\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hassex\n",
            "Comparing Truth Object: william harvey with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: william harvey with Generated Object: battle of fort wagner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: william harvey with Generated Object: soldier, medal of honor recipient\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: william harvey with Generated Object: soldier, mailman\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: william harvey with Generated Object: susannah williams\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: william harvey with Generated Object: william harvey carney\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: william harvey with Generated Object: soldier\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: william harvey with Generated Object: battle of fort wagner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: carney with Generated Object: william harvey carney\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: carney with Generated Object: william harvey carney\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: carney with Generated Object: carney\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: carney with Generated Object: william harvey\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: carney with Generated Object: american soldier during the american civil war. born enslaved, he was awarded the medal of honor in 1900 for his gallantry in saving the regimental colors during the battle of fort wagner in 1863.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: carney with Generated Object: norfolk, virginia\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: has_race\n",
            "Comparing Truth Object: carney with Generated Object: african-american\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: carney with Generated Object: february 29, 1840\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasdeathdate\n",
            "Comparing Truth Object: carney with Generated Object: december 9, 1908\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hassex\n",
            "Comparing Truth Object: carney with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: carney with Generated Object: battle of fort wagner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: carney with Generated Object: soldier, medal of honor recipient\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: carney with Generated Object: soldier, mailman\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: carney with Generated Object: susannah williams\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: carney with Generated Object: william harvey carney\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: carney with Generated Object: soldier\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: carney with Generated Object: battle of fort wagner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: william harvey carney with Generated Object: william harvey carney\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: william harvey carney with Generated Object: william harvey carney\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: william harvey carney with Generated Object: carney\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: william harvey carney with Generated Object: william harvey\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: william harvey carney with Generated Object: american soldier during the american civil war. born enslaved, he was awarded the medal of honor in 1900 for his gallantry in saving the regimental colors during the battle of fort wagner in 1863.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: william harvey carney with Generated Object: norfolk, virginia\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: has_race\n",
            "Comparing Truth Object: william harvey carney with Generated Object: african-american\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: william harvey carney with Generated Object: february 29, 1840\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasdeathdate\n",
            "Comparing Truth Object: william harvey carney with Generated Object: december 9, 1908\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hassex\n",
            "Comparing Truth Object: william harvey carney with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: william harvey carney with Generated Object: battle of fort wagner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: william harvey carney with Generated Object: soldier, medal of honor recipient\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: william harvey carney with Generated Object: soldier, mailman\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: william harvey carney with Generated Object: susannah williams\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: william harvey carney with Generated Object: william harvey carney\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: william harvey carney with Generated Object: soldier\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: william harvey carney with Generated Object: battle of fort wagner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: free before 13th amendment. with Generated Object: william harvey carney\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: free before 13th amendment. with Generated Object: william harvey carney\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: free before 13th amendment. with Generated Object: carney\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: free before 13th amendment. with Generated Object: william harvey\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: free before 13th amendment. with Generated Object: american soldier during the american civil war. born enslaved, he was awarded the medal of honor in 1900 for his gallantry in saving the regimental colors during the battle of fort wagner in 1863.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: free before 13th amendment. with Generated Object: norfolk, virginia\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: has_race\n",
            "Comparing Truth Object: free before 13th amendment. with Generated Object: african-american\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: free before 13th amendment. with Generated Object: february 29, 1840\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasdeathdate\n",
            "Comparing Truth Object: free before 13th amendment. with Generated Object: december 9, 1908\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hassex\n",
            "Comparing Truth Object: free before 13th amendment. with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: free before 13th amendment. with Generated Object: battle of fort wagner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: free before 13th amendment. with Generated Object: soldier, medal of honor recipient\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: free before 13th amendment. with Generated Object: soldier, mailman\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: free before 13th amendment. with Generated Object: susannah williams\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: free before 13th amendment. with Generated Object: william harvey carney\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: free before 13th amendment. with Generated Object: soldier\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: free before 13th amendment. with Generated Object: battle of fort wagner\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results saved to: /content/drive/My Drive/LLM-Data/evaluate-test/GPT4_Enslaved_MainAgent/William_Harvey_Carney_triples_evaluation.csv\n",
            "\n",
            "Processing truth file: Thomas_Sims.tsv with generated file: Thomas_Sims_triples.tsv\n",
            "Number of truth triples: 11\n",
            "Number of generated triples: 14\n",
            "Comparing Truth Predicate: instance of with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: person with Generated Object: thomas sims\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: person with Generated Object: thomas sims\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: person with Generated Object: sims\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: person with Generated Object: thomas\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: person with Generated Object: african american who escaped from slavery in georgia and fled to boston, massachusetts, in 1851. he was arrested the same year under the fugitive slave act of 1850, had a court hearing, and was forced to return to enslavement. a second escape brought him back to boston in 1863, where he was later appointed to a position in the u.s. department of justice in 1877.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: person with Generated Object: savannah, georgia\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: has_race\n",
            "Comparing Truth Object: person with Generated Object: african-american\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hassex\n",
            "Comparing Truth Object: person with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: person with Generated Object: escape from slavery, arrest under the fugitive slave act, return to enslavement, second escape, appointment to the u.s. department of justice\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: person with Generated Object: slave, fugitive, employee of the u.s. department of justice\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: person with Generated Object: employee of the u.s. department of justice\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: person with Generated Object: slave\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: person with Generated Object: james potter\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: person with Generated Object: thomas sims\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: male with Generated Object: thomas sims\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: male with Generated Object: thomas sims\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: male with Generated Object: sims\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: male with Generated Object: thomas\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: male with Generated Object: african american who escaped from slavery in georgia and fled to boston, massachusetts, in 1851. he was arrested the same year under the fugitive slave act of 1850, had a court hearing, and was forced to return to enslavement. a second escape brought him back to boston in 1863, where he was later appointed to a position in the u.s. department of justice in 1877.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: male with Generated Object: savannah, georgia\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: has_race\n",
            "Comparing Truth Object: male with Generated Object: african-american\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hassex\n",
            "Comparing Truth Object: male with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: male with Generated Object: escape from slavery, arrest under the fugitive slave act, return to enslavement, second escape, appointment to the u.s. department of justice\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: male with Generated Object: slave, fugitive, employee of the u.s. department of justice\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: male with Generated Object: employee of the u.s. department of justice\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: male with Generated Object: slave\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: male with Generated Object: james potter\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: male with Generated Object: thomas sims\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: child with Generated Object: thomas sims\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: child with Generated Object: thomas sims\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: child with Generated Object: sims\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: child with Generated Object: thomas\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: child with Generated Object: african american who escaped from slavery in georgia and fled to boston, massachusetts, in 1851. he was arrested the same year under the fugitive slave act of 1850, had a court hearing, and was forced to return to enslavement. a second escape brought him back to boston in 1863, where he was later appointed to a position in the u.s. department of justice in 1877.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: child with Generated Object: savannah, georgia\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: has_race\n",
            "Comparing Truth Object: child with Generated Object: african-american\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hassex\n",
            "Comparing Truth Object: child with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: child with Generated Object: escape from slavery, arrest under the fugitive slave act, return to enslavement, second escape, appointment to the u.s. department of justice\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: child with Generated Object: slave, fugitive, employee of the u.s. department of justice\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: child with Generated Object: employee of the u.s. department of justice\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: child with Generated Object: slave\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: child with Generated Object: james potter\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: child with Generated Object: thomas sims\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: deceased person with Generated Object: thomas sims\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: deceased person with Generated Object: thomas sims\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: deceased person with Generated Object: sims\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: deceased person with Generated Object: thomas\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: deceased person with Generated Object: african american who escaped from slavery in georgia and fled to boston, massachusetts, in 1851. he was arrested the same year under the fugitive slave act of 1850, had a court hearing, and was forced to return to enslavement. a second escape brought him back to boston in 1863, where he was later appointed to a position in the u.s. department of justice in 1877.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: deceased person with Generated Object: savannah, georgia\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: has_race\n",
            "Comparing Truth Object: deceased person with Generated Object: african-american\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hassex\n",
            "Comparing Truth Object: deceased person with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: deceased person with Generated Object: escape from slavery, arrest under the fugitive slave act, return to enslavement, second escape, appointment to the u.s. department of justice\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: deceased person with Generated Object: slave, fugitive, employee of the u.s. department of justice\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: deceased person with Generated Object: employee of the u.s. department of justice\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: deceased person with Generated Object: slave\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: deceased person with Generated Object: james potter\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: deceased person with Generated Object: thomas sims\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: slave with Generated Object: thomas sims\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: slave with Generated Object: thomas sims\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: slave with Generated Object: sims\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: slave with Generated Object: thomas\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: slave with Generated Object: african american who escaped from slavery in georgia and fled to boston, massachusetts, in 1851. he was arrested the same year under the fugitive slave act of 1850, had a court hearing, and was forced to return to enslavement. a second escape brought him back to boston in 1863, where he was later appointed to a position in the u.s. department of justice in 1877.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: slave with Generated Object: savannah, georgia\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: has_race\n",
            "Comparing Truth Object: slave with Generated Object: african-american\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hassex\n",
            "Comparing Truth Object: slave with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: slave with Generated Object: escape from slavery, arrest under the fugitive slave act, return to enslavement, second escape, appointment to the u.s. department of justice\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: slave with Generated Object: slave, fugitive, employee of the u.s. department of justice\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: slave with Generated Object: employee of the u.s. department of justice\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: slave with Generated Object: slave\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: slave with Generated Object: james potter\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: slave with Generated Object: thomas sims\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: litigant with Generated Object: thomas sims\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: litigant with Generated Object: thomas sims\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: litigant with Generated Object: sims\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: litigant with Generated Object: thomas\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: litigant with Generated Object: african american who escaped from slavery in georgia and fled to boston, massachusetts, in 1851. he was arrested the same year under the fugitive slave act of 1850, had a court hearing, and was forced to return to enslavement. a second escape brought him back to boston in 1863, where he was later appointed to a position in the u.s. department of justice in 1877.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: litigant with Generated Object: savannah, georgia\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: has_race\n",
            "Comparing Truth Object: litigant with Generated Object: african-american\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hassex\n",
            "Comparing Truth Object: litigant with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: litigant with Generated Object: escape from slavery, arrest under the fugitive slave act, return to enslavement, second escape, appointment to the u.s. department of justice\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: litigant with Generated Object: slave, fugitive, employee of the u.s. department of justice\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: litigant with Generated Object: employee of the u.s. department of justice\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: litigant with Generated Object: slave\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: litigant with Generated Object: james potter\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: litigant with Generated Object: thomas sims\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: fugitive slave with Generated Object: thomas sims\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: fugitive slave with Generated Object: thomas sims\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: fugitive slave with Generated Object: sims\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: fugitive slave with Generated Object: thomas\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: fugitive slave with Generated Object: african american who escaped from slavery in georgia and fled to boston, massachusetts, in 1851. he was arrested the same year under the fugitive slave act of 1850, had a court hearing, and was forced to return to enslavement. a second escape brought him back to boston in 1863, where he was later appointed to a position in the u.s. department of justice in 1877.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: fugitive slave with Generated Object: savannah, georgia\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: has_race\n",
            "Comparing Truth Object: fugitive slave with Generated Object: african-american\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hassex\n",
            "Comparing Truth Object: fugitive slave with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: fugitive slave with Generated Object: escape from slavery, arrest under the fugitive slave act, return to enslavement, second escape, appointment to the u.s. department of justice\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: fugitive slave with Generated Object: slave, fugitive, employee of the u.s. department of justice\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: fugitive slave with Generated Object: employee of the u.s. department of justice\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: fugitive slave with Generated Object: slave\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: fugitive slave with Generated Object: james potter\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: fugitive slave with Generated Object: thomas sims\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: thomas with Generated Object: thomas sims\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: thomas with Generated Object: thomas sims\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: thomas with Generated Object: sims\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: thomas with Generated Object: thomas\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: thomas with Generated Object: african american who escaped from slavery in georgia and fled to boston, massachusetts, in 1851. he was arrested the same year under the fugitive slave act of 1850, had a court hearing, and was forced to return to enslavement. a second escape brought him back to boston in 1863, where he was later appointed to a position in the u.s. department of justice in 1877.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: thomas with Generated Object: savannah, georgia\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: has_race\n",
            "Comparing Truth Object: thomas with Generated Object: african-american\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hassex\n",
            "Comparing Truth Object: thomas with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: thomas with Generated Object: escape from slavery, arrest under the fugitive slave act, return to enslavement, second escape, appointment to the u.s. department of justice\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: thomas with Generated Object: slave, fugitive, employee of the u.s. department of justice\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: thomas with Generated Object: employee of the u.s. department of justice\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: thomas with Generated Object: slave\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: thomas with Generated Object: james potter\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: thomas with Generated Object: thomas sims\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: sims with Generated Object: thomas sims\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: sims with Generated Object: thomas sims\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: sims with Generated Object: sims\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: sims with Generated Object: thomas\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: sims with Generated Object: african american who escaped from slavery in georgia and fled to boston, massachusetts, in 1851. he was arrested the same year under the fugitive slave act of 1850, had a court hearing, and was forced to return to enslavement. a second escape brought him back to boston in 1863, where he was later appointed to a position in the u.s. department of justice in 1877.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: sims with Generated Object: savannah, georgia\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: has_race\n",
            "Comparing Truth Object: sims with Generated Object: african-american\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hassex\n",
            "Comparing Truth Object: sims with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: sims with Generated Object: escape from slavery, arrest under the fugitive slave act, return to enslavement, second escape, appointment to the u.s. department of justice\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: sims with Generated Object: slave, fugitive, employee of the u.s. department of justice\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: sims with Generated Object: employee of the u.s. department of justice\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: sims with Generated Object: slave\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: sims with Generated Object: james potter\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: sims with Generated Object: thomas sims\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: thomas sims with Generated Object: thomas sims\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: thomas sims with Generated Object: thomas sims\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: thomas sims with Generated Object: sims\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: thomas sims with Generated Object: thomas\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: thomas sims with Generated Object: african american who escaped from slavery in georgia and fled to boston, massachusetts, in 1851. he was arrested the same year under the fugitive slave act of 1850, had a court hearing, and was forced to return to enslavement. a second escape brought him back to boston in 1863, where he was later appointed to a position in the u.s. department of justice in 1877.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: thomas sims with Generated Object: savannah, georgia\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: has_race\n",
            "Comparing Truth Object: thomas sims with Generated Object: african-american\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hassex\n",
            "Comparing Truth Object: thomas sims with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: thomas sims with Generated Object: escape from slavery, arrest under the fugitive slave act, return to enslavement, second escape, appointment to the u.s. department of justice\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: thomas sims with Generated Object: slave, fugitive, employee of the u.s. department of justice\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: thomas sims with Generated Object: employee of the u.s. department of justice\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: thomas sims with Generated Object: slave\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: thomas sims with Generated Object: james potter\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: thomas sims with Generated Object: thomas sims\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: enslaved. sims became widely known during an unsuccessful effort by abolitionists to prevent his re-enslavement under the fugitive slave act. after the war he worked as a mason and held various patronage jobs. with Generated Object: thomas sims\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: enslaved. sims became widely known during an unsuccessful effort by abolitionists to prevent his re-enslavement under the fugitive slave act. after the war he worked as a mason and held various patronage jobs. with Generated Object: thomas sims\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: enslaved. sims became widely known during an unsuccessful effort by abolitionists to prevent his re-enslavement under the fugitive slave act. after the war he worked as a mason and held various patronage jobs. with Generated Object: sims\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: enslaved. sims became widely known during an unsuccessful effort by abolitionists to prevent his re-enslavement under the fugitive slave act. after the war he worked as a mason and held various patronage jobs. with Generated Object: thomas\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: enslaved. sims became widely known during an unsuccessful effort by abolitionists to prevent his re-enslavement under the fugitive slave act. after the war he worked as a mason and held various patronage jobs. with Generated Object: african american who escaped from slavery in georgia and fled to boston, massachusetts, in 1851. he was arrested the same year under the fugitive slave act of 1850, had a court hearing, and was forced to return to enslavement. a second escape brought him back to boston in 1863, where he was later appointed to a position in the u.s. department of justice in 1877.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: enslaved. sims became widely known during an unsuccessful effort by abolitionists to prevent his re-enslavement under the fugitive slave act. after the war he worked as a mason and held various patronage jobs. with Generated Object: savannah, georgia\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: has_race\n",
            "Comparing Truth Object: enslaved. sims became widely known during an unsuccessful effort by abolitionists to prevent his re-enslavement under the fugitive slave act. after the war he worked as a mason and held various patronage jobs. with Generated Object: african-american\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hassex\n",
            "Comparing Truth Object: enslaved. sims became widely known during an unsuccessful effort by abolitionists to prevent his re-enslavement under the fugitive slave act. after the war he worked as a mason and held various patronage jobs. with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: enslaved. sims became widely known during an unsuccessful effort by abolitionists to prevent his re-enslavement under the fugitive slave act. after the war he worked as a mason and held various patronage jobs. with Generated Object: escape from slavery, arrest under the fugitive slave act, return to enslavement, second escape, appointment to the u.s. department of justice\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: enslaved. sims became widely known during an unsuccessful effort by abolitionists to prevent his re-enslavement under the fugitive slave act. after the war he worked as a mason and held various patronage jobs. with Generated Object: slave, fugitive, employee of the u.s. department of justice\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: enslaved. sims became widely known during an unsuccessful effort by abolitionists to prevent his re-enslavement under the fugitive slave act. after the war he worked as a mason and held various patronage jobs. with Generated Object: employee of the u.s. department of justice\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: enslaved. sims became widely known during an unsuccessful effort by abolitionists to prevent his re-enslavement under the fugitive slave act. after the war he worked as a mason and held various patronage jobs. with Generated Object: slave\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: enslaved. sims became widely known during an unsuccessful effort by abolitionists to prevent his re-enslavement under the fugitive slave act. after the war he worked as a mason and held various patronage jobs. with Generated Object: james potter\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: enslaved. sims became widely known during an unsuccessful effort by abolitionists to prevent his re-enslavement under the fugitive slave act. after the war he worked as a mason and held various patronage jobs. with Generated Object: thomas sims\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results saved to: /content/drive/My Drive/LLM-Data/evaluate-test/GPT4_Enslaved_MainAgent/Thomas_Sims_triples_evaluation.csv\n",
            "\n",
            "Processing truth file: John_Edward_Bruce.tsv with generated file: John_Edward_Bruce_triples.tsv\n",
            "Number of truth triples: 12\n",
            "Number of generated triples: 17\n",
            "Comparing Truth Predicate: instance of with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: person with Generated Object: john edward bruce\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: person with Generated Object: john edward bruce\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: person with Generated Object: bruce\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: person with Generated Object: john edward\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: person with Generated Object: american journalist, historian, writer, orator, civil rights activist and pan-african nationalist. co-founder of the negro society for historical research.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: person with Generated Object: piscataway, maryland\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: has_race\n",
            "Comparing Truth Object: person with Generated Object: african-american\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: person with Generated Object: february 22, 1856\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasdeathdate\n",
            "Comparing Truth Object: person with Generated Object: august 7, 1924\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hassex\n",
            "Comparing Truth Object: person with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: person with Generated Object: founding of numerous newspapers, co-founding of the negro society for historical research\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: person with Generated Object: journalist, historian, writer, orator, civil rights activist, pan-african nationalist\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: person with Generated Object: journalist, historian, writer, orator, civil rights activist, pan-african nationalist\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: person with Generated Object: arthur alfonso schomburg\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: person with Generated Object: john edward bruce\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: person with Generated Object: co-founder\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: person with Generated Object: founding of the negro society for historical research\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: male with Generated Object: john edward bruce\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: male with Generated Object: john edward bruce\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: male with Generated Object: bruce\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: male with Generated Object: john edward\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: male with Generated Object: american journalist, historian, writer, orator, civil rights activist and pan-african nationalist. co-founder of the negro society for historical research.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: male with Generated Object: piscataway, maryland\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: has_race\n",
            "Comparing Truth Object: male with Generated Object: african-american\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: male with Generated Object: february 22, 1856\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasdeathdate\n",
            "Comparing Truth Object: male with Generated Object: august 7, 1924\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hassex\n",
            "Comparing Truth Object: male with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: male with Generated Object: founding of numerous newspapers, co-founding of the negro society for historical research\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: male with Generated Object: journalist, historian, writer, orator, civil rights activist, pan-african nationalist\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: male with Generated Object: journalist, historian, writer, orator, civil rights activist, pan-african nationalist\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: male with Generated Object: arthur alfonso schomburg\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: male with Generated Object: john edward bruce\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: male with Generated Object: co-founder\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: male with Generated Object: founding of the negro society for historical research\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: child with Generated Object: john edward bruce\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: child with Generated Object: john edward bruce\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: child with Generated Object: bruce\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: child with Generated Object: john edward\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: child with Generated Object: american journalist, historian, writer, orator, civil rights activist and pan-african nationalist. co-founder of the negro society for historical research.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: child with Generated Object: piscataway, maryland\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: has_race\n",
            "Comparing Truth Object: child with Generated Object: african-american\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: child with Generated Object: february 22, 1856\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasdeathdate\n",
            "Comparing Truth Object: child with Generated Object: august 7, 1924\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hassex\n",
            "Comparing Truth Object: child with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: child with Generated Object: founding of numerous newspapers, co-founding of the negro society for historical research\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: child with Generated Object: journalist, historian, writer, orator, civil rights activist, pan-african nationalist\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: child with Generated Object: journalist, historian, writer, orator, civil rights activist, pan-african nationalist\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: child with Generated Object: arthur alfonso schomburg\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: child with Generated Object: john edward bruce\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: child with Generated Object: co-founder\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: child with Generated Object: founding of the negro society for historical research\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: deceased person with Generated Object: john edward bruce\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: deceased person with Generated Object: john edward bruce\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: deceased person with Generated Object: bruce\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: deceased person with Generated Object: john edward\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: deceased person with Generated Object: american journalist, historian, writer, orator, civil rights activist and pan-african nationalist. co-founder of the negro society for historical research.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: deceased person with Generated Object: piscataway, maryland\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: has_race\n",
            "Comparing Truth Object: deceased person with Generated Object: african-american\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: deceased person with Generated Object: february 22, 1856\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasdeathdate\n",
            "Comparing Truth Object: deceased person with Generated Object: august 7, 1924\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hassex\n",
            "Comparing Truth Object: deceased person with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: deceased person with Generated Object: founding of numerous newspapers, co-founding of the negro society for historical research\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: deceased person with Generated Object: journalist, historian, writer, orator, civil rights activist, pan-african nationalist\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: deceased person with Generated Object: journalist, historian, writer, orator, civil rights activist, pan-african nationalist\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: deceased person with Generated Object: arthur alfonso schomburg\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: deceased person with Generated Object: john edward bruce\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: deceased person with Generated Object: co-founder\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: deceased person with Generated Object: founding of the negro society for historical research\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: slave with Generated Object: john edward bruce\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: slave with Generated Object: john edward bruce\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: slave with Generated Object: bruce\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: slave with Generated Object: john edward\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: slave with Generated Object: american journalist, historian, writer, orator, civil rights activist and pan-african nationalist. co-founder of the negro society for historical research.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: slave with Generated Object: piscataway, maryland\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: has_race\n",
            "Comparing Truth Object: slave with Generated Object: african-american\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: slave with Generated Object: february 22, 1856\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasdeathdate\n",
            "Comparing Truth Object: slave with Generated Object: august 7, 1924\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hassex\n",
            "Comparing Truth Object: slave with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: slave with Generated Object: founding of numerous newspapers, co-founding of the negro society for historical research\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: slave with Generated Object: journalist, historian, writer, orator, civil rights activist, pan-african nationalist\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: slave with Generated Object: journalist, historian, writer, orator, civil rights activist, pan-african nationalist\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: slave with Generated Object: arthur alfonso schomburg\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: slave with Generated Object: john edward bruce\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: slave with Generated Object: co-founder\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: slave with Generated Object: founding of the negro society for historical research\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: civil rights activist with Generated Object: john edward bruce\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: civil rights activist with Generated Object: john edward bruce\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: civil rights activist with Generated Object: bruce\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: civil rights activist with Generated Object: john edward\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: civil rights activist with Generated Object: american journalist, historian, writer, orator, civil rights activist and pan-african nationalist. co-founder of the negro society for historical research.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: civil rights activist with Generated Object: piscataway, maryland\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: has_race\n",
            "Comparing Truth Object: civil rights activist with Generated Object: african-american\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: civil rights activist with Generated Object: february 22, 1856\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasdeathdate\n",
            "Comparing Truth Object: civil rights activist with Generated Object: august 7, 1924\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hassex\n",
            "Comparing Truth Object: civil rights activist with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: civil rights activist with Generated Object: founding of numerous newspapers, co-founding of the negro society for historical research\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: civil rights activist with Generated Object: journalist, historian, writer, orator, civil rights activist, pan-african nationalist\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: civil rights activist with Generated Object: journalist, historian, writer, orator, civil rights activist, pan-african nationalist\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: civil rights activist with Generated Object: arthur alfonso schomburg\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: civil rights activist with Generated Object: john edward bruce\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: civil rights activist with Generated Object: co-founder\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: civil rights activist with Generated Object: founding of the negro society for historical research\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: historian with Generated Object: john edward bruce\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: historian with Generated Object: john edward bruce\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: historian with Generated Object: bruce\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: historian with Generated Object: john edward\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: historian with Generated Object: american journalist, historian, writer, orator, civil rights activist and pan-african nationalist. co-founder of the negro society for historical research.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: historian with Generated Object: piscataway, maryland\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: has_race\n",
            "Comparing Truth Object: historian with Generated Object: african-american\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: historian with Generated Object: february 22, 1856\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasdeathdate\n",
            "Comparing Truth Object: historian with Generated Object: august 7, 1924\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hassex\n",
            "Comparing Truth Object: historian with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: historian with Generated Object: founding of numerous newspapers, co-founding of the negro society for historical research\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: historian with Generated Object: journalist, historian, writer, orator, civil rights activist, pan-african nationalist\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: historian with Generated Object: journalist, historian, writer, orator, civil rights activist, pan-african nationalist\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: historian with Generated Object: arthur alfonso schomburg\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: historian with Generated Object: john edward bruce\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: historian with Generated Object: co-founder\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: historian with Generated Object: founding of the negro society for historical research\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: print journalist with Generated Object: john edward bruce\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: print journalist with Generated Object: john edward bruce\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: print journalist with Generated Object: bruce\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: print journalist with Generated Object: john edward\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: print journalist with Generated Object: american journalist, historian, writer, orator, civil rights activist and pan-african nationalist. co-founder of the negro society for historical research.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: print journalist with Generated Object: piscataway, maryland\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: has_race\n",
            "Comparing Truth Object: print journalist with Generated Object: african-american\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: print journalist with Generated Object: february 22, 1856\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasdeathdate\n",
            "Comparing Truth Object: print journalist with Generated Object: august 7, 1924\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hassex\n",
            "Comparing Truth Object: print journalist with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: print journalist with Generated Object: founding of numerous newspapers, co-founding of the negro society for historical research\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: print journalist with Generated Object: journalist, historian, writer, orator, civil rights activist, pan-african nationalist\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: print journalist with Generated Object: journalist, historian, writer, orator, civil rights activist, pan-african nationalist\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: print journalist with Generated Object: arthur alfonso schomburg\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: print journalist with Generated Object: john edward bruce\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: print journalist with Generated Object: co-founder\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: print journalist with Generated Object: founding of the negro society for historical research\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: john edward with Generated Object: john edward bruce\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: john edward with Generated Object: john edward bruce\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: john edward with Generated Object: bruce\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: john edward with Generated Object: john edward\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: john edward with Generated Object: american journalist, historian, writer, orator, civil rights activist and pan-african nationalist. co-founder of the negro society for historical research.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: john edward with Generated Object: piscataway, maryland\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: has_race\n",
            "Comparing Truth Object: john edward with Generated Object: african-american\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: john edward with Generated Object: february 22, 1856\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasdeathdate\n",
            "Comparing Truth Object: john edward with Generated Object: august 7, 1924\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hassex\n",
            "Comparing Truth Object: john edward with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: john edward with Generated Object: founding of numerous newspapers, co-founding of the negro society for historical research\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: john edward with Generated Object: journalist, historian, writer, orator, civil rights activist, pan-african nationalist\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: john edward with Generated Object: journalist, historian, writer, orator, civil rights activist, pan-african nationalist\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: john edward with Generated Object: arthur alfonso schomburg\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: john edward with Generated Object: john edward bruce\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: john edward with Generated Object: co-founder\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: john edward with Generated Object: founding of the negro society for historical research\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: bruce with Generated Object: john edward bruce\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: bruce with Generated Object: john edward bruce\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: bruce with Generated Object: bruce\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: bruce with Generated Object: john edward\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: bruce with Generated Object: american journalist, historian, writer, orator, civil rights activist and pan-african nationalist. co-founder of the negro society for historical research.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: bruce with Generated Object: piscataway, maryland\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: has_race\n",
            "Comparing Truth Object: bruce with Generated Object: african-american\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: bruce with Generated Object: february 22, 1856\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasdeathdate\n",
            "Comparing Truth Object: bruce with Generated Object: august 7, 1924\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hassex\n",
            "Comparing Truth Object: bruce with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: bruce with Generated Object: founding of numerous newspapers, co-founding of the negro society for historical research\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: bruce with Generated Object: journalist, historian, writer, orator, civil rights activist, pan-african nationalist\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: bruce with Generated Object: journalist, historian, writer, orator, civil rights activist, pan-african nationalist\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: bruce with Generated Object: arthur alfonso schomburg\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: bruce with Generated Object: john edward bruce\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: bruce with Generated Object: co-founder\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: bruce with Generated Object: founding of the negro society for historical research\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: john edward bruce with Generated Object: john edward bruce\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: john edward bruce with Generated Object: john edward bruce\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: john edward bruce with Generated Object: bruce\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: john edward bruce with Generated Object: john edward\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: john edward bruce with Generated Object: american journalist, historian, writer, orator, civil rights activist and pan-african nationalist. co-founder of the negro society for historical research.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: john edward bruce with Generated Object: piscataway, maryland\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: has_race\n",
            "Comparing Truth Object: john edward bruce with Generated Object: african-american\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: john edward bruce with Generated Object: february 22, 1856\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasdeathdate\n",
            "Comparing Truth Object: john edward bruce with Generated Object: august 7, 1924\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hassex\n",
            "Comparing Truth Object: john edward bruce with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: john edward bruce with Generated Object: founding of numerous newspapers, co-founding of the negro society for historical research\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: john edward bruce with Generated Object: journalist, historian, writer, orator, civil rights activist, pan-african nationalist\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: john edward bruce with Generated Object: journalist, historian, writer, orator, civil rights activist, pan-african nationalist\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: john edward bruce with Generated Object: arthur alfonso schomburg\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: john edward bruce with Generated Object: john edward bruce\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: john edward bruce with Generated Object: co-founder\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: john edward bruce with Generated Object: founding of the negro society for historical research\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: born enslaved. a newspaper columnist and author who was one of the most widely read african american journalists during his life. with Generated Object: john edward bruce\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: born enslaved. a newspaper columnist and author who was one of the most widely read african american journalists during his life. with Generated Object: john edward bruce\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: born enslaved. a newspaper columnist and author who was one of the most widely read african american journalists during his life. with Generated Object: bruce\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: born enslaved. a newspaper columnist and author who was one of the most widely read african american journalists during his life. with Generated Object: john edward\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: born enslaved. a newspaper columnist and author who was one of the most widely read african american journalists during his life. with Generated Object: american journalist, historian, writer, orator, civil rights activist and pan-african nationalist. co-founder of the negro society for historical research.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: born enslaved. a newspaper columnist and author who was one of the most widely read african american journalists during his life. with Generated Object: piscataway, maryland\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: has_race\n",
            "Comparing Truth Object: born enslaved. a newspaper columnist and author who was one of the most widely read african american journalists during his life. with Generated Object: african-american\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: born enslaved. a newspaper columnist and author who was one of the most widely read african american journalists during his life. with Generated Object: february 22, 1856\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasdeathdate\n",
            "Comparing Truth Object: born enslaved. a newspaper columnist and author who was one of the most widely read african american journalists during his life. with Generated Object: august 7, 1924\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hassex\n",
            "Comparing Truth Object: born enslaved. a newspaper columnist and author who was one of the most widely read african american journalists during his life. with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: born enslaved. a newspaper columnist and author who was one of the most widely read african american journalists during his life. with Generated Object: founding of numerous newspapers, co-founding of the negro society for historical research\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: born enslaved. a newspaper columnist and author who was one of the most widely read african american journalists during his life. with Generated Object: journalist, historian, writer, orator, civil rights activist, pan-african nationalist\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: born enslaved. a newspaper columnist and author who was one of the most widely read african american journalists during his life. with Generated Object: journalist, historian, writer, orator, civil rights activist, pan-african nationalist\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: born enslaved. a newspaper columnist and author who was one of the most widely read african american journalists during his life. with Generated Object: arthur alfonso schomburg\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: born enslaved. a newspaper columnist and author who was one of the most widely read african american journalists during his life. with Generated Object: john edward bruce\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: born enslaved. a newspaper columnist and author who was one of the most widely read african american journalists during his life. with Generated Object: co-founder\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: born enslaved. a newspaper columnist and author who was one of the most widely read african american journalists during his life. with Generated Object: founding of the negro society for historical research\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results saved to: /content/drive/My Drive/LLM-Data/evaluate-test/GPT4_Enslaved_MainAgent/John_Edward_Bruce_triples_evaluation.csv\n",
            "\n",
            "Processing truth file: James_Collins_Johnson.tsv with generated file: James_Collins_Johnson_triples.tsv\n",
            "Number of truth triples: 11\n",
            "Number of generated triples: 16\n",
            "Comparing Truth Predicate: instance of with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: person with Generated Object: philip james collins\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: person with Generated Object: philip james collins\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: person with Generated Object: collins\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: person with Generated Object: philip james\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: person with Generated Object: british journalist, academic, banker and speechwriter\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: person with Generated Object: bury in greater manchester\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: person with Generated Object: 16 may 1967\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hassex\n",
            "Comparing Truth Object: person with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: person with Generated Object: career in journalism, academia, banking, and speechwriting\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: person with Generated Object: journalist, academic, banker, speechwriter\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: person with Generated Object: journalist, academic, banker, speechwriter\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: person with Generated Object: spouse\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: person with Generated Object: geeta guru-murthy\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: person with Generated Object: philip james collins\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: person with Generated Object: columnist, contributing writer\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: person with Generated Object: writing for the times and new statesman\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: male with Generated Object: philip james collins\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: male with Generated Object: philip james collins\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: male with Generated Object: collins\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: male with Generated Object: philip james\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: male with Generated Object: british journalist, academic, banker and speechwriter\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: male with Generated Object: bury in greater manchester\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: male with Generated Object: 16 may 1967\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hassex\n",
            "Comparing Truth Object: male with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: male with Generated Object: career in journalism, academia, banking, and speechwriting\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: male with Generated Object: journalist, academic, banker, speechwriter\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: male with Generated Object: journalist, academic, banker, speechwriter\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: male with Generated Object: spouse\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: male with Generated Object: geeta guru-murthy\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: male with Generated Object: philip james collins\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: male with Generated Object: columnist, contributing writer\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: male with Generated Object: writing for the times and new statesman\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: child with Generated Object: philip james collins\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: child with Generated Object: philip james collins\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: child with Generated Object: collins\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: child with Generated Object: philip james\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: child with Generated Object: british journalist, academic, banker and speechwriter\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: child with Generated Object: bury in greater manchester\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: child with Generated Object: 16 may 1967\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hassex\n",
            "Comparing Truth Object: child with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: child with Generated Object: career in journalism, academia, banking, and speechwriting\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: child with Generated Object: journalist, academic, banker, speechwriter\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: child with Generated Object: journalist, academic, banker, speechwriter\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: child with Generated Object: spouse\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: child with Generated Object: geeta guru-murthy\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: child with Generated Object: philip james collins\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: child with Generated Object: columnist, contributing writer\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: child with Generated Object: writing for the times and new statesman\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: deceased person with Generated Object: philip james collins\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: deceased person with Generated Object: philip james collins\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: deceased person with Generated Object: collins\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: deceased person with Generated Object: philip james\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: deceased person with Generated Object: british journalist, academic, banker and speechwriter\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: deceased person with Generated Object: bury in greater manchester\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: deceased person with Generated Object: 16 may 1967\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hassex\n",
            "Comparing Truth Object: deceased person with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: deceased person with Generated Object: career in journalism, academia, banking, and speechwriting\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: deceased person with Generated Object: journalist, academic, banker, speechwriter\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: deceased person with Generated Object: journalist, academic, banker, speechwriter\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: deceased person with Generated Object: spouse\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: deceased person with Generated Object: geeta guru-murthy\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: deceased person with Generated Object: philip james collins\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: deceased person with Generated Object: columnist, contributing writer\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: deceased person with Generated Object: writing for the times and new statesman\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: entrepreneur with Generated Object: philip james collins\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: entrepreneur with Generated Object: philip james collins\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: entrepreneur with Generated Object: collins\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: entrepreneur with Generated Object: philip james\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: entrepreneur with Generated Object: british journalist, academic, banker and speechwriter\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: entrepreneur with Generated Object: bury in greater manchester\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: entrepreneur with Generated Object: 16 may 1967\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hassex\n",
            "Comparing Truth Object: entrepreneur with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: entrepreneur with Generated Object: career in journalism, academia, banking, and speechwriting\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: entrepreneur with Generated Object: journalist, academic, banker, speechwriter\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: entrepreneur with Generated Object: journalist, academic, banker, speechwriter\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: entrepreneur with Generated Object: spouse\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: entrepreneur with Generated Object: geeta guru-murthy\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: entrepreneur with Generated Object: philip james collins\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: entrepreneur with Generated Object: columnist, contributing writer\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: entrepreneur with Generated Object: writing for the times and new statesman\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: slave with Generated Object: philip james collins\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: slave with Generated Object: philip james collins\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: slave with Generated Object: collins\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: slave with Generated Object: philip james\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: slave with Generated Object: british journalist, academic, banker and speechwriter\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: slave with Generated Object: bury in greater manchester\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: slave with Generated Object: 16 may 1967\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hassex\n",
            "Comparing Truth Object: slave with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: slave with Generated Object: career in journalism, academia, banking, and speechwriting\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: slave with Generated Object: journalist, academic, banker, speechwriter\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: slave with Generated Object: journalist, academic, banker, speechwriter\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: slave with Generated Object: spouse\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: slave with Generated Object: geeta guru-murthy\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: slave with Generated Object: philip james collins\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: slave with Generated Object: columnist, contributing writer\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: slave with Generated Object: writing for the times and new statesman\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: merchant with Generated Object: philip james collins\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: merchant with Generated Object: philip james collins\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: merchant with Generated Object: collins\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: merchant with Generated Object: philip james\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: merchant with Generated Object: british journalist, academic, banker and speechwriter\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: merchant with Generated Object: bury in greater manchester\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: merchant with Generated Object: 16 may 1967\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hassex\n",
            "Comparing Truth Object: merchant with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: merchant with Generated Object: career in journalism, academia, banking, and speechwriting\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: merchant with Generated Object: journalist, academic, banker, speechwriter\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: merchant with Generated Object: journalist, academic, banker, speechwriter\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: merchant with Generated Object: spouse\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: merchant with Generated Object: geeta guru-murthy\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: merchant with Generated Object: philip james collins\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: merchant with Generated Object: columnist, contributing writer\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: merchant with Generated Object: writing for the times and new statesman\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: james collins with Generated Object: philip james collins\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: james collins with Generated Object: philip james collins\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: james collins with Generated Object: collins\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: james collins with Generated Object: philip james\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: james collins with Generated Object: british journalist, academic, banker and speechwriter\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: james collins with Generated Object: bury in greater manchester\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: james collins with Generated Object: 16 may 1967\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hassex\n",
            "Comparing Truth Object: james collins with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: james collins with Generated Object: career in journalism, academia, banking, and speechwriting\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: james collins with Generated Object: journalist, academic, banker, speechwriter\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: james collins with Generated Object: journalist, academic, banker, speechwriter\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: james collins with Generated Object: spouse\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: james collins with Generated Object: geeta guru-murthy\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: james collins with Generated Object: philip james collins\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: james collins with Generated Object: columnist, contributing writer\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: james collins with Generated Object: writing for the times and new statesman\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: johnson with Generated Object: philip james collins\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: johnson with Generated Object: philip james collins\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: johnson with Generated Object: collins\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: johnson with Generated Object: philip james\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: johnson with Generated Object: british journalist, academic, banker and speechwriter\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: johnson with Generated Object: bury in greater manchester\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: johnson with Generated Object: 16 may 1967\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hassex\n",
            "Comparing Truth Object: johnson with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: johnson with Generated Object: career in journalism, academia, banking, and speechwriting\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: johnson with Generated Object: journalist, academic, banker, speechwriter\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: johnson with Generated Object: journalist, academic, banker, speechwriter\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: johnson with Generated Object: spouse\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: johnson with Generated Object: geeta guru-murthy\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: johnson with Generated Object: philip james collins\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: johnson with Generated Object: columnist, contributing writer\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: johnson with Generated Object: writing for the times and new statesman\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: james collins johnson with Generated Object: philip james collins\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: james collins johnson with Generated Object: philip james collins\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: james collins johnson with Generated Object: collins\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: james collins johnson with Generated Object: philip james\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: james collins johnson with Generated Object: british journalist, academic, banker and speechwriter\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: james collins johnson with Generated Object: bury in greater manchester\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: james collins johnson with Generated Object: 16 may 1967\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hassex\n",
            "Comparing Truth Object: james collins johnson with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: james collins johnson with Generated Object: career in journalism, academia, banking, and speechwriting\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: james collins johnson with Generated Object: journalist, academic, banker, speechwriter\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: james collins johnson with Generated Object: journalist, academic, banker, speechwriter\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: james collins johnson with Generated Object: spouse\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: james collins johnson with Generated Object: geeta guru-murthy\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: james collins johnson with Generated Object: philip james collins\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: james collins johnson with Generated Object: columnist, contributing writer\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: james collins johnson with Generated Object: writing for the times and new statesman\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: born enslaved. free before 13th amendment. tried in 1843 in what became known as the princeton fugitive slave case, johnson was nearly re-enslaved until a local woman intervened and bought his freedom. with Generated Object: philip james collins\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: born enslaved. free before 13th amendment. tried in 1843 in what became known as the princeton fugitive slave case, johnson was nearly re-enslaved until a local woman intervened and bought his freedom. with Generated Object: philip james collins\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: born enslaved. free before 13th amendment. tried in 1843 in what became known as the princeton fugitive slave case, johnson was nearly re-enslaved until a local woman intervened and bought his freedom. with Generated Object: collins\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: born enslaved. free before 13th amendment. tried in 1843 in what became known as the princeton fugitive slave case, johnson was nearly re-enslaved until a local woman intervened and bought his freedom. with Generated Object: philip james\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: born enslaved. free before 13th amendment. tried in 1843 in what became known as the princeton fugitive slave case, johnson was nearly re-enslaved until a local woman intervened and bought his freedom. with Generated Object: british journalist, academic, banker and speechwriter\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: born enslaved. free before 13th amendment. tried in 1843 in what became known as the princeton fugitive slave case, johnson was nearly re-enslaved until a local woman intervened and bought his freedom. with Generated Object: bury in greater manchester\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: born enslaved. free before 13th amendment. tried in 1843 in what became known as the princeton fugitive slave case, johnson was nearly re-enslaved until a local woman intervened and bought his freedom. with Generated Object: 16 may 1967\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hassex\n",
            "Comparing Truth Object: born enslaved. free before 13th amendment. tried in 1843 in what became known as the princeton fugitive slave case, johnson was nearly re-enslaved until a local woman intervened and bought his freedom. with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: born enslaved. free before 13th amendment. tried in 1843 in what became known as the princeton fugitive slave case, johnson was nearly re-enslaved until a local woman intervened and bought his freedom. with Generated Object: career in journalism, academia, banking, and speechwriting\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: born enslaved. free before 13th amendment. tried in 1843 in what became known as the princeton fugitive slave case, johnson was nearly re-enslaved until a local woman intervened and bought his freedom. with Generated Object: journalist, academic, banker, speechwriter\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: born enslaved. free before 13th amendment. tried in 1843 in what became known as the princeton fugitive slave case, johnson was nearly re-enslaved until a local woman intervened and bought his freedom. with Generated Object: journalist, academic, banker, speechwriter\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: born enslaved. free before 13th amendment. tried in 1843 in what became known as the princeton fugitive slave case, johnson was nearly re-enslaved until a local woman intervened and bought his freedom. with Generated Object: spouse\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: born enslaved. free before 13th amendment. tried in 1843 in what became known as the princeton fugitive slave case, johnson was nearly re-enslaved until a local woman intervened and bought his freedom. with Generated Object: geeta guru-murthy\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: born enslaved. free before 13th amendment. tried in 1843 in what became known as the princeton fugitive slave case, johnson was nearly re-enslaved until a local woman intervened and bought his freedom. with Generated Object: philip james collins\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: born enslaved. free before 13th amendment. tried in 1843 in what became known as the princeton fugitive slave case, johnson was nearly re-enslaved until a local woman intervened and bought his freedom. with Generated Object: columnist, contributing writer\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: born enslaved. free before 13th amendment. tried in 1843 in what became known as the princeton fugitive slave case, johnson was nearly re-enslaved until a local woman intervened and bought his freedom. with Generated Object: writing for the times and new statesman\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results saved to: /content/drive/My Drive/LLM-Data/evaluate-test/GPT4_Enslaved_MainAgent/James_Collins_Johnson_triples_evaluation.csv\n",
            "\n",
            "Processing truth file: Stephen_Bishop.tsv with generated file: Stephen_Bishop_(cave_explorer)_triples.tsv\n",
            "Number of truth triples: 11\n",
            "Number of generated triples: 18\n",
            "Comparing Truth Predicate: instance of with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: person with Generated Object: stephen bishop\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: person with Generated Object: stephen bishop\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: person with Generated Object: bishop\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: person with Generated Object: stephen\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: person with Generated Object: american cave explorer and self-taught geologist known for being one of the first people to explore and map mammoth cave in the u.s. state of kentucky.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: person with Generated Object: u.s. state of kentucky\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: has_race\n",
            "Comparing Truth Object: person with Generated Object: african-american\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: person with Generated Object: c. 1821\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasdeathdate\n",
            "Comparing Truth Object: person with Generated Object: 1857\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hassex\n",
            "Comparing Truth Object: person with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: person with Generated Object: manumission\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: person with Generated Object: enslaved, freed\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: person with Generated Object: cave explorer, geologist, guide at mammoth cave\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: person with Generated Object: enslaved by\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: person with Generated Object: franklin gorin\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: person with Generated Object: stephen bishop\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: person with Generated Object: explorer\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: person with Generated Object: exploration of mammoth cave\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: male with Generated Object: stephen bishop\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: male with Generated Object: stephen bishop\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: male with Generated Object: bishop\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: male with Generated Object: stephen\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: male with Generated Object: american cave explorer and self-taught geologist known for being one of the first people to explore and map mammoth cave in the u.s. state of kentucky.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: male with Generated Object: u.s. state of kentucky\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: has_race\n",
            "Comparing Truth Object: male with Generated Object: african-american\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: male with Generated Object: c. 1821\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasdeathdate\n",
            "Comparing Truth Object: male with Generated Object: 1857\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hassex\n",
            "Comparing Truth Object: male with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: male with Generated Object: manumission\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: male with Generated Object: enslaved, freed\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: male with Generated Object: cave explorer, geologist, guide at mammoth cave\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: male with Generated Object: enslaved by\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: male with Generated Object: franklin gorin\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: male with Generated Object: stephen bishop\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: male with Generated Object: explorer\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: male with Generated Object: exploration of mammoth cave\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: child with Generated Object: stephen bishop\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: child with Generated Object: stephen bishop\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: child with Generated Object: bishop\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: child with Generated Object: stephen\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: child with Generated Object: american cave explorer and self-taught geologist known for being one of the first people to explore and map mammoth cave in the u.s. state of kentucky.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: child with Generated Object: u.s. state of kentucky\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: has_race\n",
            "Comparing Truth Object: child with Generated Object: african-american\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: child with Generated Object: c. 1821\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasdeathdate\n",
            "Comparing Truth Object: child with Generated Object: 1857\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hassex\n",
            "Comparing Truth Object: child with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: child with Generated Object: manumission\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: child with Generated Object: enslaved, freed\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: child with Generated Object: cave explorer, geologist, guide at mammoth cave\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: child with Generated Object: enslaved by\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: child with Generated Object: franklin gorin\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: child with Generated Object: stephen bishop\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: child with Generated Object: explorer\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: child with Generated Object: exploration of mammoth cave\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: deceased person with Generated Object: stephen bishop\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: deceased person with Generated Object: stephen bishop\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: deceased person with Generated Object: bishop\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: deceased person with Generated Object: stephen\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: deceased person with Generated Object: american cave explorer and self-taught geologist known for being one of the first people to explore and map mammoth cave in the u.s. state of kentucky.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: deceased person with Generated Object: u.s. state of kentucky\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: has_race\n",
            "Comparing Truth Object: deceased person with Generated Object: african-american\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: deceased person with Generated Object: c. 1821\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasdeathdate\n",
            "Comparing Truth Object: deceased person with Generated Object: 1857\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hassex\n",
            "Comparing Truth Object: deceased person with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: deceased person with Generated Object: manumission\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: deceased person with Generated Object: enslaved, freed\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: deceased person with Generated Object: cave explorer, geologist, guide at mammoth cave\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: deceased person with Generated Object: enslaved by\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: deceased person with Generated Object: franklin gorin\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: deceased person with Generated Object: stephen bishop\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: deceased person with Generated Object: explorer\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: deceased person with Generated Object: exploration of mammoth cave\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: slave with Generated Object: stephen bishop\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: slave with Generated Object: stephen bishop\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: slave with Generated Object: bishop\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: slave with Generated Object: stephen\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: slave with Generated Object: american cave explorer and self-taught geologist known for being one of the first people to explore and map mammoth cave in the u.s. state of kentucky.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: slave with Generated Object: u.s. state of kentucky\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: has_race\n",
            "Comparing Truth Object: slave with Generated Object: african-american\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: slave with Generated Object: c. 1821\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasdeathdate\n",
            "Comparing Truth Object: slave with Generated Object: 1857\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hassex\n",
            "Comparing Truth Object: slave with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: slave with Generated Object: manumission\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: slave with Generated Object: enslaved, freed\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: slave with Generated Object: cave explorer, geologist, guide at mammoth cave\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: slave with Generated Object: enslaved by\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: slave with Generated Object: franklin gorin\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: slave with Generated Object: stephen bishop\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: slave with Generated Object: explorer\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: slave with Generated Object: exploration of mammoth cave\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: guide with Generated Object: stephen bishop\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: guide with Generated Object: stephen bishop\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: guide with Generated Object: bishop\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: guide with Generated Object: stephen\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: guide with Generated Object: american cave explorer and self-taught geologist known for being one of the first people to explore and map mammoth cave in the u.s. state of kentucky.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: guide with Generated Object: u.s. state of kentucky\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: has_race\n",
            "Comparing Truth Object: guide with Generated Object: african-american\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: guide with Generated Object: c. 1821\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasdeathdate\n",
            "Comparing Truth Object: guide with Generated Object: 1857\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hassex\n",
            "Comparing Truth Object: guide with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: guide with Generated Object: manumission\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: guide with Generated Object: enslaved, freed\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: guide with Generated Object: cave explorer, geologist, guide at mammoth cave\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: guide with Generated Object: enslaved by\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: guide with Generated Object: franklin gorin\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: guide with Generated Object: stephen bishop\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: guide with Generated Object: explorer\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: guide with Generated Object: exploration of mammoth cave\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: cartographer with Generated Object: stephen bishop\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: cartographer with Generated Object: stephen bishop\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: cartographer with Generated Object: bishop\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: cartographer with Generated Object: stephen\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: cartographer with Generated Object: american cave explorer and self-taught geologist known for being one of the first people to explore and map mammoth cave in the u.s. state of kentucky.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: cartographer with Generated Object: u.s. state of kentucky\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: has_race\n",
            "Comparing Truth Object: cartographer with Generated Object: african-american\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: cartographer with Generated Object: c. 1821\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasdeathdate\n",
            "Comparing Truth Object: cartographer with Generated Object: 1857\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hassex\n",
            "Comparing Truth Object: cartographer with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: cartographer with Generated Object: manumission\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: cartographer with Generated Object: enslaved, freed\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: cartographer with Generated Object: cave explorer, geologist, guide at mammoth cave\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: cartographer with Generated Object: enslaved by\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: cartographer with Generated Object: franklin gorin\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: cartographer with Generated Object: stephen bishop\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: cartographer with Generated Object: explorer\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: cartographer with Generated Object: exploration of mammoth cave\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: stephen with Generated Object: stephen bishop\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: stephen with Generated Object: stephen bishop\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: stephen with Generated Object: bishop\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: stephen with Generated Object: stephen\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: stephen with Generated Object: american cave explorer and self-taught geologist known for being one of the first people to explore and map mammoth cave in the u.s. state of kentucky.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: stephen with Generated Object: u.s. state of kentucky\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: has_race\n",
            "Comparing Truth Object: stephen with Generated Object: african-american\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: stephen with Generated Object: c. 1821\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasdeathdate\n",
            "Comparing Truth Object: stephen with Generated Object: 1857\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hassex\n",
            "Comparing Truth Object: stephen with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: stephen with Generated Object: manumission\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: stephen with Generated Object: enslaved, freed\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: stephen with Generated Object: cave explorer, geologist, guide at mammoth cave\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: stephen with Generated Object: enslaved by\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: stephen with Generated Object: franklin gorin\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: stephen with Generated Object: stephen bishop\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: stephen with Generated Object: explorer\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: stephen with Generated Object: exploration of mammoth cave\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: bishop with Generated Object: stephen bishop\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: bishop with Generated Object: stephen bishop\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: bishop with Generated Object: bishop\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: bishop with Generated Object: stephen\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: bishop with Generated Object: american cave explorer and self-taught geologist known for being one of the first people to explore and map mammoth cave in the u.s. state of kentucky.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: bishop with Generated Object: u.s. state of kentucky\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: has_race\n",
            "Comparing Truth Object: bishop with Generated Object: african-american\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: bishop with Generated Object: c. 1821\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasdeathdate\n",
            "Comparing Truth Object: bishop with Generated Object: 1857\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hassex\n",
            "Comparing Truth Object: bishop with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: bishop with Generated Object: manumission\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: bishop with Generated Object: enslaved, freed\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: bishop with Generated Object: cave explorer, geologist, guide at mammoth cave\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: bishop with Generated Object: enslaved by\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: bishop with Generated Object: franklin gorin\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: bishop with Generated Object: stephen bishop\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: bishop with Generated Object: explorer\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: bishop with Generated Object: exploration of mammoth cave\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: stephen bishop with Generated Object: stephen bishop\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: stephen bishop with Generated Object: stephen bishop\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: stephen bishop with Generated Object: bishop\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: stephen bishop with Generated Object: stephen\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: stephen bishop with Generated Object: american cave explorer and self-taught geologist known for being one of the first people to explore and map mammoth cave in the u.s. state of kentucky.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: stephen bishop with Generated Object: u.s. state of kentucky\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: has_race\n",
            "Comparing Truth Object: stephen bishop with Generated Object: african-american\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: stephen bishop with Generated Object: c. 1821\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasdeathdate\n",
            "Comparing Truth Object: stephen bishop with Generated Object: 1857\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hassex\n",
            "Comparing Truth Object: stephen bishop with Generated Object: male\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: stephen bishop with Generated Object: manumission\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: stephen bishop with Generated Object: enslaved, freed\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: stephen bishop with Generated Object: cave explorer, geologist, guide at mammoth cave\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: stephen bishop with Generated Object: enslaved by\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: stephen bishop with Generated Object: franklin gorin\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: stephen bishop with Generated Object: stephen bishop\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: stephen bishop with Generated Object: explorer\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: stephen bishop with Generated Object: exploration of mammoth cave\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: enslaved. free before 13th amendment. enslaved as a guide in mammoth caves who had extensive knowledge of the underground cave system. with Generated Object: stephen bishop\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: enslaved. free before 13th amendment. enslaved as a guide in mammoth caves who had extensive knowledge of the underground cave system. with Generated Object: stephen bishop\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: enslaved. free before 13th amendment. enslaved as a guide in mammoth caves who had extensive knowledge of the underground cave system. with Generated Object: bishop\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: enslaved. free before 13th amendment. enslaved as a guide in mammoth caves who had extensive knowledge of the underground cave system. with Generated Object: stephen\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: enslaved. free before 13th amendment. enslaved as a guide in mammoth caves who had extensive knowledge of the underground cave system. with Generated Object: american cave explorer and self-taught geologist known for being one of the first people to explore and map mammoth cave in the u.s. state of kentucky.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: enslaved. free before 13th amendment. enslaved as a guide in mammoth caves who had extensive knowledge of the underground cave system. with Generated Object: u.s. state of kentucky\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: has_race\n",
            "Comparing Truth Object: enslaved. free before 13th amendment. enslaved as a guide in mammoth caves who had extensive knowledge of the underground cave system. with Generated Object: african-american\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: enslaved. free before 13th amendment. enslaved as a guide in mammoth caves who had extensive knowledge of the underground cave system. with Generated Object: c. 1821\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasdeathdate\n",
            "Comparing Truth Object: enslaved. free before 13th amendment. enslaved as a guide in mammoth caves who had extensive knowledge of the underground cave system. with Generated Object: 1857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hassex\n",
            "Comparing Truth Object: enslaved. free before 13th amendment. enslaved as a guide in mammoth caves who had extensive knowledge of the underground cave system. with Generated Object: male\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: enslaved. free before 13th amendment. enslaved as a guide in mammoth caves who had extensive knowledge of the underground cave system. with Generated Object: manumission\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: enslaved. free before 13th amendment. enslaved as a guide in mammoth caves who had extensive knowledge of the underground cave system. with Generated Object: enslaved, freed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: enslaved. free before 13th amendment. enslaved as a guide in mammoth caves who had extensive knowledge of the underground cave system. with Generated Object: cave explorer, geologist, guide at mammoth cave\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: enslaved. free before 13th amendment. enslaved as a guide in mammoth caves who had extensive knowledge of the underground cave system. with Generated Object: enslaved by\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: enslaved. free before 13th amendment. enslaved as a guide in mammoth caves who had extensive knowledge of the underground cave system. with Generated Object: franklin gorin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: enslaved. free before 13th amendment. enslaved as a guide in mammoth caves who had extensive knowledge of the underground cave system. with Generated Object: stephen bishop\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: enslaved. free before 13th amendment. enslaved as a guide in mammoth caves who had extensive knowledge of the underground cave system. with Generated Object: explorer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: enslaved. free before 13th amendment. enslaved as a guide in mammoth caves who had extensive knowledge of the underground cave system. with Generated Object: exploration of mammoth cave\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to: /content/drive/My Drive/LLM-Data/evaluate-test/GPT4_Enslaved_MainAgent/Stephen_Bishop_(cave_explorer)_triples_evaluation.csv\n",
            "\n",
            "Processing truth file: John_Horse.tsv with generated file: John_Horse_triples.tsv\n",
            "Number of truth triples: 10\n",
            "Number of generated triples: 14\n",
            "Comparing Truth Predicate: instance of with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: person with Generated Object: john horse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: person with Generated Object: john horse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: person with Generated Object: john horse was a seminole black who led an exodus from wewoka, and indian territory in general, in the dead of night. he led over a hundred blacks including men, women and children, and at least as many fleeing seminole, out of the lands they had been placed on by the government, heading south across the red river into texas.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: person with Generated Object: florida\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: has_race\n",
            "Comparing Truth Object: person with Generated Object: seminole black\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hassex\n",
            "Comparing Truth Object: person with Generated Object: male\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: person with Generated Object: exodus from wewoka\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: person with Generated Object: leader\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: person with Generated Object: leader\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: person with Generated Object: friend\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: person with Generated Object: coacoochee\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: person with Generated Object: john horse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: person with Generated Object: leader\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: person with Generated Object: exodus from wewoka\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: male with Generated Object: john horse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: male with Generated Object: john horse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: male with Generated Object: john horse was a seminole black who led an exodus from wewoka, and indian territory in general, in the dead of night. he led over a hundred blacks including men, women and children, and at least as many fleeing seminole, out of the lands they had been placed on by the government, heading south across the red river into texas.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: male with Generated Object: florida\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: has_race\n",
            "Comparing Truth Object: male with Generated Object: seminole black\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hassex\n",
            "Comparing Truth Object: male with Generated Object: male\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: male with Generated Object: exodus from wewoka\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: male with Generated Object: leader\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: male with Generated Object: leader\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: male with Generated Object: friend\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: male with Generated Object: coacoochee\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: male with Generated Object: john horse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: male with Generated Object: leader\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: male with Generated Object: exodus from wewoka\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: child with Generated Object: john horse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: child with Generated Object: john horse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: child with Generated Object: john horse was a seminole black who led an exodus from wewoka, and indian territory in general, in the dead of night. he led over a hundred blacks including men, women and children, and at least as many fleeing seminole, out of the lands they had been placed on by the government, heading south across the red river into texas.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: child with Generated Object: florida\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: has_race\n",
            "Comparing Truth Object: child with Generated Object: seminole black\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hassex\n",
            "Comparing Truth Object: child with Generated Object: male\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: child with Generated Object: exodus from wewoka\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: child with Generated Object: leader\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: child with Generated Object: leader\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: child with Generated Object: friend\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: child with Generated Object: coacoochee\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: child with Generated Object: john horse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: child with Generated Object: leader\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: child with Generated Object: exodus from wewoka\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: deceased person with Generated Object: john horse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: deceased person with Generated Object: john horse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: deceased person with Generated Object: john horse was a seminole black who led an exodus from wewoka, and indian territory in general, in the dead of night. he led over a hundred blacks including men, women and children, and at least as many fleeing seminole, out of the lands they had been placed on by the government, heading south across the red river into texas.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: deceased person with Generated Object: florida\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: has_race\n",
            "Comparing Truth Object: deceased person with Generated Object: seminole black\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hassex\n",
            "Comparing Truth Object: deceased person with Generated Object: male\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: deceased person with Generated Object: exodus from wewoka\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: deceased person with Generated Object: leader\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: deceased person with Generated Object: leader\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: deceased person with Generated Object: friend\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: deceased person with Generated Object: coacoochee\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: deceased person with Generated Object: john horse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: deceased person with Generated Object: leader\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: deceased person with Generated Object: exodus from wewoka\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: slave with Generated Object: john horse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: slave with Generated Object: john horse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: slave with Generated Object: john horse was a seminole black who led an exodus from wewoka, and indian territory in general, in the dead of night. he led over a hundred blacks including men, women and children, and at least as many fleeing seminole, out of the lands they had been placed on by the government, heading south across the red river into texas.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: slave with Generated Object: florida\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: has_race\n",
            "Comparing Truth Object: slave with Generated Object: seminole black\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hassex\n",
            "Comparing Truth Object: slave with Generated Object: male\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: slave with Generated Object: exodus from wewoka\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: slave with Generated Object: leader\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: slave with Generated Object: leader\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: slave with Generated Object: friend\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: slave with Generated Object: coacoochee\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: slave with Generated Object: john horse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: slave with Generated Object: leader\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: slave with Generated Object: exodus from wewoka\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: american indian leader with Generated Object: john horse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: american indian leader with Generated Object: john horse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: american indian leader with Generated Object: john horse was a seminole black who led an exodus from wewoka, and indian territory in general, in the dead of night. he led over a hundred blacks including men, women and children, and at least as many fleeing seminole, out of the lands they had been placed on by the government, heading south across the red river into texas.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: american indian leader with Generated Object: florida\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: has_race\n",
            "Comparing Truth Object: american indian leader with Generated Object: seminole black\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hassex\n",
            "Comparing Truth Object: american indian leader with Generated Object: male\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: american indian leader with Generated Object: exodus from wewoka\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: american indian leader with Generated Object: leader\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: american indian leader with Generated Object: leader\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: american indian leader with Generated Object: friend\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: american indian leader with Generated Object: coacoochee\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: american indian leader with Generated Object: john horse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: american indian leader with Generated Object: leader\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: american indian leader with Generated Object: exodus from wewoka\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: john with Generated Object: john horse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: john with Generated Object: john horse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: john with Generated Object: john horse was a seminole black who led an exodus from wewoka, and indian territory in general, in the dead of night. he led over a hundred blacks including men, women and children, and at least as many fleeing seminole, out of the lands they had been placed on by the government, heading south across the red river into texas.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: john with Generated Object: florida\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: has_race\n",
            "Comparing Truth Object: john with Generated Object: seminole black\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hassex\n",
            "Comparing Truth Object: john with Generated Object: male\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: john with Generated Object: exodus from wewoka\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: john with Generated Object: leader\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: john with Generated Object: leader\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: john with Generated Object: friend\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: john with Generated Object: coacoochee\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: john with Generated Object: john horse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: john with Generated Object: leader\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: john with Generated Object: exodus from wewoka\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: horse with Generated Object: john horse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: horse with Generated Object: john horse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: horse with Generated Object: john horse was a seminole black who led an exodus from wewoka, and indian territory in general, in the dead of night. he led over a hundred blacks including men, women and children, and at least as many fleeing seminole, out of the lands they had been placed on by the government, heading south across the red river into texas.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: horse with Generated Object: florida\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: has_race\n",
            "Comparing Truth Object: horse with Generated Object: seminole black\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hassex\n",
            "Comparing Truth Object: horse with Generated Object: male\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: horse with Generated Object: exodus from wewoka\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: horse with Generated Object: leader\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: horse with Generated Object: leader\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: horse with Generated Object: friend\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: horse with Generated Object: coacoochee\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: horse with Generated Object: john horse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: horse with Generated Object: leader\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: horse with Generated Object: exodus from wewoka\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: john horse with Generated Object: john horse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: john horse with Generated Object: john horse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: john horse with Generated Object: john horse was a seminole black who led an exodus from wewoka, and indian territory in general, in the dead of night. he led over a hundred blacks including men, women and children, and at least as many fleeing seminole, out of the lands they had been placed on by the government, heading south across the red river into texas.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: john horse with Generated Object: florida\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: has_race\n",
            "Comparing Truth Object: john horse with Generated Object: seminole black\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hassex\n",
            "Comparing Truth Object: john horse with Generated Object: male\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: john horse with Generated Object: exodus from wewoka\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: john horse with Generated Object: leader\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: john horse with Generated Object: leader\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: john horse with Generated Object: friend\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: john horse with Generated Object: coacoochee\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: john horse with Generated Object: john horse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: john horse with Generated Object: leader\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: john horse with Generated Object: exodus from wewoka\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: enslaved. free before 13th amendment. organized an exodus in 1850 of about 300 black seminoles and native americans who risked being enslaved. with Generated Object: john horse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: enslaved. free before 13th amendment. organized an exodus in 1850 of about 300 black seminoles and native americans who risked being enslaved. with Generated Object: john horse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: enslaved. free before 13th amendment. organized an exodus in 1850 of about 300 black seminoles and native americans who risked being enslaved. with Generated Object: john horse was a seminole black who led an exodus from wewoka, and indian territory in general, in the dead of night. he led over a hundred blacks including men, women and children, and at least as many fleeing seminole, out of the lands they had been placed on by the government, heading south across the red river into texas.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: enslaved. free before 13th amendment. organized an exodus in 1850 of about 300 black seminoles and native americans who risked being enslaved. with Generated Object: florida\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: has_race\n",
            "Comparing Truth Object: enslaved. free before 13th amendment. organized an exodus in 1850 of about 300 black seminoles and native americans who risked being enslaved. with Generated Object: seminole black\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hassex\n",
            "Comparing Truth Object: enslaved. free before 13th amendment. organized an exodus in 1850 of about 300 black seminoles and native americans who risked being enslaved. with Generated Object: male\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: enslaved. free before 13th amendment. organized an exodus in 1850 of about 300 black seminoles and native americans who risked being enslaved. with Generated Object: exodus from wewoka\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: enslaved. free before 13th amendment. organized an exodus in 1850 of about 300 black seminoles and native americans who risked being enslaved. with Generated Object: leader\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: enslaved. free before 13th amendment. organized an exodus in 1850 of about 300 black seminoles and native americans who risked being enslaved. with Generated Object: leader\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: enslaved. free before 13th amendment. organized an exodus in 1850 of about 300 black seminoles and native americans who risked being enslaved. with Generated Object: friend\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: enslaved. free before 13th amendment. organized an exodus in 1850 of about 300 black seminoles and native americans who risked being enslaved. with Generated Object: coacoochee\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: enslaved. free before 13th amendment. organized an exodus in 1850 of about 300 black seminoles and native americans who risked being enslaved. with Generated Object: john horse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: enslaved. free before 13th amendment. organized an exodus in 1850 of about 300 black seminoles and native americans who risked being enslaved. with Generated Object: leader\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: enslaved. free before 13th amendment. organized an exodus in 1850 of about 300 black seminoles and native americans who risked being enslaved. with Generated Object: exodus from wewoka\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to: /content/drive/My Drive/LLM-Data/evaluate-test/GPT4_Enslaved_MainAgent/John_Horse_triples_evaluation.csv\n",
            "\n",
            "Processing truth file: Josiah_Henson.tsv with generated file: Josiah_Henson_triples.tsv\n",
            "Number of truth triples: 11\n",
            "Number of generated triples: 14\n",
            "Comparing Truth Predicate: instance of with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: person with Generated Object: josiah henson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: person with Generated Object: josiah henson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: person with Generated Object: henson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: person with Generated Object: josiah\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: person with Generated Object: born into slavery, josiah henson experienced numerous atrocities and hardships. he was eventually sold to isaac riley, who entrusted him as the supervisor of his farm. henson later escaped slavery with his family and settled in canada.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: person with Generated Object: port tobacco, charles county, maryland\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: has_race\n",
            "Comparing Truth Object: person with Generated Object: african-american\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hassex\n",
            "Comparing Truth Object: person with Generated Object: male\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: person with Generated Object: enslavement, escape from slavery\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: person with Generated Object: enslaved person, free person\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: person with Generated Object: farm supervisor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: person with Generated Object: enslaved by\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: person with Generated Object: isaac riley\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: person with Generated Object: josiah henson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: male with Generated Object: josiah henson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: male with Generated Object: josiah henson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: male with Generated Object: henson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: male with Generated Object: josiah\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: male with Generated Object: born into slavery, josiah henson experienced numerous atrocities and hardships. he was eventually sold to isaac riley, who entrusted him as the supervisor of his farm. henson later escaped slavery with his family and settled in canada.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: male with Generated Object: port tobacco, charles county, maryland\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: has_race\n",
            "Comparing Truth Object: male with Generated Object: african-american\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hassex\n",
            "Comparing Truth Object: male with Generated Object: male\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: male with Generated Object: enslavement, escape from slavery\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: male with Generated Object: enslaved person, free person\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: male with Generated Object: farm supervisor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: male with Generated Object: enslaved by\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: male with Generated Object: isaac riley\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassex with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: male with Generated Object: josiah henson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: child with Generated Object: josiah henson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: child with Generated Object: josiah henson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: child with Generated Object: henson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: child with Generated Object: josiah\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: child with Generated Object: born into slavery, josiah henson experienced numerous atrocities and hardships. he was eventually sold to isaac riley, who entrusted him as the supervisor of his farm. henson later escaped slavery with his family and settled in canada.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: child with Generated Object: port tobacco, charles county, maryland\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: has_race\n",
            "Comparing Truth Object: child with Generated Object: african-american\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hassex\n",
            "Comparing Truth Object: child with Generated Object: male\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: child with Generated Object: enslavement, escape from slavery\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: child with Generated Object: enslaved person, free person\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: child with Generated Object: farm supervisor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: child with Generated Object: enslaved by\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: child with Generated Object: isaac riley\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: child with Generated Object: josiah henson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: deceased person with Generated Object: josiah henson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: deceased person with Generated Object: josiah henson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: deceased person with Generated Object: henson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: deceased person with Generated Object: josiah\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: deceased person with Generated Object: born into slavery, josiah henson experienced numerous atrocities and hardships. he was eventually sold to isaac riley, who entrusted him as the supervisor of his farm. henson later escaped slavery with his family and settled in canada.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: deceased person with Generated Object: port tobacco, charles county, maryland\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: has_race\n",
            "Comparing Truth Object: deceased person with Generated Object: african-american\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hassex\n",
            "Comparing Truth Object: deceased person with Generated Object: male\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: deceased person with Generated Object: enslavement, escape from slavery\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: deceased person with Generated Object: enslaved person, free person\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: deceased person with Generated Object: farm supervisor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: deceased person with Generated Object: enslaved by\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: deceased person with Generated Object: isaac riley\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: deceased person with Generated Object: josiah henson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: slave with Generated Object: josiah henson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: slave with Generated Object: josiah henson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: slave with Generated Object: henson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: slave with Generated Object: josiah\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: slave with Generated Object: born into slavery, josiah henson experienced numerous atrocities and hardships. he was eventually sold to isaac riley, who entrusted him as the supervisor of his farm. henson later escaped slavery with his family and settled in canada.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: slave with Generated Object: port tobacco, charles county, maryland\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: has_race\n",
            "Comparing Truth Object: slave with Generated Object: african-american\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hassex\n",
            "Comparing Truth Object: slave with Generated Object: male\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: slave with Generated Object: enslavement, escape from slavery\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: slave with Generated Object: enslaved person, free person\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: slave with Generated Object: farm supervisor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: slave with Generated Object: enslaved by\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: slave with Generated Object: isaac riley\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: slave with Generated Object: josiah henson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: literary inspiration with Generated Object: josiah henson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: literary inspiration with Generated Object: josiah henson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: literary inspiration with Generated Object: henson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: literary inspiration with Generated Object: josiah\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: literary inspiration with Generated Object: born into slavery, josiah henson experienced numerous atrocities and hardships. he was eventually sold to isaac riley, who entrusted him as the supervisor of his farm. henson later escaped slavery with his family and settled in canada.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: literary inspiration with Generated Object: port tobacco, charles county, maryland\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: has_race\n",
            "Comparing Truth Object: literary inspiration with Generated Object: african-american\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hassex\n",
            "Comparing Truth Object: literary inspiration with Generated Object: male\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: literary inspiration with Generated Object: enslavement, escape from slavery\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: literary inspiration with Generated Object: enslaved person, free person\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: literary inspiration with Generated Object: farm supervisor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: literary inspiration with Generated Object: enslaved by\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: literary inspiration with Generated Object: isaac riley\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: literary inspiration with Generated Object: josiah henson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: methodist episcopal clergy with Generated Object: josiah henson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: methodist episcopal clergy with Generated Object: josiah henson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: methodist episcopal clergy with Generated Object: henson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: methodist episcopal clergy with Generated Object: josiah\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: methodist episcopal clergy with Generated Object: born into slavery, josiah henson experienced numerous atrocities and hardships. he was eventually sold to isaac riley, who entrusted him as the supervisor of his farm. henson later escaped slavery with his family and settled in canada.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: methodist episcopal clergy with Generated Object: port tobacco, charles county, maryland\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: has_race\n",
            "Comparing Truth Object: methodist episcopal clergy with Generated Object: african-american\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hassex\n",
            "Comparing Truth Object: methodist episcopal clergy with Generated Object: male\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: methodist episcopal clergy with Generated Object: enslavement, escape from slavery\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: methodist episcopal clergy with Generated Object: enslaved person, free person\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: methodist episcopal clergy with Generated Object: farm supervisor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: methodist episcopal clergy with Generated Object: enslaved by\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: methodist episcopal clergy with Generated Object: isaac riley\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescriptiveoccupation with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: methodist episcopal clergy with Generated Object: josiah henson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: josiah with Generated Object: josiah henson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: josiah with Generated Object: josiah henson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: josiah with Generated Object: henson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: josiah with Generated Object: josiah\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: josiah with Generated Object: born into slavery, josiah henson experienced numerous atrocities and hardships. he was eventually sold to isaac riley, who entrusted him as the supervisor of his farm. henson later escaped slavery with his family and settled in canada.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: josiah with Generated Object: port tobacco, charles county, maryland\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: has_race\n",
            "Comparing Truth Object: josiah with Generated Object: african-american\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hassex\n",
            "Comparing Truth Object: josiah with Generated Object: male\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: josiah with Generated Object: enslavement, escape from slavery\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: josiah with Generated Object: enslaved person, free person\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: josiah with Generated Object: farm supervisor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: josiah with Generated Object: enslaved by\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: josiah with Generated Object: isaac riley\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: josiah with Generated Object: josiah henson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: henson with Generated Object: josiah henson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: henson with Generated Object: josiah henson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: henson with Generated Object: henson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: henson with Generated Object: josiah\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: henson with Generated Object: born into slavery, josiah henson experienced numerous atrocities and hardships. he was eventually sold to isaac riley, who entrusted him as the supervisor of his farm. henson later escaped slavery with his family and settled in canada.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: henson with Generated Object: port tobacco, charles county, maryland\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: has_race\n",
            "Comparing Truth Object: henson with Generated Object: african-american\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hassex\n",
            "Comparing Truth Object: henson with Generated Object: male\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: henson with Generated Object: enslavement, escape from slavery\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: henson with Generated Object: enslaved person, free person\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: henson with Generated Object: farm supervisor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: henson with Generated Object: enslaved by\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: henson with Generated Object: isaac riley\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: henson with Generated Object: josiah henson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: josiah henson with Generated Object: josiah henson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: josiah henson with Generated Object: josiah henson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: josiah henson with Generated Object: henson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: josiah henson with Generated Object: josiah\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: josiah henson with Generated Object: born into slavery, josiah henson experienced numerous atrocities and hardships. he was eventually sold to isaac riley, who entrusted him as the supervisor of his farm. henson later escaped slavery with his family and settled in canada.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: josiah henson with Generated Object: port tobacco, charles county, maryland\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: has_race\n",
            "Comparing Truth Object: josiah henson with Generated Object: african-american\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hassex\n",
            "Comparing Truth Object: josiah henson with Generated Object: male\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: josiah henson with Generated Object: enslavement, escape from slavery\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: josiah henson with Generated Object: enslaved person, free person\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: josiah henson with Generated Object: farm supervisor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: josiah henson with Generated Object: enslaved by\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: josiah henson with Generated Object: isaac riley\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: josiah henson with Generated Object: josiah henson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: born enslaved. free before 13th amendment. an escaped slave who became a pastor, led a community of escaped slaves, wrote an autobiography and possibly was the basis for the character of uncle tom in harriet beecher stowe's famous novel. with Generated Object: josiah henson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: born enslaved. free before 13th amendment. an escaped slave who became a pastor, led a community of escaped slaves, wrote an autobiography and possibly was the basis for the character of uncle tom in harriet beecher stowe's famous novel. with Generated Object: josiah henson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: born enslaved. free before 13th amendment. an escaped slave who became a pastor, led a community of escaped slaves, wrote an autobiography and possibly was the basis for the character of uncle tom in harriet beecher stowe's famous novel. with Generated Object: henson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: born enslaved. free before 13th amendment. an escaped slave who became a pastor, led a community of escaped slaves, wrote an autobiography and possibly was the basis for the character of uncle tom in harriet beecher stowe's famous novel. with Generated Object: josiah\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: born enslaved. free before 13th amendment. an escaped slave who became a pastor, led a community of escaped slaves, wrote an autobiography and possibly was the basis for the character of uncle tom in harriet beecher stowe's famous novel. with Generated Object: born into slavery, josiah henson experienced numerous atrocities and hardships. he was eventually sold to isaac riley, who entrusted him as the supervisor of his farm. henson later escaped slavery with his family and settled in canada.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: born enslaved. free before 13th amendment. an escaped slave who became a pastor, led a community of escaped slaves, wrote an autobiography and possibly was the basis for the character of uncle tom in harriet beecher stowe's famous novel. with Generated Object: port tobacco, charles county, maryland\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: has_race\n",
            "Comparing Truth Object: born enslaved. free before 13th amendment. an escaped slave who became a pastor, led a community of escaped slaves, wrote an autobiography and possibly was the basis for the character of uncle tom in harriet beecher stowe's famous novel. with Generated Object: african-american\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hassex\n",
            "Comparing Truth Object: born enslaved. free before 13th amendment. an escaped slave who became a pastor, led a community of escaped slaves, wrote an autobiography and possibly was the basis for the character of uncle tom in harriet beecher stowe's famous novel. with Generated Object: male\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: born enslaved. free before 13th amendment. an escaped slave who became a pastor, led a community of escaped slaves, wrote an autobiography and possibly was the basis for the character of uncle tom in harriet beecher stowe's famous novel. with Generated Object: enslavement, escape from slavery\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: born enslaved. free before 13th amendment. an escaped slave who became a pastor, led a community of escaped slaves, wrote an autobiography and possibly was the basis for the character of uncle tom in harriet beecher stowe's famous novel. with Generated Object: enslaved person, free person\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: born enslaved. free before 13th amendment. an escaped slave who became a pastor, led a community of escaped slaves, wrote an autobiography and possibly was the basis for the character of uncle tom in harriet beecher stowe's famous novel. with Generated Object: farm supervisor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: hasinteragentrelationshiptype\n",
            "Comparing Truth Object: born enslaved. free before 13th amendment. an escaped slave who became a pastor, led a community of escaped slaves, wrote an autobiography and possibly was the basis for the character of uncle tom in harriet beecher stowe's famous novel. with Generated Object: enslaved by\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: born enslaved. free before 13th amendment. an escaped slave who became a pastor, led a community of escaped slaves, wrote an autobiography and possibly was the basis for the character of uncle tom in harriet beecher stowe's famous novel. with Generated Object: isaac riley\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasdescription with Generated Predicate: isrelationshipfrom\n",
            "Comparing Truth Object: born enslaved. free before 13th amendment. an escaped slave who became a pastor, led a community of escaped slaves, wrote an autobiography and possibly was the basis for the character of uncle tom in harriet beecher stowe's famous novel. with Generated Object: josiah henson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to: /content/drive/My Drive/LLM-Data/evaluate-test/GPT4_Enslaved_MainAgent/Josiah_Henson_triples_evaluation.csv\n",
            "\n",
            "Processing truth file: Asa_Coleman.tsv with generated file: Asa_Coleman_triples.tsv\n",
            "Number of truth triples: 6\n",
            "Number of generated triples: 14\n",
            "Comparing Truth Predicate: instance of with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: person with Generated Object: asa coleman\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: person with Generated Object: asa coleman\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: person with Generated Object: coleman\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: person with Generated Object: asa\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: person with Generated Object: american politician and former slave. he served in the virginia house of delegates and was recognized by the martin luther king commission and the state of virginia for his government service.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: person with Generated Object: north carolina\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: has_race\n",
            "Comparing Truth Object: person with Generated Object: african-american\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: person with Generated Object: early 1830s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: person with Generated Object: recognition by the martin luther king commission and the state of virginia\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: person with Generated Object: former slave, politician\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: person with Generated Object: politician, carpenter\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: person with Generated Object: amanda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: person with Generated Object: delegate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: person with Generated Object: virginia house of delegates\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: asa coleman with Generated Object: asa coleman\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: asa coleman with Generated Object: asa coleman\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: asa coleman with Generated Object: coleman\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: asa coleman with Generated Object: asa\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: asa coleman with Generated Object: american politician and former slave. he served in the virginia house of delegates and was recognized by the martin luther king commission and the state of virginia for his government service.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: asa coleman with Generated Object: north carolina\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: has_race\n",
            "Comparing Truth Object: asa coleman with Generated Object: african-american\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: asa coleman with Generated Object: early 1830s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: asa coleman with Generated Object: recognition by the martin luther king commission and the state of virginia\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: asa coleman with Generated Object: former slave, politician\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: asa coleman with Generated Object: politician, carpenter\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: asa coleman with Generated Object: amanda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: asa coleman with Generated Object: delegate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasname with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: asa coleman with Generated Object: virginia house of delegates\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: asa with Generated Object: asa coleman\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: asa with Generated Object: asa coleman\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: asa with Generated Object: coleman\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: asa with Generated Object: asa\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: asa with Generated Object: american politician and former slave. he served in the virginia house of delegates and was recognized by the martin luther king commission and the state of virginia for his government service.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: asa with Generated Object: north carolina\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: has_race\n",
            "Comparing Truth Object: asa with Generated Object: african-american\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: asa with Generated Object: early 1830s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: asa with Generated Object: recognition by the martin luther king commission and the state of virginia\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: asa with Generated Object: former slave, politician\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: asa with Generated Object: politician, carpenter\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: asa with Generated Object: amanda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: asa with Generated Object: delegate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasfirstname with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: asa with Generated Object: virginia house of delegates\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: coleman with Generated Object: asa coleman\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: coleman with Generated Object: asa coleman\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: coleman with Generated Object: coleman\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: coleman with Generated Object: asa\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: coleman with Generated Object: american politician and former slave. he served in the virginia house of delegates and was recognized by the martin luther king commission and the state of virginia for his government service.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: coleman with Generated Object: north carolina\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: has_race\n",
            "Comparing Truth Object: coleman with Generated Object: african-american\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: coleman with Generated Object: early 1830s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: coleman with Generated Object: recognition by the martin luther king commission and the state of virginia\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: coleman with Generated Object: former slave, politician\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: coleman with Generated Object: politician, carpenter\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: coleman with Generated Object: amanda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: coleman with Generated Object: delegate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hassurname with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: coleman with Generated Object: virginia house of delegates\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: registered person with Generated Object: asa coleman\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: registered person with Generated Object: asa coleman\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: registered person with Generated Object: coleman\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: registered person with Generated Object: asa\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: registered person with Generated Object: american politician and former slave. he served in the virginia house of delegates and was recognized by the martin luther king commission and the state of virginia for his government service.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: registered person with Generated Object: north carolina\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: has_race\n",
            "Comparing Truth Object: registered person with Generated Object: african-american\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: registered person with Generated Object: early 1830s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: registered person with Generated Object: recognition by the martin luther king commission and the state of virginia\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: registered person with Generated Object: former slave, politician\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: registered person with Generated Object: politician, carpenter\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: registered person with Generated Object: amanda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: registered person with Generated Object: delegate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: hasparticipantrole with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: registered person with Generated Object: virginia house of delegates\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: haspersonstatus with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: free person with Generated Object: asa coleman\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: haspersonstatus with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: free person with Generated Object: asa coleman\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: haspersonstatus with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: free person with Generated Object: coleman\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: haspersonstatus with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: free person with Generated Object: asa\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: haspersonstatus with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: free person with Generated Object: american politician and former slave. he served in the virginia house of delegates and was recognized by the martin luther king commission and the state of virginia for his government service.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: haspersonstatus with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: free person with Generated Object: north carolina\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: haspersonstatus with Generated Predicate: has_race\n",
            "Comparing Truth Object: free person with Generated Object: african-american\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: haspersonstatus with Generated Predicate: hasbirthdate\n",
            "Comparing Truth Object: free person with Generated Object: early 1830s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: haspersonstatus with Generated Predicate: hasstatusgeneratedevent\n",
            "Comparing Truth Object: free person with Generated Object: recognition by the martin luther king commission and the state of virginia\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: haspersonstatus with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: free person with Generated Object: former slave, politician\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: haspersonstatus with Generated Predicate: hasvalue\n",
            "Comparing Truth Object: free person with Generated Object: politician, carpenter\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: haspersonstatus with Generated Predicate: isrelationshipto\n",
            "Comparing Truth Object: free person with Generated Object: amanda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: haspersonstatus with Generated Predicate: hasparticipantroletype\n",
            "Comparing Truth Object: free person with Generated Object: delegate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: haspersonstatus with Generated Predicate: roleprovidedby\n",
            "Comparing Truth Object: free person with Generated Object: virginia house of delegates\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to: /content/drive/My Drive/LLM-Data/evaluate-test/GPT4_Enslaved_MainAgent/Asa_Coleman_triples_evaluation.csv\n",
            "\n",
            "Processing truth file: Fortune.tsv with generated file: Amos_Fortune_triples.tsv\n",
            "Number of truth triples: 8\n",
            "Number of generated triples: 19\n",
            "Comparing Truth Predicate: instance of with Generated Predicate: haspreferrednamevariant\n",
            "Comparing Truth Object: person with Generated Object: amos fortune\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: fullnameasstring\n",
            "Comparing Truth Object: person with Generated Object: amos fortune\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hassurnameasstring\n",
            "Comparing Truth Object: person with Generated Object: fortune\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasfirstnameasstring\n",
            "Comparing Truth Object: person with Generated Object: amos\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasdescription\n",
            "Comparing Truth Object: person with Generated Object: a prominent african-american citizen of jaffrey, new hampshire, in the 18th century. fortune was born in africa and brought to america as an enslaved person. he purchased his freedom at the age of 60 and moved to jaffrey to start a leather tannery business.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: referstoplaceoforigin\n",
            "Comparing Truth Object: person with Generated Object: africa\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: has_race\n",
            "Comparing Truth Object: person with Generated Object: african-american\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Truth Predicate: instance of with Generated Predicate: hasagevalue\n",
            "Comparing Truth Object: person with Generated Object: nan\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "object of type 'float' has no len()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-136dd3ba3b30>\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0;34m\"Predicate Fuzzy Ratio\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfuzz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruth_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerated_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;34m\"Predicate Jaro-Winkler\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjellyfish\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjaro_winkler_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruth_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerated_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0;34m\"Object Fuzzy Ratio\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfuzz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruth_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerated_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m                 \u001b[0;34m\"Object Jaro-Winkler\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjellyfish\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjaro_winkler_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruth_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerated_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             }\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fuzzywuzzy/utils.py\u001b[0m in \u001b[0;36mdecorator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fuzzywuzzy/utils.py\u001b[0m in \u001b[0;36mdecorator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fuzzywuzzy/utils.py\u001b[0m in \u001b[0;36mdecorator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'float' has no len()"
          ]
        }
      ]
    }
  ]
}