{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./.conda/lib/python3.11/site-packages (2.4.0)\n",
      "Requirement already satisfied: filelock in ./.conda/lib/python3.11/site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.conda/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./.conda/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in ./.conda/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in ./.conda/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./.conda/lib/python3.11/site-packages (from torch) (2024.5.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.conda/lib/python3.11/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.conda/lib/python3.11/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.conda/lib/python3.11/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.conda/lib/python3.11/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.conda/lib/python3.11/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.conda/lib/python3.11/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./.conda/lib/python3.11/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in ./.conda/lib/python3.11/site-packages (from torch) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.20)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.conda/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.conda/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: chromadb in ./.conda/lib/python3.11/site-packages (0.5.5)\n",
      "Requirement already satisfied: build>=1.0.3 in ./.conda/lib/python3.11/site-packages (from chromadb) (1.2.1)\n",
      "Requirement already satisfied: pydantic>=1.9 in ./.conda/lib/python3.11/site-packages (from chromadb) (2.8.2)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in ./.conda/lib/python3.11/site-packages (from chromadb) (0.7.6)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in ./.conda/lib/python3.11/site-packages (from chromadb) (0.112.0)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in ./.conda/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.30.5)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.22.5 in ./.conda/lib/python3.11/site-packages (from chromadb) (1.26.4)\n",
      "Requirement already satisfied: posthog>=2.4.0 in ./.conda/lib/python3.11/site-packages (from chromadb) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./.conda/lib/python3.11/site-packages (from chromadb) (4.12.2)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in ./.conda/lib/python3.11/site-packages (from chromadb) (1.18.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in ./.conda/lib/python3.11/site-packages (from chromadb) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in ./.conda/lib/python3.11/site-packages (from chromadb) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in ./.conda/lib/python3.11/site-packages (from chromadb) (0.47b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in ./.conda/lib/python3.11/site-packages (from chromadb) (1.26.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in ./.conda/lib/python3.11/site-packages (from chromadb) (0.19.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in ./.conda/lib/python3.11/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in ./.conda/lib/python3.11/site-packages (from chromadb) (4.66.5)\n",
      "Requirement already satisfied: overrides>=7.3.1 in ./.conda/lib/python3.11/site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in ./.conda/lib/python3.11/site-packages (from chromadb) (6.4.0)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in ./.conda/lib/python3.11/site-packages (from chromadb) (1.65.4)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in ./.conda/lib/python3.11/site-packages (from chromadb) (4.2.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in ./.conda/lib/python3.11/site-packages (from chromadb) (0.12.3)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in ./.conda/lib/python3.11/site-packages (from chromadb) (30.1.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in ./.conda/lib/python3.11/site-packages (from chromadb) (8.5.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in ./.conda/lib/python3.11/site-packages (from chromadb) (6.0.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in ./.conda/lib/python3.11/site-packages (from chromadb) (4.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in ./.conda/lib/python3.11/site-packages (from chromadb) (3.10.6)\n",
      "Requirement already satisfied: httpx>=0.27.0 in ./.conda/lib/python3.11/site-packages (from chromadb) (0.27.0)\n",
      "Requirement already satisfied: packaging>=19.1 in ./.conda/lib/python3.11/site-packages (from build>=1.0.3->chromadb) (24.1)\n",
      "Requirement already satisfied: pyproject_hooks in ./.conda/lib/python3.11/site-packages (from build>=1.0.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in ./.conda/lib/python3.11/site-packages (from fastapi>=0.95.2->chromadb) (0.37.2)\n",
      "Requirement already satisfied: anyio in ./.conda/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb) (4.4.0)\n",
      "Requirement already satisfied: certifi in ./.conda/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in ./.conda/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb) (1.0.5)\n",
      "Requirement already satisfied: idna in ./.conda/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb) (3.7)\n",
      "Requirement already satisfied: sniffio in ./.conda/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.conda/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: six>=1.9.0 in ./.conda/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in ./.conda/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in ./.conda/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.33.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in ./.conda/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests in ./.conda/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
      "Requirement already satisfied: requests-oauthlib in ./.conda/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in ./.conda/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in ./.conda/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.2.2)\n",
      "Requirement already satisfied: coloredlogs in ./.conda/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in ./.conda/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
      "Requirement already satisfied: protobuf in ./.conda/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (4.25.4)\n",
      "Requirement already satisfied: sympy in ./.conda/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in ./.conda/lib/python3.11/site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
      "Requirement already satisfied: importlib-metadata<=8.0.0,>=6.0 in ./.conda/lib/python3.11/site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.0.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in ./.conda/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.63.2)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.26.0 in ./.conda/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.26.0 in ./.conda/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.47b0 in ./.conda/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.47b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.47b0 in ./.conda/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.47b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.47b0 in ./.conda/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.47b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.47b0 in ./.conda/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.47b0)\n",
      "Requirement already satisfied: setuptools>=16.0 in ./.conda/lib/python3.11/site-packages (from opentelemetry-instrumentation==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (72.1.0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in ./.conda/lib/python3.11/site-packages (from opentelemetry-instrumentation==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: asgiref~=3.0 in ./.conda/lib/python3.11/site-packages (from opentelemetry-instrumentation-asgi==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in ./.conda/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in ./.conda/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.conda/lib/python3.11/site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in ./.conda/lib/python3.11/site-packages (from pydantic>=1.9->chromadb) (2.20.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in ./.conda/lib/python3.11/site-packages (from tokenizers>=0.13.2->chromadb) (0.24.5)\n",
      "Requirement already satisfied: click>=8.0.0 in ./.conda/lib/python3.11/site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./.conda/lib/python3.11/site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in ./.conda/lib/python3.11/site-packages (from typer>=0.9.0->chromadb) (13.7.1)\n",
      "Requirement already satisfied: httptools>=0.5.0 in ./.conda/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in ./.conda/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in ./.conda/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in ./.conda/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.23.0)\n",
      "Requirement already satisfied: websockets>=10.4 in ./.conda/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./.conda/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./.conda/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./.conda/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: filelock in ./.conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in ./.conda/lib/python3.11/site-packages (from importlib-metadata<=8.0.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.19.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/lib/python3.11/site-packages (from requests->kubernetes>=28.1.0->chromadb) (3.3.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.conda/lib/python3.11/site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.conda/lib/python3.11/site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (2.18.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in ./.conda/lib/python3.11/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.conda/lib/python3.11/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in ./.conda/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.0)\n",
      "Requirement already satisfied: openai in ./.conda/lib/python3.11/site-packages (1.40.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.conda/lib/python3.11/site-packages (from openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.conda/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.conda/lib/python3.11/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./.conda/lib/python3.11/site-packages (from openai) (0.5.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./.conda/lib/python3.11/site-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in ./.conda/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./.conda/lib/python3.11/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in ./.conda/lib/python3.11/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in ./.conda/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in ./.conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in ./.conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.conda/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in ./.conda/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "Requirement already satisfied: tiktoken in ./.conda/lib/python3.11/site-packages (0.7.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.conda/lib/python3.11/site-packages (from tiktoken) (2024.7.24)\n",
      "Requirement already satisfied: requests>=2.26.0 in ./.conda/lib/python3.11/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n",
      "Requirement already satisfied: langchain-community in ./.conda/lib/python3.11/site-packages (0.2.11)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.conda/lib/python3.11/site-packages (from langchain-community) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.conda/lib/python3.11/site-packages (from langchain-community) (2.0.32)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.conda/lib/python3.11/site-packages (from langchain-community) (3.10.1)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./.conda/lib/python3.11/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: langchain<0.3.0,>=0.2.12 in ./.conda/lib/python3.11/site-packages (from langchain-community) (0.2.12)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.27 in ./.conda/lib/python3.11/site-packages (from langchain-community) (0.2.29)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in ./.conda/lib/python3.11/site-packages (from langchain-community) (0.1.98)\n",
      "Requirement already satisfied: numpy<2,>=1 in ./.conda/lib/python3.11/site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.conda/lib/python3.11/site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in ./.conda/lib/python3.11/site-packages (from langchain-community) (8.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.3.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.conda/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.21.3)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./.conda/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in ./.conda/lib/python3.11/site-packages (from langchain<0.3.0,>=0.2.12->langchain-community) (0.2.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in ./.conda/lib/python3.11/site-packages (from langchain<0.3.0,>=0.2.12->langchain-community) (2.8.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.conda/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.27->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.conda/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.27->langchain-community) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./.conda/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.27->langchain-community) (4.12.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.conda/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/lib/python3.11/site-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/lib/python3.11/site-packages (from requests<3,>=2->langchain-community) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/lib/python3.11/site-packages (from requests<3,>=2->langchain-community) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/lib/python3.11/site-packages (from requests<3,>=2->langchain-community) (2024.7.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./.conda/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.conda/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.27->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.12->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in ./.conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.12->langchain-community) (2.20.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.conda/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.19.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: numpy in ./.conda/lib/python3.11/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: torch==2.4.0 in ./.conda/lib/python3.11/site-packages (from torchvision) (2.4.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.conda/lib/python3.11/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: filelock in ./.conda/lib/python3.11/site-packages (from torch==2.4.0->torchvision) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.conda/lib/python3.11/site-packages (from torch==2.4.0->torchvision) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./.conda/lib/python3.11/site-packages (from torch==2.4.0->torchvision) (1.13.1)\n",
      "Requirement already satisfied: networkx in ./.conda/lib/python3.11/site-packages (from torch==2.4.0->torchvision) (3.3)\n",
      "Requirement already satisfied: jinja2 in ./.conda/lib/python3.11/site-packages (from torch==2.4.0->torchvision) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./.conda/lib/python3.11/site-packages (from torch==2.4.0->torchvision) (2024.5.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.conda/lib/python3.11/site-packages (from torch==2.4.0->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.conda/lib/python3.11/site-packages (from torch==2.4.0->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.conda/lib/python3.11/site-packages (from torch==2.4.0->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.conda/lib/python3.11/site-packages (from torch==2.4.0->torchvision) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.conda/lib/python3.11/site-packages (from torch==2.4.0->torchvision) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.conda/lib/python3.11/site-packages (from torch==2.4.0->torchvision) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.conda/lib/python3.11/site-packages (from torch==2.4.0->torchvision) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.conda/lib/python3.11/site-packages (from torch==2.4.0->torchvision) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.conda/lib/python3.11/site-packages (from torch==2.4.0->torchvision) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./.conda/lib/python3.11/site-packages (from torch==2.4.0->torchvision) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.conda/lib/python3.11/site-packages (from torch==2.4.0->torchvision) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in ./.conda/lib/python3.11/site-packages (from torch==2.4.0->torchvision) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0->torchvision) (12.6.20)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.conda/lib/python3.11/site-packages (from jinja2->torch==2.4.0->torchvision) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.conda/lib/python3.11/site-packages (from sympy->torch==2.4.0->torchvision) (1.3.0)\n",
      "Downloading torchvision-0.19.0-cp311-cp311-manylinux1_x86_64.whl (7.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchvision\n",
      "Successfully installed torchvision-0.19.0\n"
     ]
    }
   ],
   "source": [
    "!pip -q install langchain\n",
    "!pip -q install bitsandbytes accelerate xformers einops\n",
    "!pip -q install datasets loralib sentencepiece\n",
    "!pip -q install pypdf\n",
    "\n",
    "!pip install torch\n",
    "!pip -q install sentence_transformers\n",
    "!pip install chromadb\n",
    "!pip install openai\n",
    "!pip install tiktoken\n",
    "!pip install -U langchain-community\n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/LLMDrivenOntologyDev/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import Docx2txtLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "# from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from huggingface_hub import notebook_login\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import pipeline\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = '/content/drive/MyDrive/Wiki'\n",
    "\n",
    "files = os.listdir(directory_path)\n",
    "\n",
    "#OpenAI API Key stored as codespace secret\n",
    "\n",
    "llm=ChatOpenAI(temperature=0, model_name='gpt-4')\n",
    "\n",
    "allmodule_file_path = \"/content/drive/MyDrive/ModulesR/RR/WB_Schema_Relationships.txt\"\n",
    "m_path = \"/content/drive/MyDrive/ModulesR/\"\n",
    "\n",
    "modules_files = os.listdir(m_path)\n",
    "\n",
    "### All modules\n",
    "with open(allmodule_file_path, \"r\", encoding=\"utf-8\") as module_file:\n",
    "        module_content = module_file.read()\n",
    "### Module by module\n",
    "module_content1={}\n",
    "for f in modules_files:\n",
    "  if f.endswith(\".txt\"):\n",
    "    m_path1 = m_path+f\n",
    "\n",
    "    with open(m_path1, \"r\", encoding=\"utf-8\") as module_file:\n",
    "       file_name = f.replace(\".txt\", \"\")\n",
    "       module_content1[file_name] = module_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##gpt\n",
    "\n",
    "file = files[0] # to be edited for each file\n",
    "document=[]\n",
    "if file.endswith(\".pdf\"):\n",
    "  pdf_path=\"\"+file\n",
    "  loader=PyPDFLoader(pdf_path)\n",
    "  document.extend(loader.load())\n",
    "elif file.endswith('.docx') or file.endswith('.doc'):\n",
    "  doc_path=\"\"+file\n",
    "  loader=Docx2txtLoader(doc_path)\n",
    "  document.extend(loader.load())\n",
    "elif file.endswith('.txt'):\n",
    "\n",
    "    dir_path = os.path.join(directory_path, file)\n",
    "    text_path = dir_path\n",
    "    loader=TextLoader(text_path, encoding = 'UTF-8')\n",
    "    document.extend(loader.load())\n",
    "    document_splitter= RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0, separators=[\" \", \",\", \"\\n\"])\n",
    "    document_chunks=document_splitter.split_documents(document)\n",
    "    # embeddings = OpenAIEmbeddings()\n",
    "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "    vectordb=Chroma.from_documents(document_chunks,embedding=embeddings, persist_directory='./data')\n",
    "    vectordb.persist()\n",
    "    # llm=HuggingFacePipeline(pipeline=pipe, model_kwargs={'temperature':0})\n",
    "    memory=ConversationBufferMemory(memory_key='chat_history', return_messages=True,output_key='answer')\n",
    "    pdf_qa=ConversationalRetrievalChain.from_llm(llm=llm,\n",
    "                                              retriever=vectordb.as_retriever(search_kwargs={'k':10}),\n",
    "                                              verbose=False, memory=memory,return_source_documents=True)\n",
    "    # for j in module_content1:\n",
    "    result=pdf_qa({\"question\":\"Retrieve the related parts of the document that match with this ontology module, modules:\"+str(module_content)+ \"and populate the ontology based on the text in the form of the module relations like:  Name Record Module:1. has_Name(Agent, Name): has_Name(Joseph Vance Lewis, 'Joseph Vance Lewis') 2. has_Surname(Agent, Surname): has_Surname(Joseph Vance Lewis, 'Lewis') 3. has_First_Name(Agent, First_Name): has_First_Name(Joseph Vance Lewis, 'Joseph').Interagent Relationship Record Module: 1. has_Interagent_Relationship_Type_To(Agent, Relationship_Type): has_Interagent_Relationship_Type_To(Joseph Vance Lewis, 'Enslaver') 2. is_Relationship_To(Agent, Agent): is_Relationship_To(Joseph Vance Lewis, 'Bwril Nate'). Note: The Interagent Relationship Record Module describes the relationship to another agent who is an enslaver or owner. So use the keywords Enslaver or Owner to describe the relationship type. Sex Record Module: 1. has_Sex(Agent, Sex_Type): has_Sex(Joseph Vance Lewis, 'Male'). Occupation Record Module: 1. has_Occupation(Agent, Occupation): has_Occupation(Joseph Vance Lewis, 'Slave, Educator, Lawyer, and Autobiographer'). Age Record Module: 1. has_Age(Agent, Age): has_Age(Joseph Vance Lewis, 72) 2. has_BirthDate(Agent, Date_of_Birth): has_BirthDate(Joseph Vance Lewis, 'December 25, 1853') 3. has_DeathDate(Agent, Date_of_Death): has_DeathDate(Joseph Vance Lewis, 'April 24, 1925'). Person Status Record Module: 1. has_Person_Status (Agent, Person_Status): has_Person_Status(Joseph Vance Lewis, 'Enslaved Person, Lawyer') 2. has_Status_Generating_Event(Person_Status, Event): has_Status_Generating_Event(Enslaved Person, 'Sale of estate of Joseph Dubreuil'), has_Status_Generating_Event(Lawyer, 'Emancipation') 3. recorded_At(Person_Status_information, Event): recorded_At(Enslaved Person, 'Sale of estate of Joseph Dubreuil'), recorded_At(Lawyer, 'Emancipation'). Race Record Module: has_Race_or_Color(Agent, Race_or_Color): has_Race_or_Color(Joseph Vance Lewis, African-American).  Note: Skip the relations for which there is no information listed in the text file.The birth and death dates are mentioned in parenthesis after the agent name. For example: Joseph Vance Lewis (December 25, 1853 – April 24, 1925), was a slave who was freed. Here the Birth date is December 25, 1853 and Death date is April 24, 1925.\"})\n",
    "    summ = result['answer']\n",
    "\n",
    "    source_documents = result.get(\"source_documents\", [])\n",
    "    sen=\"\"\n",
    "    for doc in source_documents:\n",
    "        sen = sen + str(doc.page_content)\n",
    "        sen = sen + \"\\n\"\n",
    "    f_name = os.path.join(\"/content/drive/MyDrive/Wiki/NewResponses/GPT4_WB_notrestrictedToMAgent\",os.path.splitext(file)[0])\n",
    "    if not os.path.exists(f_name):\n",
    "      os.makedirs(f_name, exist_ok=True)\n",
    "    # response_file_path = os.path.join(f_name, f\"{os.path.splitext(file)[0]}_\"f\"{j}_retrieved.txt\")\n",
    "    # response_file_path1 = os.path.join(f_name, f\"{os.path.splitext(file)[0]}_\"f\"{j}_triples.txt\")\n",
    "    response_file_path = os.path.join(f_name, f\"{os.path.splitext(file)[0]}_retrieved.txt\")\n",
    "    response_file_path1 = os.path.join(f_name, f\"{os.path.splitext(file)[0]}_triples.txt\")\n",
    "    with open(response_file_path1, \"w\", encoding=\"utf-8\") as response_file1:\n",
    "        response_file1.write(summ)\n",
    "    with open(response_file_path, \"w\", encoding=\"utf-8\") as response_file:\n",
    "        response_file.write(sen)\n",
    "    torch.cuda.empty_cache()\n",
    "    del result\n",
    "    del summ\n",
    "    del source_documents\n",
    "    del sen\n",
    "    memory.clear()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = files[139]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##gpt\n",
    "\n",
    "file = f\n",
    "document=[]\n",
    "if file.endswith(\".pdf\"):\n",
    "  pdf_path=\"\"+file\n",
    "  loader=PyPDFLoader(pdf_path)\n",
    "  document.extend(loader.load())\n",
    "elif file.endswith('.docx') or file.endswith('.doc'):\n",
    "  doc_path=\"\"+file\n",
    "  loader=Docx2txtLoader(doc_path)\n",
    "  document.extend(loader.load())\n",
    "elif file.endswith('.txt'):\n",
    "\n",
    "    dir_path = os.path.join(directory_path, file)\n",
    "    text_path = dir_path\n",
    "    loader=TextLoader(text_path, encoding = 'UTF-8')\n",
    "    document.extend(loader.load())\n",
    "    document_splitter= RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0, separators=[\" \", \",\", \"\\n\"])\n",
    "    document_chunks=document_splitter.split_documents(document)\n",
    "    # embeddings = OpenAIEmbeddings()\n",
    "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "    vectordb=Chroma.from_documents(document_chunks,embedding=embeddings, persist_directory='./data')\n",
    "    vectordb.persist()\n",
    "    # llm=HuggingFacePipeline(pipeline=pipe, model_kwargs={'temperature':0})\n",
    "    memory=ConversationBufferMemory(memory_key='chat_history', return_messages=True,output_key='answer')\n",
    "    pdf_qa=ConversationalRetrievalChain.from_llm(llm=llm,\n",
    "                                              retriever=vectordb.as_retriever(search_kwargs={'k':10}),\n",
    "                                              verbose=False, memory=memory,return_source_documents=True)\n",
    "    # for j in module_content1:\n",
    "    result=pdf_qa({\"question\":\"Retrieve the related parts of the document that match with this ontology module, modules:\"+str(module_content)+ \"and populate the ontology based on the text in the form of the module relations like:  Name Record Module:1. has_Name(Agent, Name): has_Name(Joseph Vance Lewis, 'Joseph Vance Lewis') 2. has_Surname(Agent, Surname): has_Surname(Joseph Vance Lewis, 'Lewis') 3. has_First_Name(Agent, First_Name): has_First_Name(Joseph Vance Lewis, 'Joseph').Interagent Relationship Record Module: 1. has_Interagent_Relationship_Type_To(Agent, Relationship_Type): has_Interagent_Relationship_Type_To(Joseph Vance Lewis, 'Enslaver') 2. is_Relationship_To(Agent, Agent): is_Relationship_To(Joseph Vance Lewis, 'Bwril Nate'). Note: The Interagent Relationship Record Module describes the relationship to another agent who is an enslaver or owner. So use the keywords Enslaver or Owner to describe the relationship type. Sex Record Module: 1. has_Sex(Agent, Sex_Type): has_Sex(Joseph Vance Lewis, 'Male'). Occupation Record Module: 1. has_Occupation(Agent, Occupation): has_Occupation(Joseph Vance Lewis, 'Slave, Educator, Lawyer, and Autobiographer'). Age Record Module: 1. has_Age(Agent, Age): has_Age(Joseph Vance Lewis, 72) 2. has_BirthDate(Agent, Date_of_Birth): has_BirthDate(Joseph Vance Lewis, 'December 25, 1853') 3. has_DeathDate(Agent, Date_of_Death): has_DeathDate(Joseph Vance Lewis, 'April 24, 1925'). Person Status Record Module: 1. has_Person_Status (Agent, Person_Status): has_Person_Status(Joseph Vance Lewis, 'Enslaved Person, Lawyer') 2. has_Status_Generating_Event(Person_Status, Event): has_Status_Generating_Event(Enslaved Person, 'Sale of estate of Joseph Dubreuil'), has_Status_Generating_Event(Lawyer, 'Emancipation') 3. recorded_At(Person_Status_information, Event): recorded_At(Enslaved Person, 'Sale of estate of Joseph Dubreuil'), recorded_At(Lawyer, 'Emancipation'). Race Record Module: has_Race_or_Color(Agent, Race_or_Color): has_Race_or_Color(Joseph Vance Lewis, African-American).  Note: Skip the relations for which there is no information listed in the text file.The birth and death dates are mentioned in parenthesis after the agent name. For example: Joseph Vance Lewis (December 25, 1853 – April 24, 1925), was a slave who was freed. Here the Birth date is December 25, 1853 and Death date is April 24, 1925.\"})\n",
    "    summ = result['answer']\n",
    "\n",
    "    source_documents = result.get(\"source_documents\", [])\n",
    "    sen=\"\"\n",
    "    for doc in source_documents:\n",
    "        sen = sen + str(doc.page_content)\n",
    "        sen = sen + \"\\n\"\n",
    "    f_name = os.path.join(\"/content/drive/MyDrive/Wiki/NewResponses/GPT4_WB_notrestrictedToMAgent\",os.path.splitext(file)[0])\n",
    "    if not os.path.exists(f_name):\n",
    "      os.makedirs(f_name, exist_ok=True)\n",
    "    # response_file_path = os.path.join(f_name, f\"{os.path.splitext(file)[0]}_\"f\"{j}_retrieved.txt\")\n",
    "    # response_file_path1 = os.path.join(f_name, f\"{os.path.splitext(file)[0]}_\"f\"{j}_triples.txt\")\n",
    "    response_file_path = os.path.join(f_name, f\"{os.path.splitext(file)[0]}_retrieved.txt\")\n",
    "    response_file_path1 = os.path.join(f_name, f\"{os.path.splitext(file)[0]}_triples.txt\")\n",
    "    with open(response_file_path1, \"w\", encoding=\"utf-8\") as response_file1:\n",
    "        response_file1.write(summ)\n",
    "    with open(response_file_path, \"w\", encoding=\"utf-8\") as response_file:\n",
    "        response_file.write(sen)\n",
    "    torch.cuda.empty_cache()\n",
    "    del result\n",
    "    del summ\n",
    "    del source_documents\n",
    "    del sen\n",
    "    memory.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gpt\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "vectordb=Chroma.from_documents(document_chunks,embedding=embeddings, persist_directory='./data')\n",
    "vectordb.persist()\n",
    "# llm=HuggingFacePipeline(pipeline=pipe, model_kwargs={'temperature':0})\n",
    "memory=ConversationBufferMemory(memory_key='chat_history', return_messages=True,output_key='answer')\n",
    "pdf_qa=ConversationalRetrievalChain.from_llm(llm=llm,\n",
    "                                          retriever=vectordb.as_retriever(search_kwargs={'k':10}),\n",
    "                                          verbose=False, memory=memory,return_source_documents=True)\n",
    "# for j in module_content1:\n",
    "result=pdf_qa({\"question\":\"Retrieve the related parts of the document that match with this ontology module that is related just for an agent that the document starts with the name, modules:\"+str(module_content)+ \"and populate the ontology based on the text in the form of the module relations like:  Name Record Module:1. has_Name(Agent, Name): has_Name(Joseph Vance Lewis, 'Joseph Vance Lewis') 2. has_Surname(Agent, Surname): has_Surname(Joseph Vance Lewis, 'Lewis') 3. has_First_Name(Agent, First_Name): has_First_Name(Joseph Vance Lewis, 'Joseph').Interagent Relationship Record Module: 1. has_Interagent_Relationship_Type_To(Agent, Relationship_Type): has_Interagent_Relationship_Type_To(Joseph Vance Lewis, 'Enslaver') 2. is_Relationship_To(Agent, Agent): is_Relationship_To(Joseph Vance Lewis, 'Bwril Nate'). Note: The Interagent Relationship Record Module describes the relationship to another agent who is an enslaver or owner. So use the keywords Enslaver or Owner to describe the relationship type. Sex Record Module: 1. has_Sex(Agent, Sex_Type): has_Sex(Joseph Vance Lewis, 'Male'). Occupation Record Module: 1. has_Occupation(Agent, Occupation): has_Occupation(Joseph Vance Lewis, 'Slave, Educator, Lawyer, and Autobiographer'). Age Record Module: 1. has_Age(Agent, Age): has_Age(Joseph Vance Lewis, 72) 2. has_BirthDate(Agent, Date_of_Birth): has_BirthDate(Joseph Vance Lewis, 'December 25, 1853') 3. has_DeathDate(Agent, Date_of_Death): has_DeathDate(Joseph Vance Lewis, 'April 24, 1925'). Person Status Record Module: 1. has_Person_Status (Agent, Person_Status): has_Person_Status(Joseph Vance Lewis, 'Enslaved Person, Lawyer') 2. has_Status_Generating_Event(Person_Status, Event): has_Status_Generating_Event(Enslaved Person, 'Sale of estate of Joseph Dubreuil'), has_Status_Generating_Event(Lawyer, 'Emancipation') 3. recorded_At(Person_Status_information, Event): recorded_At(Enslaved Person, 'Sale of estate of Joseph Dubreuil'), recorded_At(Lawyer, 'Emancipation'). Race Record Module: has_Race_or_Color(Agent, Race_or_Color): has_Race_or_Color(Joseph Vance Lewis, African-American).  Note: Skip the relations for which there is no information listed in the text file.The birth and death dates are mentioned in parenthesis after the agent name. For example: Joseph Vance Lewis (December 25, 1853 – April 24, 1925), was a slave who was freed. Here the Birth date is December 25, 1853 and Death date is April 24, 1925.\"})\n",
    "summ = result['answer']\n",
    "\n",
    "source_documents = result.get(\"source_documents\", [])\n",
    "sen=\"\"\n",
    "for doc in source_documents:\n",
    "    sen = sen + str(doc.page_content)\n",
    "    sen = sen + \"\\n\"\n",
    "f_name = os.path.join(\"/content/drive/MyDrive/Wiki/NewResponses/GPT4_WB_MainAgent\",os.path.splitext(file)[0])\n",
    "if not os.path.exists(f_name):\n",
    "  os.makedirs(f_name, exist_ok=True)\n",
    "# response_file_path = os.path.join(f_name, f\"{os.path.splitext(file)[0]}_\"f\"{j}_retrieved.txt\")\n",
    "# response_file_path1 = os.path.join(f_name, f\"{os.path.splitext(file)[0]}_\"f\"{j}_triples.txt\")\n",
    "response_file_path = os.path.join(f_name, f\"{os.path.splitext(file)[0]}_retrieved.txt\")\n",
    "response_file_path1 = os.path.join(f_name, f\"{os.path.splitext(file)[0]}_triples.txt\")\n",
    "with open(response_file_path1, \"w\", encoding=\"utf-8\") as response_file1:\n",
    "    response_file1.write(summ)\n",
    "with open(response_file_path, \"w\", encoding=\"utf-8\") as response_file:\n",
    "    response_file.write(sen)\n",
    "torch.cuda.empty_cache()\n",
    "del result\n",
    "del summ\n",
    "del source_documents\n",
    "del sen\n",
    "memory.clear()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login('Hugging face token',add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
    "\n",
    "llm1 = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    # cache_dir=\"/data/yash/base_models\",\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "                                          # cache_dir=\"/data/yash/base_models\"\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "pipe=pipeline(\"text-generation\",\n",
    "              model=llm1,\n",
    "              tokenizer=tokenizer,\n",
    "              torch_dtype=torch.bfloat16,\n",
    "              device_map='auto',\n",
    "              max_new_tokens=2048,\n",
    "              min_new_tokens=-1,\n",
    "              top_k=30\n",
    "\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "llm11 =HuggingFacePipeline(pipeline=pipe, model_kwargs={'temperature':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#llama3\n",
    "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "vectordb=Chroma.from_documents(document_chunks,embedding=embeddings, persist_directory='./data')\n",
    "vectordb.persist()\n",
    "# llm=HuggingFacePipeline(pipeline=pipe, model_kwargs={'temperature':0})\n",
    "memory=ConversationBufferMemory(memory_key='chat_history', return_messages=True,output_key='answer')\n",
    "pdf_qa=ConversationalRetrievalChain.from_llm(llm=llm11,\n",
    "                                          retriever=vectordb.as_retriever(search_kwargs={'k':10}),\n",
    "                                          verbose=False, memory=memory,return_source_documents=True)\n",
    "# for j in module_content1:\n",
    "result=pdf_qa({\"question\":\"Retrieve the related parts of the document that match with this ontology module that is related just for an agent that the document starts with the name, modules:\"+str(module_content)+ \"and populate the ontology based on the text in the form of the module relations like:  Name Record Module:1. has_Name(Agent, Name): has_Name(Joseph Vance Lewis, 'Joseph Vance Lewis') 2. has_Surname(Agent, Surname): has_Surname(Joseph Vance Lewis, 'Lewis') 3. has_First_Name(Agent, First_Name): has_First_Name(Joseph Vance Lewis, 'Joseph').Interagent Relationship Record Module: 1. has_Interagent_Relationship_Type_To(Agent, Relationship_Type): has_Interagent_Relationship_Type_To(Joseph Vance Lewis, 'Enslaver') 2. is_Relationship_To(Agent, Agent): is_Relationship_To(Joseph Vance Lewis, 'Bwril Nate'). Note: The Interagent Relationship Record Module describes the relationship to another agent who is an enslaver or owner. So use the keywords Enslaver or Owner to describe the relationship type. Sex Record Module: 1. has_Sex(Agent, Sex_Type): has_Sex(Joseph Vance Lewis, 'Male'). Occupation Record Module: 1. has_Occupation(Agent, Occupation): has_Occupation(Joseph Vance Lewis, 'Slave, Educator, Lawyer, and Autobiographer'). Age Record Module: 1. has_Age(Agent, Age): has_Age(Joseph Vance Lewis, 72) 2. has_BirthDate(Agent, Date_of_Birth): has_BirthDate(Joseph Vance Lewis, 'December 25, 1853') 3. has_DeathDate(Agent, Date_of_Death): has_DeathDate(Joseph Vance Lewis, 'April 24, 1925'). Person Status Record Module: 1. has_Person_Status (Agent, Person_Status): has_Person_Status(Joseph Vance Lewis, 'Enslaved Person, Lawyer') 2. has_Status_Generating_Event(Person_Status, Event): has_Status_Generating_Event(Enslaved Person, 'Sale of estate of Joseph Dubreuil'), has_Status_Generating_Event(Lawyer, 'Emancipation') 3. recorded_At(Person_Status_information, Event): recorded_At(Enslaved Person, 'Sale of estate of Joseph Dubreuil'), recorded_At(Lawyer, 'Emancipation'). Race Record Module: has_Race_or_Color(Agent, Race_or_Color): has_Race_or_Color(Joseph Vance Lewis, African-American).  Note: Skip the relations for which there is no information listed in the text file.The birth and death dates are mentioned in parenthesis after the agent name. For example: Joseph Vance Lewis (December 25, 1853 – April 24, 1925), was a slave who was freed. Here the Birth date is December 25, 1853 and Death date is April 24, 1925.\"})\n",
    "summ = result['answer']\n",
    "start_pos = summ.find(\"Helpful Answer:\")\n",
    "\n",
    "# Extract the text after \"Helpful Answer:\"\n",
    "if start_pos != -1:\n",
    "    helpful_answer_text = summ[start_pos + len(\"Helpful Answer:\"):].strip()\n",
    "else:\n",
    "    helpful_answer_text = \"\"\n",
    "\n",
    "# print(helpful_answer_text)\n",
    "\n",
    "\n",
    "source_documents = result.get(\"source_documents\", [])\n",
    "sen=\"\"\n",
    "for doc in source_documents:\n",
    "    sen = sen + str(doc.page_content)\n",
    "    sen = sen + \"\\n\"\n",
    "f_name = os.path.join(\"/content/drive/MyDrive/Wiki/NewResponses/llama_WB_MainAgent\",os.path.splitext(file)[0])\n",
    "if not os.path.exists(f_name):\n",
    "  os.makedirs(f_name, exist_ok=True)\n",
    "# response_file_path = os.path.join(f_name, f\"{os.path.splitext(file)[0]}_\"f\"{j}_retrieved.txt\")\n",
    "# response_file_path1 = os.path.join(f_name, f\"{os.path.splitext(file)[0]}_\"f\"{j}_triples.txt\")\n",
    "response_file_path = os.path.join(f_name, f\"{os.path.splitext(file)[0]}_retrieved.txt\")\n",
    "response_file_path1 = os.path.join(f_name, f\"{os.path.splitext(file)[0]}_triples.txt\")\n",
    "with open(response_file_path1, \"w\", encoding=\"utf-8\") as response_file1:\n",
    "    response_file1.write(helpful_answer_text)\n",
    "with open(response_file_path, \"w\", encoding=\"utf-8\") as response_file:\n",
    "    response_file.write(sen)\n",
    "torch.cuda.empty_cache()\n",
    "del result\n",
    "del summ\n",
    "del helpful_answer_text\n",
    "del source_documents\n",
    "del sen\n",
    "memory.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# llama3\n",
    "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "vectordb=Chroma.from_documents(document_chunks,embedding=embeddings, persist_directory='./data')\n",
    "vectordb.persist()\n",
    "# llm=HuggingFacePipeline(pipeline=pipe, model_kwargs={'temperature':0})\n",
    "memory=ConversationBufferMemory(memory_key='chat_history', return_messages=True,output_key='answer')\n",
    "pdf_qa=ConversationalRetrievalChain.from_llm(llm=llm11,\n",
    "                                          retriever=vectordb.as_retriever(search_kwargs={'k':10}),\n",
    "                                          verbose=False, memory=memory,return_source_documents=True)\n",
    "# for j in module_content1:\n",
    "result=pdf_qa({\"question\":\"Retrieve the related parts of the document that match with this ontology module that is related just for an agent that the document starts with the name, modules:\"+str(module_content)+ \"and populate the ontology based on the text in the form of the module relations like:  Name Record Module:1. has_Name(Agent, Name): has_Name(Joseph Vance Lewis, 'Joseph Vance Lewis') 2. has_Surname(Agent, Surname): has_Surname(Joseph Vance Lewis, 'Lewis') 3. has_First_Name(Agent, First_Name): has_First_Name(Joseph Vance Lewis, 'Joseph').Interagent Relationship Record Module: 1. has_Interagent_Relationship_Type_To(Agent, Relationship_Type): has_Interagent_Relationship_Type_To(Joseph Vance Lewis, 'Enslaver') 2. is_Relationship_To(Agent, Agent): is_Relationship_To(Joseph Vance Lewis, 'Bwril Nate'). Note: The Interagent Relationship Record Module describes the relationship to another agent who is an enslaver or owner. So use the keywords Enslaver or Owner to describe the relationship type. Sex Record Module: 1. has_Sex(Agent, Sex_Type): has_Sex(Joseph Vance Lewis, 'Male'). Occupation Record Module: 1. has_Occupation(Agent, Occupation): has_Occupation(Joseph Vance Lewis, 'Slave, Educator, Lawyer, and Autobiographer'). Age Record Module: 1. has_Age(Agent, Age): has_Age(Joseph Vance Lewis, 72) 2. has_BirthDate(Agent, Date_of_Birth): has_BirthDate(Joseph Vance Lewis, 'December 25, 1853') 3. has_DeathDate(Agent, Date_of_Death): has_DeathDate(Joseph Vance Lewis, 'April 24, 1925'). Person Status Record Module: 1. has_Person_Status (Agent, Person_Status): has_Person_Status(Joseph Vance Lewis, 'Enslaved Person, Lawyer') 2. has_Status_Generating_Event(Person_Status, Event): has_Status_Generating_Event(Enslaved Person, 'Sale of estate of Joseph Dubreuil'), has_Status_Generating_Event(Lawyer, 'Emancipation') 3. recorded_At(Person_Status_information, Event): recorded_At(Enslaved Person, 'Sale of estate of Joseph Dubreuil'), recorded_At(Lawyer, 'Emancipation'). Race Record Module: has_Race_or_Color(Agent, Race_or_Color): has_Race_or_Color(Joseph Vance Lewis, African-American).  Note: Skip the relations for which there is no information listed in the text file.The birth and death dates are mentioned in parenthesis after the agent name. For example: Joseph Vance Lewis (December 25, 1853 – April 24, 1925), was a slave who was freed. Here the Birth date is December 25, 1853 and Death date is April 24, 1925.\"})\n",
    "summ = result['answer']\n",
    "start_pos = summ.find(\"Helpful Answer:\")\n",
    "\n",
    "# Extract the text after \"Helpful Answer:\"\n",
    "if start_pos != -1:\n",
    "    helpful_answer_text = summ[start_pos + len(\"Helpful Answer:\"):].strip()\n",
    "else:\n",
    "    helpful_answer_text = \"\"\n",
    "\n",
    "\n",
    "source_documents = result.get(\"source_documents\", [])\n",
    "sen=\"\"\n",
    "for doc in source_documents:\n",
    "    sen = sen + str(doc.page_content)\n",
    "    sen = sen + \"\\n\"\n",
    "f_name = os.path.join(\"/content/drive/MyDrive/Wiki/NewResponses/llama_WB_notrestrictedToMAgent\",os.path.splitext(file)[0])\n",
    "if not os.path.exists(f_name):\n",
    "  os.makedirs(f_name, exist_ok=True)\n",
    "# response_file_path = os.path.join(f_name, f\"{os.path.splitext(file)[0]}_\"f\"{j}_retrieved.txt\")\n",
    "# response_file_path1 = os.path.join(f_name, f\"{os.path.splitext(file)[0]}_\"f\"{j}_triples.txt\")\n",
    "response_file_path = os.path.join(f_name, f\"{os.path.splitext(file)[0]}_retrieved.txt\")\n",
    "response_file_path1 = os.path.join(f_name, f\"{os.path.splitext(file)[0]}_triples.txt\")\n",
    "with open(response_file_path1, \"w\", encoding=\"utf-8\") as response_file1:\n",
    "    response_file1.write(helpful_answer_text)\n",
    "with open(response_file_path, \"w\", encoding=\"utf-8\") as response_file:\n",
    "    response_file.write(sen)\n",
    "torch.cuda.empty_cache()\n",
    "del result\n",
    "del summ\n",
    "del helpful_answer_text\n",
    "del source_documents\n",
    "del sen\n",
    "memory.clear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
