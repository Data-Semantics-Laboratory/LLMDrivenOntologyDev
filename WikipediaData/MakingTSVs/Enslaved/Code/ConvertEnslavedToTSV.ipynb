{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making TSVs from Enslaved TTL Files for individuals that exist on both Enslaved and Wikipedia\n",
    "\n",
    "### Each TSV should contain the triples from Wikidata as:\n",
    "Subject ->  Predicate   -> Object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tqdm\n",
    "import os\n",
    "import requests\n",
    "from icecream import ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| len(props): 83\n",
      "ic| requests.get(baseURL + \"P31\").json()[\"entities\"][\"P31\"][\"labels\"][\"en\"][\"value\"]: 'hasSex'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hasSex'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dictionary to store found QIDs to avoid redundant API calls\n",
    "foundQIDs = {}\n",
    "QIDCount = {}\n",
    "\n",
    "# Dictionary to store found Props to avoid redundant API calls\n",
    "props = {}\n",
    "with open(\"enslavedPropList.tsv\", \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip().split(\"\\t\")\n",
    "        props.update({line[0]: line[1]})\n",
    "PropsCount = {}\n",
    "ic(len(props))\n",
    "\n",
    "# Stores any errors to then print out at the end\n",
    "errors = []\n",
    "\n",
    "#Base Query URL for  Enslaved\n",
    "baseURL = \"https://lod.enslaved.org/w/api.php?action=wbgetentities&format=json&ids=\"\n",
    "\n",
    "ic(requests.get(baseURL + \"P31\").json()[\"entities\"][\"P31\"][\"labels\"][\"en\"][\"value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Queries EnslavedProps.json to find the property label\n",
    "def findQID(str):\n",
    "    if str in foundQIDs:\n",
    "        #print(f\"Found {str}: {foundQIDs[str]}\")\n",
    "        QIDCount.update({str: QIDCount.get(str, 0) + 1})\n",
    "        return foundQIDs[str]\n",
    "    else:\n",
    "        endURL = f\"{str}\"\n",
    "        response = requests.get(baseURL + endURL)\n",
    "        if response.status_code != 200:\n",
    "            errors.append(f\"Error: {response.status_code} for {str}\")\n",
    "            return None\n",
    "        else:\n",
    "            label = response.json()[\"entities\"][str][\"labels\"][\"en\"][\"value\"]\n",
    "            foundQIDs.update({str: label})\n",
    "            QIDCount.update({str: 0})\n",
    "            return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Queries WikidataProps.json to find the property label\n",
    "def findProperty(str):\n",
    "    if str in props:\n",
    "#       #print(f\"Found Property {str} in foundProps: {foundProps[str]}\")\n",
    "        PropsCount.update({str: PropsCount.get(str, 0) + 1})\n",
    "        return props[str]\n",
    "    else:\n",
    "        endURL = f\"{str}\"\n",
    "        response = requests.get(baseURL + endURL)\n",
    "        if response.status_code != 200:\n",
    "            errors.append(f\"findProperty Error: {response.status_code} for {str}\")\n",
    "            return None\n",
    "        else:\n",
    "            label = response.json()[\"entities\"][str][\"labels\"][\"en\"][\"value\"]\n",
    "            props.update({str: label})\n",
    "            PropsCount.update({str: 0})\n",
    " #          #print(foundProps[str])\n",
    "            return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/52 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| f\"Error with {filename}: {e}\": 'Error with Prince_Estabrook.ttl: list index out of range'\n",
      "100%|██████████| 52/52 [00:20<00:00,  2.54it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Dictionary to store the statement nodes that contain references and qualifiers\n",
    "nodes = {}\n",
    "\n",
    "# Stores any errors to then print out at the end\n",
    "errors = []\n",
    "\n",
    "# Processes statements that start with WDQ:\n",
    "def processED_Q(chunk):\n",
    "    subject = findQID(chunk.split(\" \")[0].split(\":\")[1])\n",
    "    predicate = chunk.split(\" \")[1]\n",
    "    if predicate.startswith(\"ep:\") | predicate.startswith(\"edt:p\"):\n",
    "        predicate = findProperty(predicate.split(\":\")[1])\n",
    "    object = chunk.split(\" \")[2]\n",
    "    if object.startswith(\"wds:\"):\n",
    "        statementNodes.append(chunk.split(\"wds:\")[1])\n",
    "    new_row = {\"subject\": subject, \"predicate\": predicate, \"object\": object}\n",
    "    statementList.append(new_row)\n",
    "    #ic(new_row)\n",
    "\n",
    "# Processes statements that start with WDQ:\n",
    "def processEDT_P(chunk):\n",
    "    subject = chunk.split(\" \")[0]\n",
    "    subject = findProperty(subject.split(\":\")[1])\n",
    "    predicate = chunk.split(\" \")[1]\n",
    "    if predicate.startswith(\"ep:\") or predicate.startswith(\"edt:\"):\n",
    "        predicate = findProperty(predicate.split(\":\")[1])\n",
    "    new_row = {\"subject\": subject, \"predicate\": predicate, \"object\": object}\n",
    "    statementList.append(new_row)\n",
    "    #ic(new_row)\n",
    "\n",
    "def processS(chunk):\n",
    "    statementID = chunk.split(\";\")[0].split(\" \")[0]\n",
    "    statementProp = findProperty(chunk.split(\";\")[2].split(\" \")[2].split(\":\")[1])\n",
    "    #print(statementProp)\n",
    "    #print(chunk))\n",
    "    statementValue = chunk.split(\";\")[2].split(\" \")[3]\n",
    "    if statementValue.startswith(\"ed:\"):\n",
    "        statementValue = findQID(statementValue.split(\":\")[1])\n",
    "    #print(f\"{statementProp} {statementValue}\")\n",
    "    for statement in statementList:\n",
    "        if statement[\"object\"] == statementID:\n",
    "            statement[\"object\"] = statementValue\n",
    "\n",
    "# Processes statements that start with S:\n",
    "# def processStatementNode(nodeID):\n",
    "#    statementProp = findProperty(chunk.split(\";\")[2].split(\" \")[2].split(\":\")[1])\n",
    "#    #print(statementProp)\n",
    "#    #print(chunk))\n",
    "#    statementValue = chunk.split(\";\")[2].split(\" \")[3]\n",
    "#    if statementValue.startswith(\"ed:\"):\n",
    "#        statementValue = findQID(statementValue.split(\":\")[1])\n",
    "#    #print(f\"{statementProp} {statementValue}\")\n",
    "#    for statement in statementList:\n",
    "#        if statement[\"object\"] == statementID:\n",
    "#            statement[\"object\"] = statementValue\n",
    "\n",
    "\n",
    "# Processes references (not implemented yet)\n",
    "#def processRef(chunk):\n",
    "#   #print(chunk)\n",
    "\n",
    "\n",
    "# Processes chunks based on their statement type\n",
    "def processChunk(chunk):\n",
    "    chunk = chunk.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "    if (\n",
    "        chunk.startswith(\" <https://\")\n",
    "        | chunk.startswith(\" edata:\")\n",
    "        | chunk.startswith(\"@prefix\")\n",
    "        | chunk.startswith(\"xsd:\")\n",
    "        | chunk.startswith(\"ontolex:\")\n",
    "        | chunk.startswith(\"dct:\")\n",
    "        | chunk.startswith(\"rdfs:\")\n",
    "        | chunk.startswith(\"owl:\")\n",
    "        | chunk.startswith(\"skos:\")\n",
    "        | chunk.startswith(\"schema:\")\n",
    "        | chunk.startswith(\"cc:\")\n",
    "        | chunk.startswith(\"geo:\")\n",
    "        | chunk.startswith(\"prov:\")\n",
    "        | chunk.startswith(\"data\")\n",
    "        | chunk.startswith(\"<\")\n",
    "    ):\n",
    "        pass\n",
    "    else:\n",
    "        chunk = chunk.lstrip()\n",
    "        output.write(chunk + \"\\n\")\n",
    "        if chunk.startswith(\"ed:Q\"):\n",
    "            processED_Q(chunk)\n",
    "        if chunk.startswith(\"edt:P\"):\n",
    "            processEDT_P(chunk)\n",
    "        if chunk.startswith(\"eds:\"):\n",
    "            statementNodes.append(chunk)\n",
    "            processS(chunk)\n",
    "        if chunk.startswith(\"edref:\"):\n",
    "            referenceNodes.append(chunk)\n",
    "        if chunk.startswith(\"edv:\"):\n",
    "            valueNodes.append(chunk)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "ttl_files_dir = \"../EnslavedTTL/\"\n",
    "# Processes each file in the directory, line by line, and writes the results to a TSV file\n",
    "for filename in tqdm.tqdm((os.listdir(ttl_files_dir))):\n",
    "#for filename in os.listdir(ttl_files_dir):\n",
    "    statementList = []\n",
    "    chunkList = []\n",
    "    statementNodes = []\n",
    "    referenceNodes = []\n",
    "    valueNodes = []\n",
    "\n",
    "    try:\n",
    "        if filename.endswith(\".ttl\"):\n",
    "            file_path = os.path.join(ttl_files_dir, filename)\n",
    "            with open(file_path, \"r\") as file:\n",
    "                chunk = \"\"\n",
    "                name = filename.split(\".\")[0].replace(\" \", \"_\")\n",
    "                output = open(f\"../EnslavedTSV/Chunks/{name}-chunks.txt\", \"a\")\n",
    "\n",
    "                # Iterate over each line in the file\n",
    "                for line in file:\n",
    "                    # Append the line to the current chunk\n",
    "                    chunk += line\n",
    "\n",
    "                    # Check if the line ends with a '.'\n",
    "                    if line.strip().endswith(\".\"):\n",
    "                        # Process the chunk\n",
    "                        processChunk(chunk)\n",
    "                        # Reset the chunk\n",
    "                        chunk = \"\"\n",
    "\n",
    "            with open(f\"../EnslavedTSV/{name}.tsv\",\"w\") as tsvFile:\n",
    "                name = statementList[0][\"subject\"]\n",
    "                for statement in statementList:\n",
    "                    #print(statement)\n",
    "                    if statement[\"subject\"] == name:\n",
    "                        tsvFile.write(\n",
    "                            f\"{statement['subject']}\\t{statement['predicate']}\\t{statement['object']}\\n\"\n",
    "                        )\n",
    "    except Exception as e:\n",
    "        ic(f\"Error with {filename}: {e}\")\n",
    "        errors.append(f\"Error with {filename}: {e}\")\n",
    "        pass\n",
    "        \n",
    "        output.close()\n",
    "        tsvFile.close()\n",
    "        file.close()\n",
    "    #for node in statementNodes:\n",
    "    #   #print(node)\n",
    "\n",
    "#with open(\"properties.txt\", \"w\") as propOutput:\n",
    "#    for prop in sorted(props):\n",
    "#        propOutput.write(f\"{prop}\\t{props[prop]}\\t{PropsCount[prop]}\\n\")\n",
    "#    propOutput.close()\n",
    "    \n",
    "with open(\"QIDs.txt\", \"w\") as qidOutput:\n",
    "    for qid in sorted(foundQIDs):\n",
    "        qidOutput.write(f\"{qid}\\t{foundQIDs[qid]}\\t{QIDCount[qid]}\\n\")\n",
    "    qidOutput.close()\n",
    "    \n",
    "with open(\"errors.txt\", \"w\") as file:\n",
    "    file.write(f\"{filename} Errors:\" + \"*\"*50 + \"\\n\\n\")\n",
    "    for error in errors:\n",
    "        file.write(error + \"\\n\")\n",
    "    file.write(\"\\n\\n\")\n",
    "    file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
