{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd1a995",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start Jupyter in the environment 'Python 3.8.18 ('agnlp') (~/miniforge3/envs/agnlp/bin/python)'. \n",
      "ImportError: cannot import name 'notebookapp' from 'notebook' (/Users/ngautam/miniforge3/envs/agnlp/lib/python3.8/site-packages/notebook/__init__.py) \n",
      "View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Generate Holistic Summary\n",
    "import os\n",
    "import openai\n",
    "\n",
    "# Define your OpenAI API key\n",
    "api_key = \"sk-proj-ubS0yXkdcbS919SL0iqxT3BlbkFJ8QnPPfMCnpbYZRR2bUGM\"\n",
    "openai.api_key = api_key\n",
    "\n",
    "# Function to generate summaries based on a query prompt using GPT-3\n",
    "def generate_summary_with_modules(query, text_chunks, module_file_path):\n",
    "    # Read module file content\n",
    "    with open(module_file_path, \"r\", encoding=\"utf-8\") as module_file:\n",
    "        module_content = module_file.read()\n",
    "\n",
    "    summaries = []\n",
    "    chunk = \"\"\n",
    "    for paragraph in text_chunks:\n",
    "        # Check if adding the next paragraph will exceed the chunk size limit\n",
    "        if len(chunk) + len(paragraph) <= 8000:\n",
    "            # Add the paragraph to the current chunk\n",
    "            chunk += paragraph + \"\\n\"\n",
    "        else:\n",
    "            # Generate summary for the current chunk\n",
    "            summary = generate_summary(query, chunk, module_content)\n",
    "            summaries.append(summary)\n",
    "            # Start a new chunk with the current paragraph\n",
    "            chunk = paragraph + \"\\n\"\n",
    "\n",
    "    # Generate summary for the remaining chunk, if any\n",
    "    if chunk:\n",
    "        summary = generate_summary(query, chunk, module_content)\n",
    "        summaries.append(summary)\n",
    "\n",
    "    return summaries\n",
    "\n",
    "# Function to generate summary for a single chunk\n",
    "def generate_summary(query, chunk, module_content):\n",
    "    # Concatenate the query, text chunk, and module content as input\n",
    "    prompt = f\"\"\"{query},by keeping the relevant information from these modules (if any):\n",
    "    {module_content}.\n",
    "    The birth and death dates are mentioned in parenthesis after the agent name. For example : Joseph Vance Lewis (December 25, 1853 – April 24, 1925), was a slave who was freed. Here the Birth date is December 25, 1853 and Death date is April 24, 1925. So keep those informations in the summery.\n",
    "    The given text to summarize:{chunk}\"\"\"\n",
    "\n",
    "    # Use GPT-4 to generate summary\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",  # Choose the gpt-4 model\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0,  # Adjust the temperature for generating diverse responses\n",
    "        stop=\"\\n\"  # Stop generation at a newline to ensure a concise summary\n",
    "    )\n",
    "\n",
    "    # Extract and return the generated summary from the response\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "# Function to read input text from file and split into paragraphs\n",
    "def read_text_and_split_into_paragraphs(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text = file.read()\n",
    "    # Split text into paragraphs\n",
    "    paragraphs = text.split(\"\\n\")\n",
    "    return paragraphs\n",
    "\n",
    "# Function to generate holistic summary for a text file\n",
    "def generate_holistic_summary(file_name, context, module_content, holistic_query):\n",
    "    # Use GPT-4 to generate a holistic summary\n",
    "    holistic_response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"{holistic_query}\\n{context}\"}],\n",
    "        temperature=0,  # Adjust the temperature for generating diverse responses\n",
    "        stop=\"\\n\"  # Stop generation at a newline to ensure a concise summary\n",
    "    )\n",
    "\n",
    "    # Extract the generated holistic summary from the response\n",
    "    holistic_summary = holistic_response.choices[0].message.content.strip()\n",
    "\n",
    "    # Save the holistic summary to a file\n",
    "    response_file_path = os.path.join(\"OntoLLM/summaries\", f\"{os.path.splitext(file_name)[0]}_summary.txt\")\n",
    "    with open(response_file_path, \"w\", encoding=\"utf-8\") as response_file:\n",
    "        response_file.write(holistic_summary)\n",
    "\n",
    "# Main function to process text files and generate summaries\n",
    "def process_text_files(folder_path, module_file_path):\n",
    "    # Create the responses folder if it doesn't exist\n",
    "    os.makedirs(\"OntoLLM/summaries\", exist_ok=True)\n",
    "\n",
    "    # Define your query prompt\n",
    "    query = \"Summarize the following text\"\n",
    "\n",
    "    # Read module file content\n",
    "    with open(module_file_path, \"r\", encoding=\"utf-8\") as module_file:\n",
    "        module_content = module_file.read()\n",
    "\n",
    "    # Define the query prompt for the holistic summary\n",
    "    holistic_query = f\"\"\"Here is an example summary that highlights the key points from a text file based on the given module. The text file discusses a single agent or person who is introduced initially. So, the summary should focus solely on that particular agent. The birth and death dates are mentioned in parenthesis after the agent name. For example : Joseph Vance Lewis (December 25, 1853 – April 24, 1925), was a slave who was freed. Here the Birth date is December 25, 1853 and Death date is April 24, 1925. The Interagent Relationship Record Module describes whether the agent has a relationship with another Enslaver or Owner. The Person Status Module indicates whether the agent is an enslaved person and mentions any status-generating events.\n",
    "\n",
    "                    Example summary: The text provides information about Joseph Vance Lewis, who was born on December 25, 1853, and died on April 24, 1925. From the Name Record Module, his first name is Joseph, surname is Lewis, and he doesn't have an alternate name mentioned. The Interagent Relationship Record Module doesn't apply as no other agent who is an enslaver or owner is mentioned. The Participant Role Record Module doesn't apply as he was not a participant in any event. The Sex Record Module identifies him as male. From the Occupation Record Module, his occupation was a slave, educator, lawyer, and autobiographer. The Age Record Module indicates he was born on December 25, 1853, and died on April 24, 1925. So, his age was 72. The Race Record Module and Ethnolinguistic Descriptor Record Module are not explicitly mentioned in the text. From the Person Status Record Module, he was an enslaved person and was sold during the Sale of estate of Joseph Dubreuil, indicating the status-generating event. His status changed from being a slave to a lawyer, with emancipation being the status-generating event.\n",
    "\n",
    "                    Please provide a holistic summary from the given text that follows the format of the example summary and keeps the relevant information from these modules (if any): \n",
    "                    {module_content}. \n",
    "                    The given text is: \"\"\"\n",
    "    # Iterate over text files in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            # Path to the current text file\n",
    "            text_file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "            # Read input text from file and split into paragraphs\n",
    "            paragraphs = read_text_and_split_into_paragraphs(text_file_path)\n",
    "\n",
    "            # Generate summaries based on the query prompt, input text paragraphs, and module file\n",
    "            summaries = generate_summary_with_modules(query, paragraphs, module_file_path)\n",
    "\n",
    "            # Concatenate all individual summaries into context for generating holistic summary\n",
    "            context = \"\\n\".join(summaries)\n",
    "\n",
    "            # Generate and save holistic summary for the current text file\n",
    "            generate_holistic_summary(file_name, context, module_content, holistic_query)\n",
    "\n",
    "            print(f\"Holistic summary generated and saved for {file_name}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to the folder containing input text files\n",
    "    folder_path = \"/Users/ngautam/Desktop/LLMOnto/Untitled/Mini-Files\"\n",
    "\n",
    "    # Path to the module file containing relevant information\n",
    "    module_file_path = \"/Users/ngautam/Desktop/LLMOnto/Untitled/LLM-Source/Enslaved_Schema_Relationships.txt\"\n",
    "\n",
    "    # Process text files and generate summaries\n",
    "    process_text_files(folder_path, module_file_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf56041",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Populate the ontology\n",
    "import os\n",
    "import openai\n",
    "\n",
    "# Function to read the text file\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Function to interact with ChatGPT and ask questions\n",
    "def populate_ontology(file_name, text_file, module_content, query):\n",
    "    # Use GPT-4 to populate the ontology modules\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"{query}\\n{text_file}\"}],\n",
    "        temperature=0,  # Adjust the temperature for generating diverse responses\n",
    "    )\n",
    "\n",
    "    # Extract the generated response text from the response\n",
    "    response_text = response.choices[0].message.content.strip()\n",
    "\n",
    "    # Save the holistic summary to a file\n",
    "    response_file_path = os.path.join(\"OntoLLM/responses\", f\"{os.path.splitext(file_name)[0]}_response.txt\")\n",
    "    with open(response_file_path, \"w\", encoding=\"utf-8\") as response_file:\n",
    "        response_file.write(response_text)\n",
    "\n",
    "# Main function to process text files and generate summaries\n",
    "def process_text_files(folder_path, module_file_path):\n",
    "    # Create the responses folder if it doesn't exist\n",
    "    os.makedirs(\"OntoLLM/responses\", exist_ok=True)\n",
    "\n",
    "    # Read module file content\n",
    "    with open(module_file_path, \"r\", encoding=\"utf-8\") as module_file:\n",
    "        module_content = module_file.read()\n",
    "        \n",
    "    # Define the query prompt\n",
    "    query = f\"\"\" Populate the ontology modules based on the information given in the text file, follwoing the example format. Use the keywords used to describe the relations in the example format.\n",
    "            Example format:\n",
    "            Name Record Module: \n",
    "            1. has_Name(Agent, Name): has_Name(Joseph Vance Lewis, \"Joseph Vance Lewis\")\n",
    "            2. has_Surname(Agent, Surname): has_Surname(Joseph Vance Lewis, \"Lewis\")\n",
    "            3. has_First_Name(Agent, First_Name): has_First_Name(Joseph Vance Lewis, \"Joseph\").\n",
    "            \n",
    "            Interagent Relationship Record Module:\n",
    "            1. has_Interagent_Relationship_Type_To(Agent, Relationship_Type): has_Interagent_Relationship_Type_To(Joseph Vance Lewis, \"Enslaver or Owner\") \n",
    "            2. is_Relationship_To(Agent, Agent): is_Relationship_To(Joseph Vance Lewis, \"Bwril Nate\").\n",
    "            Note: The Interagent Relationship Record Module describes the relationship to another agent who is an enslaver or owner. So use the keywords Enslaver or Owner to describe the relationship type. \n",
    "            \n",
    "            Sex Record Module:\n",
    "            1. has_Sex(Agent, Sex_Type): has_Sex(Joseph Vance Lewis, \"Male\").\n",
    "            \n",
    "            Occupation Record Module: \n",
    "            1. has_Occupation(Agent, Occupation): has_Occupation(Joseph Vance Lewis, \"Slave, Educator, Lawyer, and Autobiographer\").\n",
    "\n",
    "            Age Record Module: \n",
    "            1. has_Age(Agent, Age): has_Age(Joseph Vance Lewis, 72)\n",
    "            2. has_BirthDate(Agent, Date_of_Birth): has_BirthDate(Joseph Vance Lewis, \"December 25, 1853\")\n",
    "            3. has_DeathDate(Agent, Date_of_Death): has_DeathDate(Joseph Vance Lewis, \"April 24, 1925\").\n",
    "\n",
    "            Person Status Record Module: \n",
    "            1. has_Person_Status (Agent, Person_Status): has_Person_Status(Joseph Vance Lewis, \"Enslaved Person, Lawyer\")\n",
    "            2. has_Status_Generating_Event(Person_Status, Event): has_Status_Generating_Event(Enslaved Person, \"Sale of estate of Joseph Dubreuil\"), has_Status_Generating_Event(Lawyer, \"Emancipation\")\n",
    "            3. recorded_At(Person_Status_information, Event): recorded_At(Enslaved Person, \"Sale of estate of Joseph Dubreuil\"), recorded_At(Lawyer, \"Emancipation\").\n",
    "            Note: Skip the relations for which there is no information listed in the text file.\n",
    "            The given ontology modules: \n",
    "            {module_content}.\n",
    "            The given text file is:\"\"\"\n",
    "\n",
    "    # Iterate over text files in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            # Path to the current text file\n",
    "            text_file_path = os.path.join(folder_path, file_name)\n",
    "            \n",
    "            # Read input text from file\n",
    "            text_file = read_text_file(text_file_path)\n",
    "            \n",
    "            # Generate and save response for the current text file\n",
    "            populate_ontology(file_name, text_file, module_content, query)\n",
    "            \n",
    "            print(f\"The response is generated and saved for {file_name}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to the folder containing input text files\n",
    "    folder_path = \"/Users/ngautam/Desktop/LLMOnto/Untitled/Mini-Files\"\n",
    "\n",
    "    # Path to the module file containing relevant information\n",
    "    module_file_path = \"/Users/ngautam/Desktop/LLMOnto/Untitled/LLM-Source/Enslaved_Schema_Relationships.txt\"\n",
    "\n",
    "    # Process text files and generate summaries\n",
    "    process_text_files(folder_path, module_file_path) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
